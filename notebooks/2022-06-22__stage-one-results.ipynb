{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import ast\n",
    "from sklearn.metrics import f1_score\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pred_output_file(fname):\n",
    "    with open(fname) as f:\n",
    "        output = f.read()\n",
    "        preds = list(filter(lambda x: x != '', output.split('\\n')))\n",
    "        preds = list(zip(preds[::2], preds[1::2]))\n",
    "        preds = pd.Series(dict(preds))\n",
    "        preds = preds.apply(lambda x: x.replace('{}', '')).apply(ast.literal_eval)\n",
    "        \n",
    "    return (\n",
    "        preds\n",
    "             .apply(pd.Series)\n",
    "             .unstack()\n",
    "             .dropna()\n",
    "             .reset_index()\n",
    "             .rename(columns={'level_1': 'file_id', 'level_0': 'sent_idx', 0:'pred'})\n",
    "             .sort_values(['file_id', 'sent_idx'])\n",
    "             [['file_id', 'sent_idx', 'pred']]\n",
    "             .reset_index(drop=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: Polnear-only Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "polnear_only_fn = '../models_neural/quote_detection/output/first_run_annotated_scores.txt'\n",
    "polnear_only_pred_df = read_pred_output_file(polnear_only_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_df = pd.read_csv('../data/our-annotated-data-full.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "polnear_only_pred_and_label_df = (\n",
    "    polnear_only_pred_df\n",
    "    .merge(input_data_df, left_on=['file_id', 'sent_idx'], right_on=['entry_id', 'sent_idx'])\n",
    "    .drop('entry_id', axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32312070986104136"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    polnear_only_pred_and_label_df\n",
    "     .loc[lambda df: df['sentence'].str.strip().apply(unidecode.unidecode) != '\"']\n",
    "     .pipe(lambda df: f1_score(df['label'], df['pred']))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    polnear_only_pred_and_label_df\n",
    "     .loc[lambda df: df['sentence'].str.strip().apply(unidecode.unidecode) != '\"']\n",
    "     .pipe(lambda df: f1_score(df['label'], df['pred']))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>sent_idx</th>\n",
       "      <th>pred</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "      <th>quote_type</th>\n",
       "      <th>tagline</th>\n",
       "      <th>source_type</th>\n",
       "      <th>affiliation</th>\n",
       "      <th>role</th>\n",
       "      <th>role_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc_0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>A two - day rally in global stocks looked set ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc_0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Tokyo stocks were down significantly at midday...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc_0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Other markets in the Asian - Pacific region we...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc_0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>U.S. Treasury bonds , typically seen by invest...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc_0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Global stocks have been buoyed this week by pr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6830</th>\n",
       "      <td>doc_956</td>\n",
       "      <td>51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>Gabriela Ramirez , 33 , said that “ there ’s s...</td>\n",
       "      <td>QUOTE</td>\n",
       "      <td>33</td>\n",
       "      <td>Named Individual</td>\n",
       "      <td>Witness</td>\n",
       "      <td>Participant</td>\n",
       "      <td>Current</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6831</th>\n",
       "      <td>doc_956</td>\n",
       "      <td>52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>“</td>\n",
       "      <td>QUOTE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6832</th>\n",
       "      <td>doc_956</td>\n",
       "      <td>53</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>What kind of society do you live in</td>\n",
       "      <td>QUOTE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6833</th>\n",
       "      <td>doc_956</td>\n",
       "      <td>54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>, ” she said , “ if you just shoot with no con...</td>\n",
       "      <td>QUOTE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6834</th>\n",
       "      <td>doc_956</td>\n",
       "      <td>55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>”</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6835 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      file_id  sent_idx  pred  label  \\\n",
       "0       doc_0         0   0.0  False   \n",
       "1       doc_0         1   1.0  False   \n",
       "2       doc_0         2   1.0  False   \n",
       "3       doc_0         3   1.0  False   \n",
       "4       doc_0         4   0.0  False   \n",
       "...       ...       ...   ...    ...   \n",
       "6830  doc_956        51   0.0   True   \n",
       "6831  doc_956        52   0.0   True   \n",
       "6832  doc_956        53   1.0   True   \n",
       "6833  doc_956        54   0.0   True   \n",
       "6834  doc_956        55   0.0  False   \n",
       "\n",
       "                                               sentence quote_type tagline  \\\n",
       "0     A two - day rally in global stocks looked set ...        NaN     NaN   \n",
       "1     Tokyo stocks were down significantly at midday...        NaN     NaN   \n",
       "2     Other markets in the Asian - Pacific region we...        NaN     NaN   \n",
       "3     U.S. Treasury bonds , typically seen by invest...        NaN     NaN   \n",
       "4     Global stocks have been buoyed this week by pr...        NaN     NaN   \n",
       "...                                                 ...        ...     ...   \n",
       "6830  Gabriela Ramirez , 33 , said that “ there ’s s...      QUOTE      33   \n",
       "6831                                                  “      QUOTE     NaN   \n",
       "6832                What kind of society do you live in      QUOTE     NaN   \n",
       "6833  , ” she said , “ if you just shoot with no con...      QUOTE     NaN   \n",
       "6834                                                  ”        NaN     NaN   \n",
       "\n",
       "           source_type affiliation         role role_status  \n",
       "0                  NaN         NaN          NaN         NaN  \n",
       "1                  NaN         NaN          NaN         NaN  \n",
       "2                  NaN         NaN          NaN         NaN  \n",
       "3                  NaN         NaN          NaN         NaN  \n",
       "4                  NaN         NaN          NaN         NaN  \n",
       "...                ...         ...          ...         ...  \n",
       "6830  Named Individual     Witness  Participant     Current  \n",
       "6831               NaN         NaN          NaN         NaN  \n",
       "6832               NaN         NaN          NaN         NaN  \n",
       "6833               NaN         NaN          NaN         NaN  \n",
       "6834               NaN         NaN          NaN         NaN  \n",
       "\n",
       "[6835 rows x 11 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polnear_only_pred_and_label_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: Our Dataset only Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_run_fn = '../models_neural/quote_detection/output/second_run_annotated_scores.txt'\n",
    "second_run_pred_df = read_pred_output_file(second_run_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_training_df = pd.read_csv('../data/our-annotated-source-training-df.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>s</th>\n",
       "      <th>t_id</th>\n",
       "      <th>sent_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>BANGKOK</td>\n",
       "      <td>/train/doc_902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>—</td>\n",
       "      <td>/train/doc_902</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source        s            t_id  sent_idx\n",
       "0   False  BANGKOK  /train/doc_902         0\n",
       "1   False        —  /train/doc_902         1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_training_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_df = pd.read_csv('../data/our-annotated-data-full.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_run_pred_df_w_labels = (\n",
    "    second_run_pred_df\n",
    "         .merge(\n",
    "            orig_training_df.assign(join_key=lambda df: df['t_id'].str.split('/').str.get(2)), \n",
    "            left_on=['file_id', 'sent_idx'], \n",
    "            right_on=['join_key', 'sent_idx']\n",
    "         )\n",
    "         .assign(split=lambda df: df['t_id'].str.split('/').str.get(1))\n",
    "        .merge(\n",
    "            full_data_df, \n",
    "            left_on=['file_id', 'sent_idx'],\n",
    "            right_on=['entry_id', 'sent_idx']\n",
    "        )    \n",
    "        .drop(['t_id', 'join_key', 's', 'entry_id', 'source'], axis=1)\n",
    ").loc[lambda df: df['split'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8461137193531559"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(second_run_pred_df_w_labels\n",
    "  .pipe(lambda df: f1_score(df['label'], df['pred']))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quote_type\n",
       "BACKGROUND                      0.859873\n",
       "COMMUNICATION, NOT TO JOURNO    0.888889\n",
       "DECLINED COMMENT                0.941176\n",
       "DOCUMENT                        0.914286\n",
       "LAWSUIT                         0.000000\n",
       "Other: Data Analysis            1.000000\n",
       "Other: LAWSUIT                  0.969697\n",
       "PRESS REPORT                    0.892857\n",
       "PROPOSAL                        0.500000\n",
       "PUBLIC SPEECH, NOT TO JOURNO    1.000000\n",
       "PUBLISHED WORK                  0.789474\n",
       "QUOTE                           0.944649\n",
       "STATEMENT                       0.851852\n",
       "TWEET                           0.833333\n",
       "VOTE/POLL                       0.666667\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(second_run_pred_df_w_labels\n",
    " .groupby('quote_type')\n",
    " .apply(lambda df: f1_score(df['label'], df['pred']))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs = second_run_pred_df_w_labels.file_id.drop_duplicates().values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyperclip\n",
    "pyperclip.copy(', '.join(list(map(lambda x: '\"%s\"' % x,  test_docs))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "second_run_source_grouped = (\n",
    "    second_run_pred_df_w_labels\n",
    "         .assign(head=lambda df: df['head'].fillna('None'))\n",
    "         .groupby(['file_id', 'head'])\n",
    "         [['label', 'pred', 'source_type', 'affiliation', 'role', 'role_status']]\n",
    "         .aggregate(list)\n",
    "        #  ['label'].iloc[0]\n",
    "         .applymap(lambda x: list(filter(lambda y: pd.notnull(y), x)))\n",
    "         \n",
    "         .assign(pred=lambda df: df['pred'].apply(lambda x: (sum(x) / len(x)) > .3))\n",
    "#          .assign(pred=lambda df: df['pred'].apply(any))\n",
    "         .assign(label=lambda df: df['label'].apply(any))\n",
    "         .applymap(lambda x: x if not isinstance(x, list) else ('' if len(x) == 0 else x[0]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.923076923076923"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(second_run_source_grouped\n",
    " .loc[lambda df: df['source_type'] == '']\n",
    " .pipe(lambda df: f1_score(~df['label'], ~df['pred']))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.951048951048951"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_run_source_grouped.pipe(lambda df: f1_score(df['label'], df['pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_order = [\n",
    "    'Named Individual',\n",
    "    'Named Group',\n",
    "    'Report/Document',\n",
    "    'Unnamed Group',\n",
    "    'Unnamed Individual',\n",
    "    'Vote/Poll'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BERT Classification</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source_type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Named Individual</th>\n",
       "      <td>99.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Named Group</th>\n",
       "      <td>90.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Report/Document</th>\n",
       "      <td>94.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed Group</th>\n",
       "      <td>90.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed Individual</th>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vote/Poll</th>\n",
       "      <td>66.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    BERT Classification\n",
       "source_type                            \n",
       "Named Individual                   99.6\n",
       "Named Group                        90.6\n",
       "Report/Document                    94.7\n",
       "Unnamed Group                      90.9\n",
       "Unnamed Individual                100.0\n",
       "Vote/Poll                          66.7"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(second_run_source_grouped\n",
    " .groupby('source_type')\n",
    " .apply(lambda df: f1_score(df['label'], df['pred']))\n",
    " .loc[ind_order]\n",
    " .to_frame('BERT Classification').pipe(lambda df: df * 100).round(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARC3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "parc3_files = glob.glob('../data/academic-datasets/PARC3_complete/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "parc3_files = list(filter(lambda x: x.endswith('.xml'), parc3_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed...\n",
      "failed...\n",
      "failed...\n",
      "failed...\n",
      "failed...\n",
      "failed...\n",
      "failed...\n",
      "failed...\n",
      "failed...\n",
      "failed...\n",
      "failed...\n",
      "failed...\n"
     ]
    }
   ],
   "source": [
    "if False:\n",
    "    xml_str = open(parc3_files[400]).read()\n",
    "    tree = etree.fromstring(xml_str)\n",
    "    print(etree.tostring(tree, pretty_print=True).decode())\n",
    "\n",
    "    \n",
    "all_candidates = []\n",
    "for p in parc3_files:\n",
    "    xml_str = open(p).read()\n",
    "    try:\n",
    "        tree = etree.fromstring(xml_str)\n",
    "        c = tree.findall('candidate')\n",
    "        all_candidates += c\n",
    "    except:\n",
    "        print('failed...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_candidates = list(map(lambda x: x.text, all_candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "anon_candidate_list = (pd.Series(all_candidates)\n",
    " .value_counts()\n",
    " .reset_index()\n",
    " .rename(columns={0:'count', 'index':'name'})\n",
    " .loc[lambda df: df['name'].str.split().apply(lambda x: all(map(lambda y: y[0].upper() != y[0], x )))]\n",
    " ['name'].tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_exclue = ['he', 'she', 'they', 'it', 'i', 'me', 'you', 'we', \"said\",]\n",
    "additional_anon_source_list = list(filter(lambda x: x not in to_exclue, anon_candidate_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_string(inp_list, split_lines_on_k=4):\n",
    "    list_of_lists = []\n",
    "    for i in range(split_lines_on_k):\n",
    "        list_of_lists.append(inp_list[i::split_lines_on_k])\n",
    "    # \n",
    "    pre_line_split_output = []\n",
    "    rows = list(zip(*list_of_lists))\n",
    "    # remainder\n",
    "    remainder = inp_list[len(inp_list) - (len(inp_list) % split_lines_on_k):]\n",
    "    rows.append(remainder)\n",
    "    for l in rows:\n",
    "        pre_line_split_output.append(', '.join(list(map(lambda x: '\"%s\"' % x.strip(), l))))\n",
    "    return ',\\n'.join(pre_line_split_output)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "mods = ['a', 'the', 'one', 'some', 'several', 'many', 'its', 'even', 'big', 'this']\n",
    "additional_anon_source_list = list(map(lambda x: \n",
    "                                       ' '.join(list(filter(lambda y: y not in mods, x.split()))), \n",
    "                                       additional_anon_source_list\n",
    "                                      ))\n",
    "additional_anon_source_list = list(filter(lambda x: len(x.split()) < 5, additional_anon_source_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyperclip.copy(list_to_string(additional_anon_source_list, split_lines_on_k=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for Stage 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine ambiguous sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import json\n",
    "doc_to_look_at = 'doc_564'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_files = glob.glob('../app/data/input_data/*/*')\n",
    "annotated_files = glob.glob('../app/data/output_data_affil-role/*/*')\n",
    "checked_files = glob.glob('../app/data/checked_data_affil-role/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_target_file = list(filter(lambda x: '564' in x, annotated_files + checked_files))[0]\n",
    "input_target_file = list(filter(lambda x: '564' in x, input_data_files))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_df(annotated_fn, input_fn):\n",
    "    json_dat = json.load(open(annotated_fn))['data']\n",
    "    if isinstance(json_dat, dict) and 'row_data' in json_dat:\n",
    "        json_dat = json_dat['row_data']\n",
    "    annot_df = pd.DataFrame(json_dat)\n",
    "    annot_df = annot_df.applymap(lambda x: x['field_value'] if isinstance(x, dict) else x)\n",
    "    \n",
    "    input_dat = json.load(open(input_fn))['html_data']\n",
    "    input_df = pd.DataFrame(input_dat)\n",
    "\n",
    "    annot_df_with_input = (\n",
    "        input_df[['sent', 'sent_idx']]\n",
    "             .merge(annot_df[['row_idx', 'head', 'quote_type']], left_on='sent_idx', right_on='row_idx')\n",
    "             .drop(['row_idx', ], axis=1)\n",
    "#      .loc[lambda df: df['sent'].str.strip().str.len() > 1]\n",
    "    )\n",
    "    \n",
    "    return annot_df_with_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_multiply_annotated_sentences = []\n",
    "all_sources = []\n",
    "\n",
    "for annot_fn in annotated_files:\n",
    "    doc_id = re.search('\\d+', annot_fn.split('/')[-1])[0]\n",
    "    input_fn = annot_fn.replace('output_', 'input_').replace('_affil-role', '').replace('annotated-', 'to-annotate-')\n",
    "    checked_cand = annot_fn.replace('output_', 'checked_').replace('annotated-', 'checked-')\n",
    "    if checked_cand in checked_files:\n",
    "        annot_fn = checked_cand\n",
    "        \n",
    "    annot_df_w_input = get_combined_df(annot_fn, input_fn)\n",
    "    annot_df_w_input['doc_id'] = doc_id\n",
    "    multiply_annotated = annot_df_w_input.loc[lambda df: df['head'].str.contains('-\\d') == True]\n",
    "    all_multiply_annotated_sentences.append(multiply_annotated)\n",
    "    all_sources.append(annot_df_w_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_doc_df = pd.concat(all_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_multiply_annotated_df = pd.concat(all_multiply_annotated_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pyperclip\n",
    "\n",
    "pyperclip.copy(\n",
    "    (\n",
    "        all_doc_df\n",
    "         .loc[lambda df: df['doc_id'] ==  all_multiply_annotated_df['doc_id'].drop_duplicates().iloc[12]]\n",
    "         .to_csv(sep='\\t')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45    564\n",
       "25    592\n",
       "47    852\n",
       "34     99\n",
       "82     77\n",
       "18    514\n",
       "62    510\n",
       "24    431\n",
       "39    446\n",
       "24    204\n",
       "55    165\n",
       "6     602\n",
       "23    109\n",
       "Name: doc_id, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_multiply_annotated_df['doc_id'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Dataset for Stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BANGKOK\tNone\t0\t/train/902\r\n",
      "—\tNone\t1\t/train/902\r\n",
      "A plane carrying key senior Laotian government officials crashed Saturday morning , leaving at least four people dead , Laotian diplomats said Saturday .  \tLaotian diplomats\t2\t/train/902\r\n",
      "Killed in the crash were two top figures in the security apparatus of the authoritarian Lao government : the deputy prime minister , Douangchay Phichit , and Thongbane Sengaphone , the minister of public security , according to two Lao diplomats .  \tNone\t3\t/train/902\r\n",
      "For a Communist party that relies on force and intimidation to stay in power , the loss of what were arguably the two most powerful people in the security apparatus was a significant blow .  \tNone\t4\t/train/902\r\n",
      "The governor of Vientiane province was also killed in the crash .  \tNone\t5\t/train/902\r\n",
      "In addition to his post as deputy prime minister Mr. Douangchay was defense minister and a member of the Politburo , the highest decision - making body of the Communist party .\tNone\t6\t/train/902\r\n",
      "Mr. Thongbane , the public security head , was feared in the country and was said to be one of the officials leading a crackdown against dissent over the past year and half .  \tNone\t7\t/train/902\r\n",
      "That crackdown included the disappearance of the most prominent civic leader in the country , Sombath Somphone , a United States - trained agriculture specialist who led efforts to liberalize the hermetic communist leadership .  \tNone\t8\t/train/902\r\n",
      "Mr. Sombath was stopped at a police checkpoint in Dec. 2012 and has not been seen again .  \tNone\t9\t/train/902\r\n"
     ]
    }
   ],
   "source": [
    "!head ../models_neural/quote_attribution/data/our-annotated-data__stage-2.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "CLEANR = re.compile('<.*?>') \n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "    cleantext = re.sub(CLEANR, '', raw_html)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "out_file = '../models_neural/quote_attribution/data/our-annotated-data__stage-2.tsv'\n",
    "\n",
    "entry_ids = all_doc_df['doc_id'].unique().tolist()\n",
    "train_files, test_files = train_test_split(entry_ids)\n",
    "split_df = pd.concat([\n",
    "    pd.Series(train_files).to_frame('file_id').assign(split='train'),\n",
    "    pd.Series(test_files).to_frame('file_id').assign(split='test')\n",
    "])\n",
    "\n",
    "(\n",
    "    all_doc_df\n",
    "         .merge(split_df, left_on='doc_id', right_on='file_id')\n",
    "         .assign(entry_id=lambda df: '/' + df['split'] + '/' + df['doc_id'])\n",
    "         .assign(sent=lambda df: df['sent'].apply(cleanhtml))\n",
    "         .assign(head=lambda df: df.apply(lambda x: x['head'] if x['quote_type'] not in ['BACKGROUND', 'NARRATIVE'] else '', axis=1))\n",
    "         .assign(head=lambda df: df['head'].fillna('None').apply(lambda x: {'':'None'}.get(x, x)))\n",
    "        [['sent', 'head', 'sent_idx', 'entry_id']]\n",
    "     .to_csv(\n",
    "         out_file,\n",
    "         sep='\\t',\n",
    "         index=False,\n",
    "         header=False\n",
    "     )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide up data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_files = glob.glob('../app/data/input_data/*/*')\n",
    "annotated_files = glob.glob('../app/data/output_data_affil-role/*/*')\n",
    "checked_files = glob.glob('../app/data/checked_data_affil-role/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../app/data/output_data_affil-role/18/annotated-902.json'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../app/data/input_data/18/to-annotate-931.json'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_to_annotate = []\n",
    "for input_fn in input_data_files:\n",
    "    annot_fn = input_fn.replace('input_data', 'output_data_affil-role').replace('to-annotate-', 'annotated-')\n",
    "    if annot_fn in annotated_files:\n",
    "        continue\n",
    "    checked_cand = annot_fn.replace('output_', 'checked_').replace('annotated-', 'checked-')\n",
    "    if checked_cand in checked_files:\n",
    "        continue\n",
    "    left_to_annotate.append(input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_to_annotate_df = pd.Series(left_to_annotate).to_frame('fn').sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    left_to_annotate_df\n",
    "        .pipe(lambda df: \n",
    "                pd.concat([`\n",
    "                    df.iloc[:int(len(df)/2)].assign(annotator='alex'),\n",
    "                    df.iloc[int(len(df)/2):].assign(annotator='james'),\n",
    "                ])\n",
    "        )\n",
    "        .assign(file_id=lambda df: df['fn'].str.split('-').str.get(-1))\n",
    "    .to_csv('../app/data/2022-07-08__annotator-assignments.csv.gz', compression='gzip')\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['746.json',\n",
       " '364.json',\n",
       " '806.json',\n",
       " '917.json',\n",
       " '814.json',\n",
       " '48.json',\n",
       " '655.json',\n",
       " '506.json',\n",
       " '820.json',\n",
       " '555.json',\n",
       " '843.json',\n",
       " '187.json',\n",
       " '561.json',\n",
       " '813.json',\n",
       " '329.json',\n",
       " '614.json',\n",
       " '488.json',\n",
       " '523.json',\n",
       " '464.json',\n",
       " '716.json',\n",
       " '659.json',\n",
       " '682.json',\n",
       " '319.json',\n",
       " '954.json',\n",
       " '670.json',\n",
       " '529.json',\n",
       " '671.json',\n",
       " '388.json',\n",
       " '356.json',\n",
       " '240.json',\n",
       " '857.json',\n",
       " '770.json',\n",
       " '430.json',\n",
       " '213.json',\n",
       " '594.json',\n",
       " '465.json',\n",
       " '691.json',\n",
       " '789.json',\n",
       " '371.json',\n",
       " '260.json',\n",
       " '834.json',\n",
       " '301.json',\n",
       " '497.json',\n",
       " '538.json',\n",
       " '702.json',\n",
       " '706.json',\n",
       " '330.json',\n",
       " '251.json',\n",
       " '926.json',\n",
       " '22.json',\n",
       " '503.json',\n",
       " '743.json',\n",
       " '142.json',\n",
       " '350.json',\n",
       " '654.json',\n",
       " '617.json',\n",
       " '548.json',\n",
       " '950.json',\n",
       " '853.json',\n",
       " '825.json',\n",
       " '766.json',\n",
       " '589.json',\n",
       " '314.json',\n",
       " '199.json',\n",
       " '439.json',\n",
       " '863.json',\n",
       " '218.json',\n",
       " '605.json',\n",
       " '705.json',\n",
       " '693.json',\n",
       " '726.json',\n",
       " '489.json',\n",
       " '765.json',\n",
       " '939.json',\n",
       " '737.json',\n",
       " '534.json',\n",
       " '664.json',\n",
       " '635.json',\n",
       " '550.json',\n",
       " '96.json',\n",
       " '757.json',\n",
       " '482.json',\n",
       " '473.json',\n",
       " '749.json',\n",
       " '893.json',\n",
       " '248.json',\n",
       " '57.json',\n",
       " '895.json',\n",
       " '304.json',\n",
       " '263.json',\n",
       " '699.json',\n",
       " '237.json',\n",
       " '637.json',\n",
       " '718.json',\n",
       " '565.json',\n",
       " '650.json',\n",
       " '816.json',\n",
       " '612.json',\n",
       " '54.json',\n",
       " '870.json',\n",
       " '578.json',\n",
       " '27.json',\n",
       " '560.json',\n",
       " '827.json',\n",
       " '212.json',\n",
       " '295.json',\n",
       " '321.json',\n",
       " '918.json',\n",
       " '398.json',\n",
       " '501.json',\n",
       " '381.json',\n",
       " '264.json',\n",
       " '763.json',\n",
       " '478.json',\n",
       " '411.json',\n",
       " '796.json',\n",
       " '492.json',\n",
       " '516.json',\n",
       " '385.json',\n",
       " '844.json',\n",
       " '171.json',\n",
       " '817.json',\n",
       " '854.json',\n",
       " '524.json',\n",
       " '338.json',\n",
       " '704.json',\n",
       " '527.json',\n",
       " '764.json',\n",
       " '75.json',\n",
       " '729.json',\n",
       " '117.json',\n",
       " '466.json',\n",
       " '528.json',\n",
       " '102.json',\n",
       " '32.json',\n",
       " '193.json',\n",
       " '845.json',\n",
       " '904.json',\n",
       " '571.json',\n",
       " '170.json',\n",
       " '244.json',\n",
       " '357.json',\n",
       " '233.json',\n",
       " '97.json',\n",
       " '547.json',\n",
       " '250.json',\n",
       " '866.json',\n",
       " '856.json',\n",
       " '835.json',\n",
       " '869.json',\n",
       " '343.json',\n",
       " '51.json',\n",
       " '732.json',\n",
       " '597.json',\n",
       " '809.json',\n",
       " '549.json',\n",
       " '837.json',\n",
       " '797.json',\n",
       " '286.json',\n",
       " '736.json',\n",
       " '957.json',\n",
       " '657.json',\n",
       " '581.json',\n",
       " '656.json',\n",
       " '226.json',\n",
       " '701.json',\n",
       " '137.json',\n",
       " '945.json',\n",
       " '202.json',\n",
       " '751.json',\n",
       " '394.json',\n",
       " '595.json',\n",
       " '275.json',\n",
       " '85.json',\n",
       " '620.json',\n",
       " '400.json',\n",
       " '432.json',\n",
       " '225.json',\n",
       " '802.json',\n",
       " '69.json',\n",
       " '249.json',\n",
       " '231.json',\n",
       " '122.json',\n",
       " '148.json',\n",
       " '801.json',\n",
       " '95.json',\n",
       " '740.json',\n",
       " '788.json',\n",
       " '406.json',\n",
       " '236.json',\n",
       " '951.json',\n",
       " '616.json',\n",
       " '881.json',\n",
       " '566.json',\n",
       " '143.json',\n",
       " '908.json',\n",
       " '335.json',\n",
       " '471.json',\n",
       " '324.json',\n",
       " '779.json',\n",
       " '38.json',\n",
       " '334.json',\n",
       " '71.json',\n",
       " '89.json',\n",
       " '185.json',\n",
       " '316.json',\n",
       " '241.json',\n",
       " '438.json',\n",
       " '769.json',\n",
       " '238.json',\n",
       " '582.json',\n",
       " '672.json',\n",
       " '162.json',\n",
       " '322.json',\n",
       " '907.json',\n",
       " '794.json',\n",
       " '306.json',\n",
       " '176.json',\n",
       " '449.json',\n",
       " '505.json',\n",
       " '690.json',\n",
       " '634.json',\n",
       " '413.json',\n",
       " '409.json',\n",
       " '282.json',\n",
       " '733.json',\n",
       " '460.json',\n",
       " '257.json',\n",
       " '173.json',\n",
       " '811.json',\n",
       " '532.json',\n",
       " '167.json',\n",
       " '29.json',\n",
       " '930.json',\n",
       " '487.json',\n",
       " '804.json',\n",
       " '958.json',\n",
       " '551.json',\n",
       " '392.json',\n",
       " '610.json',\n",
       " '135.json',\n",
       " '552.json',\n",
       " '662.json',\n",
       " '63.json',\n",
       " '540.json',\n",
       " '598.json',\n",
       " '280.json',\n",
       " '112.json',\n",
       " '683.json',\n",
       " '91.json',\n",
       " '376.json',\n",
       " '596.json',\n",
       " '889.json',\n",
       " '689.json',\n",
       " '668.json',\n",
       " '277.json',\n",
       " '414.json',\n",
       " '293.json',\n",
       " '429.json',\n",
       " '291.json',\n",
       " '106.json',\n",
       " '287.json',\n",
       " '127.json',\n",
       " '830.json',\n",
       " '803.json',\n",
       " '865.json',\n",
       " '53.json',\n",
       " '436.json',\n",
       " '580.json',\n",
       " '190.json',\n",
       " '819.json',\n",
       " '41.json',\n",
       " '418.json',\n",
       " '708.json',\n",
       " '8.json',\n",
       " '318.json',\n",
       " '627.json',\n",
       " '339.json',\n",
       " '234.json',\n",
       " '734.json',\n",
       " '101.json',\n",
       " '498.json',\n",
       " '626.json',\n",
       " '531.json',\n",
       " '508.json',\n",
       " '289.json',\n",
       " '546.json',\n",
       " '150.json',\n",
       " '651.json',\n",
       " '145.json',\n",
       " '340.json',\n",
       " '805.json',\n",
       " '607.json',\n",
       " '370.json',\n",
       " '484.json',\n",
       " '351.json',\n",
       " '445.json',\n",
       " '115.json',\n",
       " '521.json',\n",
       " '245.json',\n",
       " '404.json',\n",
       " '891.json',\n",
       " '799.json',\n",
       " '761.json',\n",
       " '425.json',\n",
       " '773.json',\n",
       " '453.json',\n",
       " '258.json',\n",
       " '194.json',\n",
       " '151.json',\n",
       " '720.json',\n",
       " '415.json',\n",
       " '639.json',\n",
       " '83.json',\n",
       " '771.json',\n",
       " '660.json',\n",
       " '673.json',\n",
       " '883.json',\n",
       " '444.json',\n",
       " '688.json',\n",
       " '851.json',\n",
       " '79.json',\n",
       " '624.json',\n",
       " '49.json',\n",
       " '105.json',\n",
       " '161.json',\n",
       " '896.json',\n",
       " '694.json',\n",
       " '942.json',\n",
       " '604.json',\n",
       " '459.json',\n",
       " '87.json',\n",
       " '78.json',\n",
       " '374.json',\n",
       " '188.json',\n",
       " '21.json',\n",
       " '452.json',\n",
       " '325.json',\n",
       " '687.json',\n",
       " '903.json',\n",
       " '573.json',\n",
       " '721.json',\n",
       " '511.json',\n",
       " '741.json',\n",
       " '223.json',\n",
       " '927.json',\n",
       " '346.json',\n",
       " '742.json',\n",
       " '727.json',\n",
       " '276.json',\n",
       " '747.json',\n",
       " '461.json',\n",
       " '271.json',\n",
       " '227.json',\n",
       " '873.json',\n",
       " '841.json',\n",
       " '713.json',\n",
       " '878.json',\n",
       " '205.json',\n",
       " '629.json',\n",
       " '419.json',\n",
       " '412.json',\n",
       " '396.json',\n",
       " '885.json',\n",
       " '352.json',\n",
       " '469.json',\n",
       " '923.json',\n",
       " '59.json',\n",
       " '157.json',\n",
       " '848.json',\n",
       " '838.json',\n",
       " '155.json',\n",
       " '695.json',\n",
       " '636.json',\n",
       " '421.json',\n",
       " '365.json',\n",
       " '332.json',\n",
       " '858.json',\n",
       " '348.json',\n",
       " '928.json',\n",
       " '486.json',\n",
       " '90.json']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(    left_to_annotate_df\n",
    "        .pipe(lambda df: \n",
    "                pd.concat([\n",
    "                    df.iloc[:int(len(df)/2)].assign(annotator='alex'),\n",
    "                    df.iloc[int(len(df)/2):].assign(annotator='james'),\n",
    "                ])\n",
    "        )\n",
    "        .assign(file_id=lambda df: df['fn'].str.split('-').str.get(-1))\n",
    " .loc[lambda df: df['annotator'] == 'alex']\n",
    " ['file_id'].tolist()\n",
    " \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226.66666666666666"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "680 / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
