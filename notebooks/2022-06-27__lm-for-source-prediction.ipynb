{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/bart-base were not used when initializing BartForCausalLM: ['encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.1.fc2.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.embed_tokens.weight', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.1.fc1.weight', 'encoder.layers.5.final_layer_norm.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.2.fc2.bias', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.5.fc2.weight', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.5.fc1.weight', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.4.self_attn_layer_norm.weight', 'shared.weight', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.3.fc1.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.fc1.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.0.fc1.weight', 'encoder.layernorm_embedding.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.fc2.bias', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.bias', 'encoder.layers.1.fc1.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.embed_positions.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.0.fc2.bias', 'encoder.layers.4.fc1.bias', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.3.fc1.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.4.fc2.weight', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layernorm_embedding.bias', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.4.fc2.bias']\n",
      "- This IS expected if you are initializing BartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForCausalLM\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "model = BartForCausalLM.from_pretrained(\"facebook/bart-base\", add_cross_attention=False)\n",
    "# assert model.config.is_decoder, f\"{model.__class__} has to be configured as a decoder.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_shape = [1, inputs.input_ids.shape[-1], model.config.vocab_size]\n",
    "list(logits.shape) == expected_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 50265])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roberta Causal LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = RobertaForMaskedLM.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "':'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"He said Jonathan <mask>\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# retrieve index of <mask>\n",
    "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "tokenizer.decode(predicted_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForCausalLM, RobertaConfig\n",
    "config = RobertaConfig.from_pretrained(\"roberta-base\")\n",
    "config.is_decoder = True\n",
    "model = RobertaForCausalLM.from_pretrained(\"roberta-base\", config=config)\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[27.7075, -4.6183, 25.2726,  ...,  0.9433,  2.5662, 11.1329],\n",
       "         [12.9955, -3.3435, 32.1472,  ...,  2.9392,  0.1569,  9.3820],\n",
       "         [ 9.7384, -3.5373, 21.8192,  ...,  0.4942, -0.2668,  6.7838],\n",
       "         ...,\n",
       "         [ 5.6973, -4.3181, 21.6467,  ..., -3.2017, -4.4629,  4.5802],\n",
       "         [ 4.9998, -4.6587, 15.6738,  ..., -3.9533, -4.5533,  2.9168],\n",
       "         [24.6395, -4.3410, 23.1517,  ...,  0.3120,  2.7200,  9.1151]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 6, 2, 2, 2, 2, 0])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits.argmax(axis=-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'].shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'additional_special_tokens': ['<ANS>']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_tokens('<ANS>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_seq = tokenizer.encode('hello my name is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[31373,   616,  1438,   318]]),\n",
       " 'labels': tensor([[31373,   616,  1438,   318]])}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator([x_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.sep_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50257]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.additional_special_tokens_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 31414, 50265, 2], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('Hello <ANS>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<mask>'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 10975, 48560, 742, 2]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('[SOURCE]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "training_df = pd.read_csv('../data/our-annotated-data-full.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaModel, BertModel\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(2, 768)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings.token_type_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "entry_ids = training_df['entry_id'].unique().tolist()\n",
    "\n",
    "train_files, test_files = train_test_split(entry_ids)\n",
    "\n",
    "split_df = pd.concat([\n",
    "    pd.Series(train_files).to_frame('file_id').assign(split='train'),\n",
    "    pd.Series(test_files).to_frame('file_id').assign(split='test')\n",
    "])\n",
    "\n",
    "(\n",
    "    training_df[['sentence', 'head', 'entry_id', 'sent_idx']]\n",
    "     .assign(head=lambda df: df['head'].fillna('None'))\n",
    "     .merge(split_df, left_on='entry_id', right_on='file_id')\n",
    "     .assign(entry_id=lambda df: '/' + df['split'] + '/' + df['entry_id'])\n",
    "     .to_csv('../models_neural/quote_attribution/data/our-annotated-data__stage-2.tsv', sep='\\t', index=False, header=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_x = t('hello there', 'goodbye there', return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.4661,  0.4146, -0.2632,  ..., -0.6092,  0.3328,  0.2058],\n",
       "         [ 0.0529, -0.1565,  0.6466,  ..., -0.1399,  0.5327, -0.0591],\n",
       "         [-0.5668,  0.3817,  0.0310,  ...,  0.1186,  0.1656, -0.8779],\n",
       "         ...,\n",
       "         [ 0.5606, -0.7884,  0.8330,  ...,  0.1023, -0.4338,  0.1270],\n",
       "         [-0.3168,  0.7927,  0.4076,  ..., -0.2205, -0.1307, -0.7212],\n",
       "         [ 0.7880, -0.0190, -0.3237,  ...,  0.2183, -0.6140, -0.4414]]],\n",
       "       grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[-0.9465, -0.6739, -0.9829,  0.9434,  0.8044, -0.4242,  0.9825,  0.4936,\n",
       "         -0.9570, -1.0000, -0.6956,  0.9828,  0.9738,  0.8625,  0.9699, -0.8913,\n",
       "         -0.7156, -0.7530,  0.5997, -0.8519,  0.8314,  1.0000, -0.2099,  0.5029,\n",
       "          0.6551,  0.9992, -0.9150,  0.9447,  0.9711,  0.6582, -0.9089,  0.3807,\n",
       "         -0.9853, -0.4662, -0.9850, -0.9959,  0.6584, -0.8156, -0.2740, -0.2343,\n",
       "         -0.9203,  0.5705,  1.0000,  0.3078,  0.5422, -0.5018, -1.0000,  0.5096,\n",
       "         -0.9100,  0.9848,  0.9678,  0.9574,  0.4434,  0.7307,  0.6927, -0.2426,\n",
       "          0.2323,  0.3462, -0.4678, -0.7804, -0.7784,  0.6465, -0.9647, -0.9548,\n",
       "          0.9751,  0.9612, -0.5083, -0.5356, -0.4373,  0.1054,  0.9716,  0.5010,\n",
       "         -0.1245, -0.9040,  0.9111,  0.5254, -0.7951,  1.0000, -0.8180, -0.9776,\n",
       "          0.9700,  0.9542,  0.7421, -0.8068,  0.8712, -1.0000,  0.7550, -0.2730,\n",
       "         -0.9869,  0.5231,  0.7524, -0.5105,  0.8658,  0.7915, -0.8843, -0.6600,\n",
       "         -0.6398, -0.9412, -0.5517, -0.4881,  0.3506, -0.5103, -0.6580, -0.6492,\n",
       "          0.5000, -0.7137, -0.7490,  0.7510,  0.5846,  0.8281,  0.5926, -0.6367,\n",
       "          0.7117, -0.9755,  0.8190, -0.5637, -0.9814, -0.7783, -0.9872,  0.8359,\n",
       "         -0.6333, -0.5399,  0.9789, -0.5602,  0.6473, -0.3735, -0.9834, -1.0000,\n",
       "         -0.8347, -0.7168, -0.5764, -0.5643, -0.9738, -0.9623,  0.8076,  0.9574,\n",
       "          0.4787,  1.0000, -0.6298,  0.9581, -0.7714, -0.8770,  0.8519, -0.7531,\n",
       "          0.8731,  0.8167, -0.8363,  0.4449, -0.4755,  0.7158, -0.8717, -0.4982,\n",
       "         -0.9411, -0.9557, -0.5727,  0.9630, -0.7796, -0.9932, -0.6057, -0.4321,\n",
       "         -0.7454,  0.9220,  0.8301,  0.6320, -0.6279,  0.6899,  0.8739,  0.7688,\n",
       "         -0.8921, -0.5920,  0.6982, -0.5169, -0.9665, -0.9776, -0.6662,  0.7201,\n",
       "          0.9875,  0.8321,  0.5180,  0.9282, -0.5276,  0.8787, -0.9618,  0.9795,\n",
       "         -0.5194,  0.4925, -0.5171,  0.6592, -0.8938,  0.5097,  0.9500, -0.8762,\n",
       "         -0.8417, -0.3792, -0.6861, -0.6386, -0.9040,  0.5660, -0.5402, -0.6113,\n",
       "         -0.3769,  0.9252,  0.9971,  0.8580,  0.6547,  0.7511, -0.9539, -0.7034,\n",
       "          0.3289,  0.5121,  0.4487,  0.9946, -0.8015, -0.3425, -0.9522, -0.9885,\n",
       "          0.2486, -0.9300, -0.4157, -0.8372,  0.8839, -0.5411,  0.9123,  0.7113,\n",
       "         -0.9989, -0.8572,  0.5597, -0.6267,  0.7148, -0.4546,  0.4664,  0.9846,\n",
       "         -0.7538,  0.9389,  0.9134, -0.9704, -0.7983,  0.9494, -0.5966,  0.9371,\n",
       "         -0.8716,  0.9964,  0.9844,  0.9283, -0.9517, -0.9303, -0.9178, -0.9587,\n",
       "         -0.3371,  0.7595,  0.9869,  0.7892,  0.6123, -0.4233, -0.8485,  0.9999,\n",
       "         -0.5659, -0.9443, -0.4158, -0.7468, -0.9839,  0.9557,  0.5963,  0.4481,\n",
       "         -0.7028, -0.8459, -0.9623,  0.9709,  0.4483,  0.9982, -0.4546, -0.9866,\n",
       "         -0.8343, -0.9376,  0.1140, -0.5665, -0.7993,  0.0229, -0.9711,  0.7071,\n",
       "          0.6067,  0.7306, -0.9721,  0.9999,  1.0000,  0.9736,  0.9305,  0.9803,\n",
       "         -1.0000, -0.3575,  1.0000, -0.9987, -1.0000, -0.9499, -0.8706,  0.5369,\n",
       "         -1.0000, -0.3195, -0.2156, -0.9270,  0.9160,  0.9744,  0.9993, -1.0000,\n",
       "          0.8376,  0.9530, -0.8243,  0.9921, -0.6253,  0.9721,  0.8387,  0.5271,\n",
       "         -0.5325,  0.6675, -0.9896, -0.9586, -0.8785, -0.8972,  0.9998,  0.3739,\n",
       "         -0.8914, -0.9497,  0.5466, -0.3448,  0.3294, -0.9725, -0.4777,  0.8881,\n",
       "          0.9041,  0.3400,  0.5534, -0.8201,  0.5097,  0.3659,  0.7243,  0.8202,\n",
       "         -0.9662, -0.7653, -0.4300,  0.1484, -0.9188, -0.9595,  0.9790, -0.6932,\n",
       "          0.9746,  1.0000,  0.3513, -0.9425,  0.8514,  0.5776, -0.3524,  1.0000,\n",
       "          0.8647, -0.9725, -0.7419,  0.7764, -0.7909, -0.8266,  0.9999, -0.5298,\n",
       "         -0.9289, -0.6553,  0.9679, -0.9838,  0.9993, -0.9486, -0.9703,  0.9785,\n",
       "          0.9448, -0.8072, -0.6451,  0.4825, -0.9446,  0.5999, -0.9792,  0.9013,\n",
       "          0.7913, -0.3564,  0.8983, -0.9747, -0.6920,  0.5660, -0.8088, -0.5789,\n",
       "          0.9832,  0.7820, -0.4922,  0.2708, -0.6259, -0.2961, -0.9807,  0.6283,\n",
       "          1.0000, -0.6029,  0.8451, -0.8442, -0.2439,  0.2283,  0.7246,  0.7893,\n",
       "         -0.5732, -0.9280,  0.9011, -0.9943, -0.9830,  0.9045,  0.5595, -0.5699,\n",
       "          1.0000,  0.8391,  0.4798,  0.6852,  0.9977,  0.3001,  0.8462,  0.9847,\n",
       "          0.9682, -0.5027,  0.7615,  0.9148, -0.9850, -0.5873, -0.8045,  0.3054,\n",
       "         -0.9088, -0.2629, -0.9642,  0.9667,  0.9896,  0.6432,  0.5282,  0.8240,\n",
       "          1.0000, -0.3665,  0.7882, -0.8295,  0.9720, -1.0000, -0.9468, -0.6108,\n",
       "         -0.4044, -0.9685, -0.5602,  0.5203, -0.9659,  0.9670,  0.8947, -0.9988,\n",
       "         -0.9910, -0.6020,  0.9659,  0.3719, -0.9965, -0.9245, -0.6297,  0.8909,\n",
       "         -0.5503, -0.9569, -0.7794, -0.5185,  0.7402, -0.4467,  0.7259,  0.9823,\n",
       "         -0.0428, -0.7842, -0.5294, -0.3674, -0.9119,  0.9268, -0.8925, -0.9824,\n",
       "         -0.4569,  1.0000, -0.5706,  0.9686,  0.8639,  0.8559, -0.4490,  0.3772,\n",
       "          0.9831,  0.5629, -0.9655, -0.9684, -0.9301, -0.6899,  0.7599,  0.6102,\n",
       "          0.9488,  0.8656,  0.8566,  0.3249, -0.3004,  0.3310,  1.0000, -0.4789,\n",
       "         -0.4448, -0.7244, -0.2947, -0.6355, -0.7864,  1.0000,  0.5704,  0.7049,\n",
       "         -0.9897, -0.9595, -0.9789,  1.0000,  0.8321, -0.8529,  0.8223,  0.8333,\n",
       "         -0.3764,  0.9369, -0.5078, -0.5571,  0.5242,  0.3856,  0.9635, -0.8026,\n",
       "         -0.9635, -0.7488,  0.7517, -0.9691,  1.0000, -0.7751, -0.5212, -0.5957,\n",
       "         -0.4504,  0.8535,  0.1974, -0.9804, -0.5182,  0.4570,  0.9578,  0.4937,\n",
       "         -0.7617, -0.9314,  0.9647,  0.9187, -0.9810, -0.9614,  0.9647, -0.9909,\n",
       "          0.7902,  1.0000,  0.4441,  0.4150,  0.5797, -0.6644,  0.6684, -0.5150,\n",
       "          0.8716, -0.9716, -0.5697, -0.4676,  0.6481, -0.3401, -0.3815,  0.8053,\n",
       "          0.4413, -0.7486, -0.8279, -0.4501,  0.6469,  0.9517, -0.5791, -0.4729,\n",
       "          0.3039, -0.3843, -0.9803, -0.5255, -0.7234, -1.0000,  0.8296, -1.0000,\n",
       "          0.7243,  0.7524, -0.4666,  0.8535,  0.2483,  0.8010, -0.8427, -0.9683,\n",
       "         -0.1052,  0.8759, -0.6482, -0.8163, -0.8193,  0.6266, -0.4760,  0.4937,\n",
       "         -0.8007,  0.8416, -0.4689,  1.0000,  0.4806, -0.8609, -0.9974,  0.4486,\n",
       "         -0.5014,  1.0000, -0.9535, -0.9525,  0.6838, -0.8735, -0.9028,  0.5652,\n",
       "          0.2844, -0.9160, -0.9907,  0.9828,  0.9745, -0.7156,  0.7075, -0.6087,\n",
       "         -0.7589,  0.3697,  0.9618,  0.9846,  0.7449,  0.9423, -0.0812, -0.5492,\n",
       "          0.9696,  0.4486,  0.7115,  0.4239,  1.0000,  0.5869, -0.9442, -0.2807,\n",
       "         -0.9862, -0.5455, -0.9686,  0.5992,  0.4995,  0.9073, -0.5737,  0.9727,\n",
       "         -0.9653,  0.2993, -0.7726, -0.8490,  0.6366, -0.9437, -0.9796, -0.9844,\n",
       "          0.7691, -0.6324, -0.3174,  0.4470,  0.3008,  0.6949,  0.7288, -1.0000,\n",
       "          0.9425,  0.7138,  0.9867,  0.9591,  0.9108,  0.6464,  0.5152, -0.9811,\n",
       "         -0.9980, -0.6439, -0.4594,  0.8835,  0.7771,  0.9299,  0.6966, -0.6093,\n",
       "         -0.4791, -0.8266, -0.4791, -0.9920,  0.6763, -0.8926, -0.9929,  0.9617,\n",
       "          0.4992, -0.4577, -0.6388, -0.9211,  0.9895,  0.8747,  0.7123,  0.3544,\n",
       "          0.5058,  0.9190,  0.9860,  0.9845, -0.9660,  0.9028, -0.8794,  0.7187,\n",
       "          0.6518, -0.9442,  0.4443,  0.6398, -0.7491,  0.4876, -0.4730, -0.9952,\n",
       "          0.4704, -0.5429,  0.7961, -0.7007, -0.2448, -0.7007, -0.3506, -0.7561,\n",
       "         -0.8899,  0.8100,  0.6506,  0.9078,  0.9413, -0.3596, -0.8952, -0.5206,\n",
       "         -0.9633, -0.9300,  0.9834, -0.4274, -0.8746,  0.7410,  0.2781,  0.7340,\n",
       "          0.6030, -0.6282, -0.6318, -0.8078,  0.9429, -0.2434, -0.7547, -0.8074,\n",
       "          0.8325,  0.5545,  1.0000, -0.9567, -0.9882, -0.2810, -0.6418,  0.5941,\n",
       "         -0.7285, -1.0000,  0.5999, -0.8122,  0.9267, -0.8950,  0.9376, -0.8277,\n",
       "         -0.9953, -0.5837,  0.5162,  0.8891, -0.7294, -0.8435,  0.7742, -0.5935,\n",
       "          0.9958,  0.9177, -0.3852, -0.4292,  0.8035, -0.8148, -0.8114,  0.9574]],\n",
       "       grad_fn=<TanhBackward>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**input_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = RobertaModel.from_pretrained('/Users/alex/.cache/torch/transformers/named-models/roberta-base-expanded-embeddings')\n",
    "tokenizer = RobertaTokenizer.from_pretrained('/Users/alex/.cache/torch/transformers/named-models/roberta-base-expanded-embeddings') \n",
    "\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tokenizer.encode('hello there', return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tokenizer('hello there. goodbye there', return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 42891,    89,     4, 15364,    89,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_type_ids = torch.tensor([[0, 0, 0, 1, 1, 2, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_type_ids.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['input_ids'].shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.0946,  0.1196, -0.0174,  ..., -0.0560, -0.0533,  0.0211],\n",
       "         [-0.1686,  0.0156, -0.0141,  ..., -0.2361,  0.0356,  0.2221],\n",
       "         [-0.0015, -0.0080, -0.1995,  ..., -0.2882, -0.0861,  0.4210],\n",
       "         ...,\n",
       "         [-0.1293,  0.1659,  0.0207,  ..., -0.6263,  0.0744,  0.2371],\n",
       "         [-0.0430,  0.0038, -0.1387,  ..., -0.2977, -0.0662,  0.3205],\n",
       "         [-0.0884,  0.1214, -0.0362,  ..., -0.1134, -0.0571, -0.0015]]],\n",
       "       grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[ 1.5123e-03, -2.0938e-01, -2.2946e-01, -6.5062e-02,  1.2375e-01,\n",
       "          1.8425e-01,  2.6301e-01, -6.8447e-02, -6.4302e-02, -1.9072e-01,\n",
       "          2.1776e-01, -5.5137e-03, -1.2040e-01,  1.1931e-01, -1.3478e-01,\n",
       "          4.9272e-01,  2.0823e-01, -4.7860e-01,  6.7879e-02, -3.0249e-02,\n",
       "         -2.8252e-01,  9.0588e-02,  4.7406e-01,  3.5760e-01,  1.2559e-01,\n",
       "          6.0991e-02, -1.4532e-01,  6.8363e-03,  1.7409e-01,  2.3602e-01,\n",
       "          3.0788e-01,  7.3320e-02,  7.3060e-02,  2.6752e-01, -2.5963e-01,\n",
       "          4.5639e-02, -3.3681e-01,  1.8924e-02,  2.6367e-01, -1.9622e-01,\n",
       "         -7.3829e-02,  1.9139e-01,  2.0871e-01, -1.2917e-01, -1.5057e-01,\n",
       "          4.1971e-01,  2.9333e-01, -1.1339e-02, -1.3759e-01, -9.5045e-02,\n",
       "         -3.5833e-01,  3.4591e-01,  3.2079e-01,  2.1147e-01, -2.7206e-02,\n",
       "          5.7990e-02, -1.2725e-01,  2.7027e-01, -6.1255e-02, -9.9187e-02,\n",
       "         -1.0962e-01, -2.1369e-01, -5.4431e-03, -8.6019e-02,  2.3716e-02,\n",
       "         -1.5886e-01,  8.9409e-02, -1.5739e-01, -1.5151e-01,  2.8144e-02,\n",
       "         -9.1885e-02,  1.7161e-01,  1.6557e-01, -3.0940e-01, -3.0200e-01,\n",
       "          6.9337e-02, -5.8865e-01, -1.0194e-01,  3.0539e-01,  4.2655e-01,\n",
       "         -1.2405e-01,  2.1233e-01,  2.6339e-02,  2.4160e-01, -1.6804e-02,\n",
       "         -3.6186e-02, -2.9394e-02, -1.2837e-01,  1.8032e-01,  2.7136e-01,\n",
       "         -1.8851e-01, -3.9861e-01,  5.1360e-02,  2.1384e-03, -1.0567e-01,\n",
       "          1.4786e-02, -5.0631e-03, -9.1412e-02, -1.6448e-01, -1.7347e-01,\n",
       "          9.9689e-02, -2.7350e-01, -1.6786e-01,  3.1065e-01, -1.1915e-02,\n",
       "         -1.9025e-01, -4.3409e-02,  3.1638e-01,  5.9115e-02, -1.1002e-01,\n",
       "         -1.9522e-01,  4.3785e-01,  3.1964e-01,  1.2631e-02,  1.0124e-02,\n",
       "          1.9594e-01,  1.5005e-01, -3.0281e-01,  4.4225e-01, -3.1928e-01,\n",
       "         -1.3898e-02, -9.6325e-02,  1.0131e-01,  1.6325e-01, -2.3830e-01,\n",
       "          2.9795e-01,  1.4088e-01,  2.5989e-01,  1.7301e-01,  1.0674e-01,\n",
       "         -5.2693e-02,  1.6065e-01, -9.8103e-02,  1.3616e-01,  2.4311e-01,\n",
       "          1.2297e-01, -2.0789e-02, -3.3681e-01, -2.1919e-01,  2.8453e-01,\n",
       "          3.3839e-01,  1.5618e-01, -4.6942e-02,  1.9364e-01,  1.0139e-01,\n",
       "          2.2661e-01,  1.5325e-01, -4.1637e-01,  4.0458e-02,  3.6436e-01,\n",
       "          1.0385e-01,  1.5217e-01, -9.8225e-02, -2.9255e-01, -2.7739e-01,\n",
       "         -1.1480e-01,  3.3322e-02, -3.4567e-01, -1.3649e-01,  3.6544e-01,\n",
       "          3.4885e-02, -2.3441e-02, -1.5552e-01, -2.3122e-01, -1.7554e-02,\n",
       "         -1.2006e-01,  2.9026e-02,  9.3562e-02, -5.8461e-02, -4.4728e-01,\n",
       "         -9.7935e-02, -5.5449e-01, -1.2963e-01,  2.1226e-01, -3.4677e-01,\n",
       "          2.7227e-01, -2.9105e-01,  1.1095e-01,  4.1483e-01,  3.7381e-02,\n",
       "         -1.8257e-02, -2.1251e-01, -9.0646e-03,  9.8140e-02,  3.1744e-01,\n",
       "          2.7511e-01, -4.3166e-01,  1.0724e-01,  1.6489e-01,  2.6362e-01,\n",
       "          1.2264e-01, -6.9744e-02, -1.6127e-01,  1.6183e-01, -1.8997e-01,\n",
       "          1.9128e-01, -2.5418e-01,  1.6931e-01, -2.4420e-01, -2.2393e-01,\n",
       "          3.0071e-01, -4.3407e-01, -2.8165e-02,  8.1658e-02,  2.7211e-01,\n",
       "          2.7230e-02, -1.1275e-02, -8.9824e-02,  1.1513e-01,  1.6435e-01,\n",
       "          1.4913e-01, -4.0875e-01,  2.9249e-01, -2.7029e-02, -2.8928e-02,\n",
       "         -1.5175e-02,  1.9354e-01,  2.7338e-01,  1.2085e-01, -4.0945e-01,\n",
       "         -1.3527e-01,  1.3987e-01,  3.0760e-01, -2.4140e-01,  1.6395e-01,\n",
       "         -2.9031e-01, -4.1394e-01, -1.1914e-01,  2.2587e-01,  2.4795e-01,\n",
       "          1.7613e-01, -2.9965e-01,  1.6742e-01, -1.2376e-01, -4.2353e-01,\n",
       "         -3.9120e-01, -1.1062e-01,  2.3654e-01,  1.5245e-01,  1.7757e-01,\n",
       "          2.5609e-01,  1.6195e-02,  1.4582e-01,  1.5153e-01,  1.8252e-01,\n",
       "         -1.4472e-01,  1.8197e-01, -3.7736e-01, -6.5894e-02, -2.8680e-01,\n",
       "         -2.0297e-01, -2.2585e-01,  4.1913e-01, -2.5579e-01,  2.4192e-01,\n",
       "          3.9825e-01, -3.2348e-01, -1.1575e-01,  1.3258e-01,  1.0412e-01,\n",
       "          7.5582e-02, -1.1314e-01,  2.1630e-01,  1.7299e-01, -9.7097e-02,\n",
       "          2.5134e-01,  5.9149e-04,  2.7757e-01,  1.6848e-01,  8.5652e-02,\n",
       "          1.5078e-01,  1.4992e-01, -1.3554e-01,  1.0483e-01,  3.7710e-03,\n",
       "         -3.0040e-02, -2.4881e-01, -1.4086e-01,  2.4278e-01, -6.4308e-02,\n",
       "          4.7073e-02, -1.7439e-01, -1.1022e-01,  2.2001e-02,  4.3367e-01,\n",
       "         -3.6736e-01,  2.5768e-01,  7.4801e-02,  1.6187e-01, -2.4229e-01,\n",
       "         -2.2724e-01,  1.0053e-01,  1.9844e-01, -4.1661e-01, -9.7294e-03,\n",
       "          1.4333e-01,  1.3280e-01,  2.2856e-01,  2.9078e-01, -6.1349e-03,\n",
       "         -1.2896e-01,  5.0101e-01, -1.7245e-01, -1.4589e-01,  2.4985e-01,\n",
       "         -2.5984e-01, -2.8415e-01,  2.3365e-01, -4.3392e-02,  3.0636e-01,\n",
       "          1.4306e-01,  2.7995e-02,  7.1587e-02, -6.0691e-01,  3.0707e-02,\n",
       "         -4.5625e-01, -3.3258e-02,  4.1012e-02, -8.7203e-02, -1.9358e-01,\n",
       "          1.5511e-01,  2.9424e-01, -2.5619e-01, -2.3868e-02,  1.9071e-01,\n",
       "          6.3192e-02, -1.2380e-01,  4.9094e-01, -1.6214e-04,  2.3175e-01,\n",
       "         -5.7480e-02,  2.6778e-01, -2.2273e-01,  2.7400e-01, -2.5398e-01,\n",
       "         -1.0345e-01,  2.3244e-02,  8.6574e-02,  5.4588e-02, -6.2653e-02,\n",
       "         -3.5697e-01,  2.2064e-01, -1.4418e-02, -4.2913e-02, -6.2886e-02,\n",
       "          1.1681e-01, -4.0414e-03,  5.6967e-02,  3.9370e-02,  3.4560e-01,\n",
       "          2.2915e-01, -1.2604e-02, -3.8422e-01, -5.2680e-02, -1.0729e-01,\n",
       "          6.3701e-02,  5.6079e-02,  2.6179e-03,  4.5740e-01, -7.1446e-02,\n",
       "         -1.4334e-02, -1.2671e-01,  2.7214e-01,  2.1429e-01,  1.3745e-01,\n",
       "          1.3221e-01,  4.7496e-02,  1.4615e-01, -3.6947e-02, -1.8598e-02,\n",
       "         -1.6225e-01, -2.2504e-01, -2.6762e-01,  2.0649e-01, -2.3600e-01,\n",
       "         -1.7646e-01,  1.3166e-01,  2.1111e-01, -1.6795e-01,  1.3762e-01,\n",
       "          3.3055e-01,  1.2251e-01, -1.5488e-01,  2.7350e-01, -1.1311e-01,\n",
       "          1.0501e-01,  2.9979e-01, -3.8461e-02,  1.8314e-01,  5.0140e-01,\n",
       "          2.1648e-01, -3.6239e-01, -6.0483e-02, -2.1256e-01,  6.1590e-03,\n",
       "          2.3484e-01, -1.4864e-01,  1.9703e-01,  3.9228e-01,  3.1270e-01,\n",
       "          4.5196e-01,  7.0954e-03, -1.2868e-01,  7.8695e-02,  2.2928e-01,\n",
       "          5.2915e-02, -1.6326e-01, -1.8832e-01,  2.7158e-01,  6.2837e-02,\n",
       "         -1.4194e-01, -5.3869e-03, -1.3759e-01,  5.0994e-02, -1.2730e-01,\n",
       "         -3.9080e-01,  3.5740e-02,  1.9801e-01, -4.9388e-01,  1.0773e-01,\n",
       "         -3.0316e-01,  4.4954e-02, -2.5041e-01,  2.3524e-01, -2.2552e-01,\n",
       "         -1.2725e-01,  4.1359e-01, -6.9440e-02,  4.2380e-02, -2.0574e-01,\n",
       "         -1.5392e-01, -2.5423e-03,  1.5576e-02, -5.4681e-02, -3.5224e-02,\n",
       "          3.4140e-01, -1.4571e-01,  5.1584e-02,  2.4899e-02,  2.0370e-01,\n",
       "         -5.8990e-02,  1.8907e-01, -3.1989e-03, -1.4407e-01, -3.8676e-01,\n",
       "          1.2959e-01, -1.8639e-01, -4.4919e-01, -3.9556e-01,  3.8943e-01,\n",
       "         -1.1744e-01, -2.8153e-01, -2.0233e-01, -2.6087e-01,  7.5622e-02,\n",
       "          2.1309e-01,  4.5962e-01, -3.9421e-01, -7.7025e-02,  4.9063e-01,\n",
       "         -6.1255e-02, -2.0291e-01,  3.0721e-01,  2.2204e-01, -3.3976e-01,\n",
       "          3.4014e-01,  2.9911e-01, -6.2256e-02,  4.8107e-02,  5.3038e-01,\n",
       "          1.2301e-01,  2.0176e-01, -2.0517e-01,  4.5475e-01, -2.1414e-01,\n",
       "          3.3542e-01, -1.5327e-01, -2.0414e-01, -2.5243e-01, -9.9282e-04,\n",
       "          3.4308e-01,  2.1600e-01, -4.3088e-01, -1.5159e-01,  4.2626e-02,\n",
       "          3.5496e-01, -3.9883e-01, -8.9748e-02,  3.1657e-03, -3.6194e-01,\n",
       "          1.2940e-01,  1.4283e-01,  2.3632e-01, -3.9476e-01, -3.5208e-02,\n",
       "          3.9642e-01, -3.0106e-01,  1.4578e-01,  3.1380e-01,  7.1925e-02,\n",
       "          3.6465e-01, -1.1529e-02, -5.3837e-03,  3.6593e-02, -2.4200e-01,\n",
       "         -6.0414e-02,  1.3587e-01,  5.7492e-01,  1.5049e-01, -3.7022e-01,\n",
       "          9.6015e-02,  2.7439e-01, -1.5632e-01,  3.1776e-01, -1.0060e-01,\n",
       "         -5.5852e-02,  2.8549e-01, -5.9479e-02,  1.4455e-01, -9.9521e-02,\n",
       "         -2.2974e-01, -3.0538e-01,  4.0626e-01, -2.2193e-01, -1.0349e-01,\n",
       "         -1.8281e-01, -1.2561e-01, -1.7403e-01,  2.2781e-02, -3.7963e-01,\n",
       "          3.3671e-01,  1.3799e-01, -2.2463e-01, -6.1878e-02, -9.7565e-02,\n",
       "         -1.5408e-01, -2.2538e-01, -2.7900e-01,  4.3546e-01, -1.8605e-01,\n",
       "         -4.6026e-01,  2.8337e-01,  3.1800e-02,  3.5379e-01,  2.8450e-02,\n",
       "          1.1123e-01, -4.4267e-02,  1.0965e-01,  8.9577e-02, -9.6881e-02,\n",
       "          2.9590e-01,  7.0461e-02, -5.7478e-01, -1.2624e-01, -2.2522e-01,\n",
       "          7.4024e-02,  2.0615e-01, -3.6313e-01, -4.2768e-04,  3.8921e-02,\n",
       "          1.7314e-01,  1.1502e-02, -1.3006e-01, -6.7937e-02,  4.1917e-01,\n",
       "          2.3976e-01,  3.1065e-01,  1.1239e-01,  2.3495e-01, -2.2911e-02,\n",
       "         -3.5191e-01,  2.5525e-02,  9.6386e-02, -2.1801e-01,  4.4456e-01,\n",
       "         -1.0505e-01, -4.1481e-01, -8.6288e-02,  4.0459e-01,  1.0236e-01,\n",
       "         -4.1468e-02, -1.1985e-02,  2.2855e-01,  1.5840e-01, -1.2831e-01,\n",
       "          1.8492e-01, -2.6198e-02, -1.4406e-01, -1.1913e-01,  9.3479e-02,\n",
       "         -2.2535e-01,  6.1666e-02, -1.5437e-01, -2.7252e-02, -2.0039e-01,\n",
       "         -6.1689e-04, -2.0282e-01,  2.7634e-01, -3.3814e-01,  7.0112e-02,\n",
       "          6.5168e-02,  3.0308e-01, -3.5617e-01, -1.9309e-01, -2.6294e-02,\n",
       "          1.7596e-01,  2.6001e-01,  3.4562e-01,  1.6487e-02,  2.9472e-02,\n",
       "         -1.4868e-01, -2.7388e-01,  5.8492e-02, -2.2353e-01,  1.5141e-01,\n",
       "          7.4659e-02,  2.7402e-01, -3.1691e-01, -1.8356e-01,  2.2793e-01,\n",
       "         -9.3598e-02, -1.5674e-01,  4.1448e-01,  2.5099e-01,  2.0929e-01,\n",
       "          1.6159e-02,  2.6753e-01,  5.2531e-02, -1.9465e-01, -1.0951e-01,\n",
       "         -2.4839e-01,  1.0800e-01, -9.8620e-02, -6.0991e-02, -7.7790e-02,\n",
       "         -1.2676e-01, -2.1492e-01, -1.6425e-01,  1.5038e-01,  1.6393e-01,\n",
       "          1.9503e-02, -7.0705e-02, -3.7364e-02, -3.0541e-01,  3.0841e-01,\n",
       "          9.1982e-03,  1.0269e-01, -8.4712e-02,  6.8617e-02, -1.6877e-01,\n",
       "          2.4287e-01,  2.0647e-01,  7.2539e-02, -2.0678e-01, -4.1427e-02,\n",
       "         -3.0865e-01, -3.5912e-01,  5.7869e-02,  1.3258e-01,  1.0674e-01,\n",
       "         -1.0029e-01, -2.7581e-01, -4.3657e-02, -1.3497e-01,  1.9209e-01,\n",
       "          3.4606e-03, -1.3425e-01, -1.0803e-01, -5.1499e-02, -5.7767e-02,\n",
       "          8.3742e-02, -2.1855e-01, -1.8808e-01, -1.3960e-01, -7.9605e-02,\n",
       "         -7.9404e-02,  3.6663e-01, -6.3278e-02,  3.2299e-01, -1.5819e-01,\n",
       "          2.3202e-03, -2.0281e-01,  1.0151e-01, -8.4147e-02,  6.5385e-02,\n",
       "          2.8724e-01, -4.4219e-01, -1.6563e-01,  2.9917e-03, -2.1487e-01,\n",
       "         -1.6421e-01, -6.1212e-02, -5.2700e-02,  2.2287e-01, -3.6973e-01,\n",
       "          2.3460e-01, -1.2857e-01,  1.7792e-01, -6.8041e-02, -2.6969e-01,\n",
       "         -1.6810e-01,  4.7150e-02,  2.5534e-01, -3.6477e-01, -2.4569e-01,\n",
       "         -2.7209e-01, -1.0485e-01, -9.4616e-02, -2.6704e-01,  4.5083e-01,\n",
       "         -1.1131e-01, -8.7175e-02,  3.7181e-02,  4.4429e-01,  2.0573e-01,\n",
       "          1.6244e-01,  2.0985e-01,  8.8609e-03,  2.8910e-02,  1.3537e-01,\n",
       "         -4.6914e-01,  2.4514e-01, -2.4849e-01, -1.1870e-01,  5.6816e-04,\n",
       "          1.1304e-01, -2.9886e-02,  1.3584e-02, -1.5278e-01, -9.7373e-02,\n",
       "          2.3629e-01, -3.6946e-01, -2.9681e-02,  2.9476e-01,  1.5139e-01,\n",
       "         -2.5573e-01,  4.5750e-02,  1.1592e-01,  3.7298e-01,  1.1493e-01,\n",
       "         -2.4479e-01,  1.2387e-01, -3.5605e-01, -4.2282e-02, -1.7834e-01,\n",
       "         -3.0793e-01,  1.6072e-01, -6.3578e-02,  7.1159e-02, -8.7459e-02,\n",
       "         -3.1188e-01,  2.2942e-01, -6.3662e-02, -5.9008e-02,  4.2767e-01,\n",
       "          4.4383e-02, -9.7950e-02,  1.3400e-01,  9.9270e-03,  2.8033e-02,\n",
       "         -1.0291e-01,  2.8449e-01,  2.2737e-01, -2.9730e-01,  1.1878e-01,\n",
       "         -1.5158e-01, -4.7867e-02, -1.0420e-01]], grad_fn=<TanhBackward>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(**x, token_type_ids=token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(1, 768)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings.token_type_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = model.embeddings.token_type_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(1, 768)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings.token_type_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.type_vocab_size = 3\n",
    "single_emb = model.embeddings.token_type_embeddings\n",
    "model.embeddings.token_type_embeddings = torch.nn.Embedding(3, single_emb.embedding_dim)\n",
    "model.embeddings.token_type_embeddings.weight = torch.nn.Parameter(single_emb.weight.repeat([3, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForQuestionAnswering\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"deepset/roberta-base-squad2\")\n",
    "model = RobertaForQuestionAnswering.from_pretrained(\"deepset/roberta-base-squad2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n",
    "\n",
    "inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>Who was Jim Henson?</s></s>Jim Henson was a nice puppet</s>'"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31373, 616, 1438, 318]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('hello my name is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2Model.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_batch_encode_plus() got an unexpected keyword argument 'add_prefix_space'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-3e312d0ca1a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m bad_words_ids = [\n\u001b[1;32m      9\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbad_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_prefix_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbad_word\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"idiot\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stupid\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shut up\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m ]\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-119-3e312d0ca1a8>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m bad_words_ids = [\n\u001b[1;32m      9\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbad_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_prefix_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbad_word\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"idiot\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stupid\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shut up\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m ]\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2440\u001b[0m                 \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2441\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2442\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2443\u001b[0m             )\n\u001b[1;32m   2444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2510\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2511\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2512\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2513\u001b[0m         )\n\u001b[1;32m   2514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m         )\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_directory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename_prefix\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m         )\n\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         )\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_encode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_encode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: _batch_encode_plus() got an unexpected keyword argument 'add_prefix_space'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "input_context = \"My cute dog\"\n",
    "\n",
    "# get tokens of words that should not be generated\n",
    "bad_words_ids = [\n",
    "    tokenizer(bad_word, add_prefix_space=True).input_ids \n",
    "    for bad_word in [\"idiot\", \"stupid\", \"shut up\"]\n",
    "]\n",
    "\n",
    "# encode input context\n",
    "input_ids = tokenizer(input_context, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# generate sequences without allowing bad_words to be generated\n",
    "outputs = model.generate(\n",
    "    input_ids=input_ids, \n",
    "    max_length=20, \n",
    "    do_sample=True, \n",
    "    bad_words_ids=bad_words_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiheaded Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 935,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 5\n",
    "N = 10\n",
    "E = 512\n",
    "\n",
    "L = 1\n",
    "x = torch.rand(( N, S, E))\n",
    "\n",
    "query_layer = torch.nn.Linear(S, L)\n",
    "key_layer = torch.nn.Linear(S, S)\n",
    "value_layer = torch.nn.Linear(S, S)\n",
    "attention = torch.nn.MultiheadAttention(E, 8, batch_first=True)\n",
    "\n",
    "batch_size, seq_len, e_dim = x.shape\n",
    "\n",
    "x_t = x.view(batch_size, e_dim, seq_len)\n",
    "Q = query_layer(x_t).view(batch_size, L, e_dim)\n",
    "K = key_layer(x_t).view(batch_size, seq_len, e_dim)\n",
    "V = value_layer(x_t).view(batch_size, seq_len, e_dim)\n",
    "\n",
    "o, attn_weights = attention.forward(Q, K, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 937,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 512, 5])"
      ]
     },
     "execution_count": 937,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 938,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 512])"
      ]
     },
     "execution_count": 938,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 939,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 512])"
      ]
     },
     "execution_count": 939,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 512])"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2011, 0.1972, 0.2087, 0.1930, 0.2000], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights[0].sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Special Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import sys\n",
    "import utils_params as params\n",
    "from importlib import reload\n",
    "reload(utils_params)\n",
    "import jellyfish\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "sys.path.append('../models_neural/quote_attribution/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_doc = [\n",
    "    ['BANGKOK', 'None', '/test/doc_902', '0', 'doc_902', 'test'], \n",
    "    ['', 'None', '/test/doc_902', '1', 'doc_902', 'test'], \n",
    "    ['A plane carrying key senior Laotian government officials crashed Saturday morning , leaving at least four people dead , Laotian diplomats said Saturday .  ', 'Laotian diplomats', '/test/doc_902', '2', 'doc_902', 'test'], \n",
    "    ['Killed in the crash were two top figures in the security apparatus of the authoritarian Lao government : the deputy prime minister , Douangchay Phichit , and Thongbane Sengaphone , the minister of public security , according to two Lao diplomats .  ', 'None', '/test/doc_902', '3', 'doc_902', 'test'], ['For a Communist party that relies on force and intimidation to stay in power , the loss of what were arguably the two most powerful people in the security apparatus was a significant blow .  ', 'None', '/test/doc_902', '4', 'doc_902', 'test'], ['The governor of Vientiane province was also killed in the crash .  ', 'None', '/test/doc_902', '5', 'doc_902', 'test'], ['In addition to his post as deputy prime minister Mr. Douangchay was defense minister and a member of the Politburo , the highest decision - making body of the Communist party .', 'None', '/test/doc_902', '6', 'doc_902', 'test'], ['Mr. Thongbane , the public security head , was feared in the country and was said to be one of the officials leading a crackdown against dissent over the past year and half .  ', 'None', '/test/doc_902', '7', 'doc_902', 'test'], ['That crackdown included the disappearance of the most prominent civic leader in the country , Sombath Somphone , a United States - trained agriculture specialist who led efforts to liberalize the hermetic communist leadership .  ', 'None', '/test/doc_902', '8', 'doc_902', 'test'], ['Mr. Sombath was stopped at a police checkpoint in Dec. 2012 and has not been seen again .  ', 'None', '/test/doc_902', '9', 'doc_902', 'test'], ['The Lao news agency posted photos to a web site Saturday showing the mangled wreckage of the plane , a Russian - made Antonov AN-74TK300 .  ', 'The Lao news agency', '/test/doc_902', '10', 'doc_902', 'test'], ['The news agency , citing a statement from the prime minister s office , said it was a Laotian Air Force aircraft', 'The Lao news agency', '/test/doc_902', '11', 'doc_902', 'test'], ['and it had crashed in Nadi village , west of the Xiangkhouang airport and not far from a major archeological site of prehistoric carved stone vessels , the Plain of Jars .  ', 'The Lao news agency', '/test/doc_902', '12', 'doc_902', 'test'], ['The authorities were  helping to rescue the survivors ,  the news agency said , without offering details on the number of people killed .  ', 'The Lao news agency', '/test/doc_902', '13', 'doc_902', 'test'], ['The plane was traveling from the capital , Vientiane , to the mountainous northeastern province of Xiangkhouang , where the officials were due to attend a military ceremony .  ', 'None', '/test/doc_902', '14', 'doc_902', 'test'], ['State television in Laos showed footage of rescue workers recovering debris from the aircraft , which appeared to have crashed in a jungle - covered area .  ', 'State television', '/test/doc_902', '15', 'doc_902', 'test'], ['A news presenter said the crash occurred at around 7 a.m.', 'State television', '/test/doc_902', '16', 'doc_902', 'test'], ['', 'None', '/test/doc_902', '17', 'doc_902', 'test'], ['The cause of the plane crash is still unknown ,  the presenter said .  ', 'State television', '/test/doc_902', '18', 'doc_902', 'test'], ['A Lao Facebook page showed images of thick black smoke rising up near what appeared to be an airport runway .', 'The Lao news agency', '/test/doc_902', '19', 'doc_902', 'test'], ['The images could not be independently confirmed .  ', 'None', '/test/doc_902', '20', 'doc_902', 'test'], ['The crash was the second in Laos in the last year .', 'None', '/test/doc_902', '21', 'doc_902', 'test'], ['Last October 49 people were killed when a Lao Airlines flight crashed in the south of the country .  ', 'None', '/test/doc_902', '22', 'doc_902', 'test'], ['The Foreign Ministry in neighboring Thailand said it  received reports  about the crash .  ', 'Foreign Ministry', '/test/doc_902', '23', 'doc_902', 'test'], ['', 'None', '/test/doc_902', '24', 'doc_902', 'test'], ['There were about 20 passengers on board of which most were of high stature ,  said Sek Wannamethee , a spokesman for the Thai Foreign Ministry .', 'Sek Wannamethee', '/test/doc_902', '25', 'doc_902', 'test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1. extract a candidate list\n",
    "# step 2. reconcile the extracted candidate list with the list of annotated sources to see \n",
    "#             if any are slight deviations. Use the extracted source version\n",
    "# step 3. get the word offsets of the first full mention of the source in the document\n",
    "# step 4. generate token sequences for each source, generate token sequences for each sentences\n",
    "# step 5. get the cross-product of source and sentence token sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_candidates = get_unique_spacy_ents(ent_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ent_candidates_df['source_tokenized'] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ent_candidates_df = pd.DataFrame(all_ent_candidates).drop_duplicates('candidate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source_ind_tokens      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "sentence_ind_tokens    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "doc_tokens             [0, 387, 9298, 530, 9335, 2, 0, 578, 2, 0, 250...\n",
       "label                                                              False\n",
       "dtype: object"
      ]
     },
     "execution_count": 1012,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(output_data[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2064,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "951"
      ]
     },
     "execution_count": 2064,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1018,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BANGKOK',\n",
       " 'Laotian',\n",
       " 'government officials',\n",
       " 'Laotian diplomats',\n",
       " 'four people',\n",
       " 'Lao',\n",
       " 'Douangchay Phichit',\n",
       " 'Thongbane Sengaphone',\n",
       " 'diplomats',\n",
       " 'Communist',\n",
       " 'people',\n",
       " 'Vientiane province',\n",
       " 'Douangchay',\n",
       " 'Politburo',\n",
       " 'Thongbane',\n",
       " 'officials',\n",
       " 'a United States',\n",
       " 'Sombath',\n",
       " 'police',\n",
       " 'Russian',\n",
       " 'Antonov',\n",
       " 'the lao news agency',\n",
       " 'Air Force',\n",
       " 'Nadi',\n",
       " 'Xiangkhouang',\n",
       " 'authorities',\n",
       " 'Vientiane',\n",
       " 'Laos',\n",
       " 'state television',\n",
       " 'Lao Airlines',\n",
       " 'The Foreign Ministry',\n",
       " 'Thailand',\n",
       " 'reports',\n",
       " 'Sek Wannamethee',\n",
       " 'the Thai Foreign Ministry',\n",
       " 'spokesman']"
      ]
     },
     "execution_count": 1018,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1029,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1030,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = AutoConfig.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1042,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.12.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 1042,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1043,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 5\n",
    "N = 10\n",
    "E = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1044,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = torch.rand((N, S, E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1047,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5, 768])"
      ]
     },
     "execution_count": 1047,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.layer[0].attention.self.forward(hidden)[0].shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1295,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = list(map(lambda x: tokenizer.encode( x, return_tensors='pt').squeeze() , [\n",
    "    'Hello my name is Alex',\n",
    "    'Alex is my name'\n",
    "]))\n",
    "\n",
    "input_ids = pad_sequence( input_ids, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1064,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_sequence, pad_packed_sequence, pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1080,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 7, 768])"
      ]
     },
     "execution_count": 1080,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings.forward(input_ids).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1083,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "max_seq_len = 10        # S\n",
    "emb_dim = 512           # E\n",
    "hidden_dim = 768        # H\n",
    "output_seq_len = 1      # L\n",
    "num_heads = 12          # M\n",
    "\n",
    "query_layer = torch.nn.Linear(emb_dim, hidden_dim)\n",
    "compressor_layer = torch.nn.Linear(max_seq_len, output_seq_len)\n",
    "key_layer = torch.nn.Linear(emb_dim, hidden_dim)\n",
    "value_layer = torch.nn.Linear(emb_dim, hidden_dim)\n",
    "attention = torch.nn.MultiheadAttention(hidden_dim, num_heads, batch_first=True)\n",
    "\n",
    "batch_size = 15\n",
    "x_seq_len = 5\n",
    "x = torch.rand((batch_size, x_seq_len, emb_dim))\n",
    "\n",
    "z = torch.zeros(batch_size, max_seq_len - x_seq_len, emb_dim)\n",
    "x_inp = torch.hstack([x, z])\n",
    "\n",
    "Q = query_layer(x_inp)\n",
    "V = value_layer(x_inp)\n",
    "K = key_layer(x_inp)\n",
    "\n",
    "Q_s = compressor_layer(Q.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "\n",
    "K.shape \n",
    "\n",
    "Q.shape \n",
    "\n",
    "o, _ = attention.forward(Q_s, K, V, need_weights=False)\n",
    "\n",
    "o.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 768])"
      ]
     },
     "execution_count": 1161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "model_dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 5, 512])"
      ]
     },
     "execution_count": 1193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1209,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_mask = torch.tensor([[[1.,1.,1.,0.,0.]]]*15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 1, 5])"
      ]
     },
     "execution_count": 1210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_mask.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1211,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_layer = torch.nn.Linear(emb_dim, hidden_dim)\n",
    "key_layer = torch.nn.Linear(emb_dim, hidden_dim)\n",
    "value_layer = torch.nn.Linear(emb_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1212,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_repr = nn.Parameter(torch.zeros(hidden_dim))\n",
    "attn = torch.nn.MultiheadAttention(hidden_dim, 8, batch_first=True)\n",
    "\n",
    "B, T, D = x.size()  # [Batch, Time, Dim]\n",
    "query = cls_repr.view(1, 1, hidden_dim).repeat(B, 1, 1)\n",
    "\n",
    "key = key_layer(x)\n",
    "value = value_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1213,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The shape of the 3D attn_mask is torch.Size([15, 1, 5]), but should be (120, 1, 5).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1213-710e89487f88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Args: Query, Key, Value, Mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcls_repr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[1;32m   1036\u001b[0m                 \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m                 \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneed_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m                 attn_mask=attn_mask)\n\u001b[0m\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_output_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[1;32m   4995\u001b[0m             \u001b[0mcorrect_3d_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4996\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mcorrect_3d_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4997\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4998\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4999\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"attn_mask's dimension {attn_mask.dim()} is not supported\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The shape of the 3D attn_mask is torch.Size([15, 1, 5]), but should be (120, 1, 5)."
     ]
    }
   ],
   "source": [
    "# Args: Query, Key, Value, Mask\n",
    "cls_repr, _  = attn.forward(query, key, value, need_weights=False, attn_mask=attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0860,  0.1208,  0.0414,  ...,  0.0887,  0.1254, -0.0819],\n",
       "        [-0.1348,  0.1030,  0.0248,  ...,  0.1277,  0.0863, -0.0379],\n",
       "        [-0.1329,  0.1024,  0.0467,  ...,  0.1019,  0.1245, -0.0619],\n",
       "        ...,\n",
       "        [-0.1245,  0.0934,  0.0171,  ...,  0.1142,  0.1566, -0.0690],\n",
       "        [-0.1655,  0.0969,  0.0866,  ...,  0.1305,  0.1565, -0.0559],\n",
       "        [-0.0945,  0.1202,  0.0599,  ...,  0.0963,  0.0844, -0.0768]],\n",
       "       grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 1192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_repr.view(B, hidden_dim)  # [B, D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 1, 512])"
      ]
     },
     "execution_count": 1181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_repr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1217,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(1, 2, 3, 4)\n",
    "b = a.transpose(1, 2)  # Swaps 2nd and 3rd dimension\n",
    "c = a.view(1, 3, 2, 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1224,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_inp = torch.arange(15*768).reshape(1, 15, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[    0,     1,     2,  ...,   765,   766,   767],\n",
       "         [  768,   769,   770,  ...,  1533,  1534,  1535],\n",
       "         [ 1536,  1537,  1538,  ...,  2301,  2302,  2303],\n",
       "         ...,\n",
       "         [ 9216,  9217,  9218,  ...,  9981,  9982,  9983],\n",
       "         [ 9984,  9985,  9986,  ..., 10749, 10750, 10751],\n",
       "         [10752, 10753, 10754,  ..., 11517, 11518, 11519]]])"
      ]
     },
     "execution_count": 1225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[    0,     1,     2,  ...,    61,    62,    63],\n",
       "         [   64,    65,    66,  ...,   125,   126,   127],\n",
       "         [  128,   129,   130,  ...,   189,   190,   191],\n",
       "         ...,\n",
       "         [11328, 11329, 11330,  ..., 11389, 11390, 11391],\n",
       "         [11392, 11393, 11394,  ..., 11453, 11454, 11455],\n",
       "         [11456, 11457, 11458,  ..., 11517, 11518, 11519]]])"
      ]
     },
     "execution_count": 1226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_inp.contiguous().view(1, 15 * 12, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".transpose(0, 1).shape \n",
    "Out[2]: torch.Size([180, 1, 64])\n",
    "q.contiguous().shape \n",
    "Out[3]: torch.Size([1, 15, 768])\n",
    "q.contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1229,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = nn.Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1238,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0900, 0.2447, 0.6652, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 1238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s(torch.tensor([1.,2.,3., -np.inf, -np.inf]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1510,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 768\n",
    "hidden_dim = 768\n",
    "\n",
    "query_layer = torch.nn.Linear(emb_dim, hidden_dim)\n",
    "key_layer = torch.nn.Linear(emb_dim, hidden_dim)\n",
    "value_layer = torch.nn.Linear(emb_dim, hidden_dim)\n",
    "compressor_layer = torch.nn.Linear(max_sequence_len, 1)\n",
    "attn = torch.nn.MultiheadAttention(hidden_dim, 12, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1882,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10])"
      ]
     },
     "execution_count": 1882,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2039,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = list(map(lambda x:\n",
    "                     tokenizer.encode(x, return_tensors='pt').squeeze(),\n",
    "                     ['Hello my name is Alex there man',\n",
    "                      'Alex is there, man, man, man.',\n",
    "                      'Alex is there, man.',\n",
    "                     ]\n",
    "                    )\n",
    ")\n",
    "\n",
    "max_sequence_len = 13\n",
    "\n",
    "seq_lens = list(map(len, input_ids))\n",
    "max_seq_len = max(seq_lens)\n",
    "attn_mask = list(map(lambda x: [1] * x + [0] * (max_seq_len - x), seq_lens))\n",
    "attn_mask = torch.tensor(attn_mask)\n",
    "input_ids = pad_sequence(input_ids, batch_first=True)\n",
    "batch_size, _ = input_ids.shape \n",
    "hidden = model.embeddings(input_ids)\n",
    "\n",
    "num_cols_zeros = max_sequence_len - hidden.shape[1]\n",
    "hidden = torch.hstack([hidden, torch.zeros((batch_size, num_cols_zeros, 768))])\n",
    "attn_mask = torch.hstack([attn_mask, torch.zeros((batch_size, num_cols_zeros))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2040,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_test = hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2053,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_test = hidden.where(hidden != 0, torch.ones_like(hidden) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2054,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = query_layer(hidden_test)\n",
    "K = key_layer(hidden_test)\n",
    "V = value_layer(hidden_test)\n",
    "\n",
    "t_a = attn_mask.unsqueeze(-1).repeat(1, 1, hidden_dim)\n",
    "masked_Q = Q + torch.where(t_a != 0, torch.zeros_like(Q), -Q)\n",
    "masked_K = K + torch.where(t_a != 0, torch.zeros_like(K), -K)\n",
    "masked_V = V + torch.where(t_a != 0, torch.zeros_like(V), -V)\n",
    "\n",
    "Q_s = compressor_layer(masked_Q.permute(0, 2, 1)).permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2055,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_first_row_old = Q_s[0]\n",
    "V_first_row_old = masked_V[0]\n",
    "K_first_row_old = masked_K[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2056,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 2056,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_first_row_old.isclose(Q_s[0]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2057,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 2057,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_first_row_old.isclose(masked_V[0]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2058,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 2058,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_first_row_old.isclose(masked_K[0]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2059,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# attn_mask.repeat(1, 1, 12).reshape(24, 1, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2060,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = attn_mask.unsqueeze(1).repeat(12, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2061,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t_a_2 = attn_mask.unsqueeze(1).repeat(12, 1, 1)\n",
    "t_a_2 = torch.log(t_a_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2062,
   "metadata": {},
   "outputs": [],
   "source": [
    "o, _ = attn.forward(Q_s, masked_K, masked_V, need_weights=False)#, attn_mask=t_a_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2051,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_o = o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2063,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0601, -0.0689, -0.0302,  ...,  0.0335,  0.0272,  0.0230]],\n",
       "\n",
       "        [[-0.0581, -0.0296, -0.0408,  ..., -0.0189,  0.0677,  0.0551]],\n",
       "\n",
       "        [[-0.0462, -0.0207, -0.0120,  ...,  0.0096,  0.0374,  0.0389]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 2063,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2038,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0601, -0.0689, -0.0302,  ...,  0.0335,  0.0272,  0.0230]],\n",
       "\n",
       "        [[-0.0510, -0.0283, -0.0287,  ..., -0.0005,  0.0516,  0.0473]],\n",
       "\n",
       "        [[-0.0462, -0.0207, -0.0120,  ...,  0.0096,  0.0374,  0.0389]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 2038,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1964,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 768])"
      ]
     },
     "execution_count": 1964,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1931,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = list(map(lambda x:\n",
    "                     tokenizer.encode(x, return_tensors='pt').squeeze(),\n",
    "                     ['Hello my name is Alex there man',\n",
    "                      'Alex is there, man man man.',\n",
    "                      'Alex is there, man man man.'\n",
    "                     ]\n",
    "                    )\n",
    ")\n",
    "\n",
    "max_sequence_len = 13\n",
    "\n",
    "seq_lens = list(map(len, input_ids))\n",
    "max_seq_len = max(seq_lens)\n",
    "attn_mask = list(map(lambda x: [1] * x + [0] * (max_seq_len - x), seq_lens))\n",
    "attn_mask = torch.tensor(attn_mask)\n",
    "input_ids = pad_sequence(input_ids, batch_first=True)\n",
    "batch_size, _ = input_ids.shape \n",
    "hidden = model.embeddings(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1932,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = query_layer(hidden)\n",
    "K = key_layer(hidden)\n",
    "V = value_layer(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1933,
   "metadata": {},
   "outputs": [],
   "source": [
    "o, _ = attention.forward(Q, K, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1930,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "old_o = o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1937,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 768])"
      ]
     },
     "execution_count": 1937,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_o.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1945,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7649)"
      ]
     },
     "execution_count": 1945,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o[0].isclose( old_o[0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1920,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 768])"
      ]
     },
     "execution_count": 1920,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1280,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_attn = torch.nn.MultiheadAttention(4, 2, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1288,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_mask = torch.tensor([\n",
    "    [[1, 0]],\n",
    "    [[1, 0]]\n",
    "])\n",
    "attn_mask = torch.log(attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1291,
   "metadata": {},
   "outputs": [],
   "source": [
    "o, _ = small_attn(q, k, v, attn_mask=attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.7300e-02,  8.3342e-05, -2.8119e-02,  3.0445e-02]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 1292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1311,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extended_attention_mask(attention_mask, dtype=torch.float32):\n",
    "    \"\"\"\n",
    "    Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n",
    "\n",
    "    Arguments:\n",
    "        attention_mask (:obj:`torch.Tensor`):\n",
    "            Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n",
    "        device: (:obj:`torch.device`):\n",
    "            The device of the input to the model.\n",
    "\n",
    "    Returns:\n",
    "        :obj:`torch.Tensor` The extended attention mask, with a the same dtype as :obj:`attention_mask.dtype`.\n",
    "    \"\"\"\n",
    "    # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "    # ourselves in which case we just need to make it broadcastable to all heads.\n",
    "    if attention_mask.dim() == 3:\n",
    "        extended_attention_mask = attention_mask[:, None, :, :]\n",
    "    elif attention_mask.dim() == 2:\n",
    "        # Provided a padding mask of dimensions [batch_size, seq_length]\n",
    "        # - make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "        extended_attention_mask = attention_mask[:, None, None, :]\n",
    "    extended_attention_mask = extended_attention_mask.to(dtype=dtype)  # fp16 compatibility\n",
    "    extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "    return extended_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 1, 7])"
      ]
     },
     "execution_count": 1321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_extended_attention_mask(attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
