{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import json\n",
    "import glob\n",
    "import re \n",
    "CLEANR = re.compile('<.*?>') \n",
    "\n",
    "\n",
    "def check_source_row(row):\n",
    "    head = row['head']\n",
    "    sent = row['sent']\n",
    "    if re.search('-\\d', head):\n",
    "        head = re.sub('-\\d', '', head)\n",
    "    \n",
    "    heads = head.split(';')\n",
    "    checks = []\n",
    "    for x in heads:\n",
    "        not_present = x.strip().lower() in ' '.join(sent).lower()\n",
    "        checks.append(not_present)\n",
    "        \n",
    "    return all(checks)\n",
    "\n",
    "\n",
    "def get_combined_df(annotated_fn, input_fn):\n",
    "    json_dat = json.load(open(annotated_fn))['data']\n",
    "    if isinstance(json_dat, dict) and 'row_data' in json_dat:\n",
    "        json_dat = json_dat['row_data']\n",
    "    annot_df = pd.DataFrame(json_dat)\n",
    "    annot_df = annot_df.applymap(lambda x: x['field_value'] if isinstance(x, dict) else x)\n",
    "    \n",
    "    input_dat = json.load(open(input_fn))['html_data']\n",
    "    input_df = pd.DataFrame(input_dat)\n",
    "\n",
    "    annot_df_with_input = (\n",
    "        input_df[['sent', 'sent_idx']]\n",
    "             .merge(annot_df[['row_idx', 'head', 'quote_type', 'source_type']], left_on='sent_idx', right_on='row_idx')\n",
    "             .drop(['row_idx', ], axis=1)\n",
    "#      .loc[lambda df: df['sent'].str.strip().str.len() > 1]\n",
    "    )\n",
    "    \n",
    "    return annot_df_with_input\n",
    "\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "    cleantext = re.sub(CLEANR, '', raw_html)\n",
    "    return cleantext\n",
    "\n",
    "####### \n",
    "input_data_files = glob.glob('../app/data/input_data/*/*')\n",
    "annotated_files = glob.glob('../app/data/output_data_affil-role/*/*')\n",
    "checked_files = glob.glob('../app/data/checked_data_affil-role/*/*')\n",
    "all_multiply_annotated_sentences = []\n",
    "all_sources = []\n",
    "\n",
    "for annot_fn in annotated_files:\n",
    "    doc_id = re.search('\\d+', annot_fn.split('/')[-1])[0]\n",
    "    input_fn = annot_fn.replace('output_', 'input_').replace('_affil-role', '').replace('annotated-', 'to-annotate-')\n",
    "    checked_cand = annot_fn.replace('output_', 'checked_').replace('annotated-', 'checked-')\n",
    "    if checked_cand in checked_files:\n",
    "        annot_fn = checked_cand\n",
    "        \n",
    "    annot_df_w_input = get_combined_df(annot_fn, input_fn)\n",
    "    annot_df_w_input['doc_id'] = doc_id\n",
    "    multiply_annotated = annot_df_w_input.loc[lambda df: df['head'].str.contains('-\\d') == True]\n",
    "    all_multiply_annotated_sentences.append(multiply_annotated)\n",
    "    all_sources.append(annot_df_w_input)\n",
    "\n",
    "all_sources_df = pd.concat(all_sources)\n",
    "all_sources_df['sent'] = all_sources_df['sent'].apply(cleanhtml)\n",
    "\n",
    "\n",
    "def cache_doc_tokens(input_doc, tokenizer, nlp):\n",
    "    doc_tokens_by_word = []\n",
    "    doc_tokens_by_sentence = []\n",
    "    for sent, _, _, _ in input_doc:\n",
    "        words = list(map(str, nlp(sent.strip())))\n",
    "        enc = []\n",
    "        for w_idx, w in enumerate(words):\n",
    "            if w_idx == 0:\n",
    "                add_prefix_space = False\n",
    "            else:\n",
    "                add_prefix_space = True\n",
    "            enc.append(\n",
    "                tokenizer.encode(w, add_special_tokens=False, add_prefix_space=add_prefix_space)\n",
    "            )\n",
    "        doc_tokens_by_word.append(enc)\n",
    "        tokenized_sentence = [tokenizer.bos_token_id] + [i for l in enc for i in l] + [tokenizer.eos_token_id]\n",
    "        doc_tokens_by_sentence.append(tokenized_sentence)\n",
    "        \n",
    "    doc_tokens = [i for l in doc_tokens_by_sentence for i in l]    \n",
    "    word_lens_by_sent = [list(map(len, x)) for x in doc_tokens_by_word]\n",
    "    word_lens_by_sent_cumsum = list(map(lambda x: np.cumsum([1] + x), word_lens_by_sent)) # we need a [1] offset\n",
    "                                                                                          # in the cumsum because there \n",
    "                                                                                          # is an extra bos token added.\n",
    "    sent_lens = list(map(len, doc_tokens_by_sentence))\n",
    "    sent_lens_cumsum = np.cumsum([0] + sent_lens)\n",
    "                         \n",
    "    return (\n",
    "        doc_tokens_by_word,\n",
    "        doc_tokens_by_sentence,\n",
    "        doc_tokens,\n",
    "        word_lens_by_sent_cumsum,\n",
    "        sent_lens,\n",
    "        sent_lens_cumsum\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20760]"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('hello', add_special_tokens=False, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[89]"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('there', add_special_tokens=False, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42891]"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('hello', add_special_tokens=False, add_prefix_space=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8585]"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('there', add_special_tokens=False, add_prefix_space=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' there'"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([89])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'there'"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([8585])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check which sources are not found in their directly-tagged sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfound = (\n",
    "    all_sources_df\n",
    "        .loc[lambda df: df['head'] != '']\n",
    "        .groupby(['doc_id', 'head'])\n",
    "        .aggregate(list)\n",
    "        .reset_index()\n",
    "        .loc[lambda df: ~df.apply(check_source_row, axis=1)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['journalist',\n",
       " 'passive-voice',\n",
       " 'Joseph R. Biden Jr.',\n",
       " 'Donald J. Trump',\n",
       " 'Elizabeth Warren',\n",
       " 'Trump',\n",
       " 'David Cameron',\n",
       " 'John J. Bates',\n",
       " 'Mahmoud Badr; Mohammed Abdel-Aziz; Hassan Shahin; Mai Wahba; Mohammed Heikal',\n",
       " 'Mai Wahba',\n",
       " 'Ron DeSantis',\n",
       " 'Trevor Haynes',\n",
       " 'Fu Cheng Qiu',\n",
       " 'Amy McGrath',\n",
       " 'Sputnik news agency',\n",
       " 'Mika Brzezinski',\n",
       " 'I. Launa',\n",
       " 'Donald Trump',\n",
       " ' Food and Drug Administration',\n",
       " 'Vladimir V. Putin',\n",
       " 'Austin Gilbert',\n",
       " 'Jonathan Segal',\n",
       " 'Mike Pence',\n",
       " 'Ryan Ellis',\n",
       " 'some men',\n",
       " 'David W. Eaton; Xuewei Bao',\n",
       " \"Kenya's top politicians\",\n",
       " \"mine's owners\",\n",
       " 'Mahmoud Badr',\n",
       " \"Paul Ryan's spokesman\",\n",
       " 'Hassan Shahin',\n",
       " 'bill-7',\n",
       " 'Giulio Regeni',\n",
       " 'epytians',\n",
       " 'Gov. Kate Brown',\n",
       " 'Oregon State Senate',\n",
       " \"Oregon's house\",\n",
       " 'Vermont Senate Judiciary Committee',\n",
       " \"bill's sponsors\",\n",
       " 'bill-2',\n",
       " 'bill-3',\n",
       " 'bill-4',\n",
       " 'bill-8',\n",
       " 'Cody Wilson ',\n",
       " 'Jerome H. Powell',\n",
       " 'Fire departement',\n",
       " 'Asia Development Bank',\n",
       " 'Mary Walsh',\n",
       " 'Ohio State University',\n",
       " 'campaign; Kamala Harris',\n",
       " 'David Briley',\n",
       " 'Barack Obama',\n",
       " 'Kelly Ayottee',\n",
       " 'Sebastian Gorka',\n",
       " 'Uber',\n",
       " 'Target spokeswoman',\n",
       " 'CVS representatives; Wegmans representatives',\n",
       " 'Starbucks spokesman',\n",
       " 'Nagashiki Shipping representative',\n",
       " 'The Mauritian environment ministry',\n",
       " 'Cary Roberts',\n",
       " 'Bill Gates',\n",
       " 'Corporate and Leisure Aviation',\n",
       " 'Glenn Garland',\n",
       " \"Benjamin Netanyahu's administration\",\n",
       " 'George W. Bush',\n",
       " 'CBS staff member',\n",
       " 'Democrats; Justice Department',\n",
       " 'Roy D. Blunt',\n",
       " 'Kayleigh McEnany',\n",
       " 'Lin Boqiang; Lauri Myllyvirta',\n",
       " 'Julia Louis - Dreyfus',\n",
       " 'Mark Reckless',\n",
       " 'voters',\n",
       " 'Jim Wallis',\n",
       " 'Daron Acemoglu',\n",
       " 'Nation',\n",
       " 'Salon',\n",
       " 'Chinese court',\n",
       " 'Calli Donohue',\n",
       " 'journalist ',\n",
       " 'Peter Fleming',\n",
       " 'chinese government',\n",
       " 'Ms. Taylor ’s lawyers',\n",
       " 'republican legislator',\n",
       " 'Ronaldo Pérez Garcia',\n",
       " 'Saúl Nicolas Pérez',\n",
       " 'State officials; local officials',\n",
       " 'parents',\n",
       " 'Bill Clinton',\n",
       " 'Australian government',\n",
       " ' Global Carbon Project',\n",
       " 'Paul Ryan',\n",
       " 'nytimes staff members',\n",
       " 'campaign finance data',\n",
       " 'tennessee regulations',\n",
       " 'texas regulations',\n",
       " 'images and videos',\n",
       " 'CNBC\\'s \"Squawk Box\"',\n",
       " 'Scott Pruitt',\n",
       " 'Dazia Lee',\n",
       " 'Almar Latour',\n",
       " 'Andy Ngo',\n",
       " \"coroner's report\",\n",
       " 'Jerry Armstrong',\n",
       " 'Kenneth Walker',\n",
       " 'Dan Rengering',\n",
       " 'Suthep Thaugsuban',\n",
       " 'Bernie Sanders',\n",
       " 'Bernie Sanders; Elizabeth Warren; Pete Buttigieg',\n",
       " 'Julian Castro',\n",
       " 'Kamala Harris; Amy Klobuchar',\n",
       " \"Mr. Biden's team\",\n",
       " 'Brazilian government',\n",
       " 'Jair Bolsonaro',\n",
       " 'Biden campaign ',\n",
       " 'Richard McDaniel',\n",
       " 'Georgina Mary Mace',\n",
       " 'Benjamin Netanyahu',\n",
       " 'Kathleen Hartnett White ’s',\n",
       " 'Francisco José Garzón Amo',\n",
       " 'Jill Stein',\n",
       " 'Associated Press',\n",
       " 'Council of Europe',\n",
       " 'Geert Wilders',\n",
       " 'Alberto Nisman',\n",
       " 'Cristina Fernández de Kirchner',\n",
       " 'Wilbur Ross',\n",
       " 'de Blasio; commissioner',\n",
       " 'iowa couple',\n",
       " 'article-2',\n",
       " 'Jerry Brown',\n",
       " 'Mark Harris',\n",
       " 'John Kasich',\n",
       " 'government officials; Asiana officials',\n",
       " 'Kris W. Kobach',\n",
       " 'Laura Kelly',\n",
       " 'Sam Brownback',\n",
       " ' Penpa Tsering',\n",
       " \"Dalai Lama's Secretary\",\n",
       " 'Lobsang Sangay',\n",
       " 'Hanan al-Khatib',\n",
       " \"Marianne Williamson's state director\",\n",
       " 'José Dominguez',\n",
       " 'Luiz Alberto Herrera; José Dominguez',\n",
       " 'Ed Miliband',\n",
       " 'Amarillo Globe-News',\n",
       " 'Billl Bunting',\n",
       " 'Hubei Shuanghuan',\n",
       " 'Bernstein analysts',\n",
       " 'International Aviation Safety Assessment Program',\n",
       " 'Andrew R. Wheeler',\n",
       " 'Doug Jones',\n",
       " 'Election experts',\n",
       " 'Jonathon Morgan',\n",
       " 'Cheryl Costantino; Edward McCall',\n",
       " 'proponents-2 ',\n",
       " 'customers',\n",
       " 'Jean - Marie Le Pen',\n",
       " 'Stephen K. Bannon',\n",
       " 'James B. Comey',\n",
       " 'Justice Department',\n",
       " 'Rod J. Rosenstein',\n",
       " 'Damon Winter',\n",
       " 'Donna Brazile',\n",
       " 'Paulo Guedes',\n",
       " 'opponents-2',\n",
       " ' Federal Aviation Administration',\n",
       " 'Boeing',\n",
       " 'Mark Herr',\n",
       " 'Steven A. Cohen',\n",
       " 'Joe Biden',\n",
       " 'Ed Gonzalez',\n",
       " 'stock market',\n",
       " 'Lamar Smith',\n",
       " 'Muhammad Naeem',\n",
       " 'proponents',\n",
       " 'opponents',\n",
       " 'colleague of Reid Hoffman',\n",
       " 'Lamar McKay ’s',\n",
       " 'Elizabeth M. Alderman',\n",
       " 'Julián Castro',\n",
       " 'Calvin Hunt',\n",
       " 'Timothy M. Dolan',\n",
       " 'François Hollande',\n",
       " \"members of the party's left wing\",\n",
       " 'officers',\n",
       " 'protestors',\n",
       " 'Facebook spokewoman',\n",
       " 'defendents lawyer',\n",
       " 'David Duckenfield',\n",
       " \"Julián Castro's campaign\",\n",
       " 'The Guardian',\n",
       " \"India 's environment minister; environment secretary\",\n",
       " 'Nicola Sturgeon ’s',\n",
       " 'David Wahl; Lysanna Anderson',\n",
       " 'Exxon Mobile',\n",
       " 'Federal Emergency Management Agency',\n",
       " 'Court documents',\n",
       " 'Michael T. Flynn',\n",
       " 'senior official']"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unfound['head'].value_counts().index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tag sources in document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_rk(seq, subseq):\n",
    "    n = len(seq)\n",
    "    m = len(subseq)\n",
    "    if seq[:m] == subseq:\n",
    "        return 0\n",
    "    hash_subseq = sum(hash(x) for x in subseq)  # compute hash\n",
    "    curr_hash = sum(hash(x) for x in seq[:m])  # compute hash\n",
    "    for i in range(1, n - m + 1):\n",
    "        curr_hash += hash(seq[i + m - 1]) - hash(seq[i - 1])   # update hash\n",
    "        if hash_subseq == curr_hash and seq[i:i + m] == subseq:\n",
    "            return i\n",
    "    return False\n",
    "\n",
    "def get_source_in_sentence(source_head, sentence):\n",
    "    if re.search('-\\d', source_head):\n",
    "        source_head = re.sub('-\\d', '', source_head)\n",
    "    if source_head in sentence:\n",
    "        return find_rk(sentence.split(), source_head.split())\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_source_offset(source_head, source_sents, doc_sents, tok_lens_by_sent, sent_lens):\n",
    "    # 1. iterate through source-related sentences first\n",
    "    found = False\n",
    "    for sentence, _, s_idx, _ in source_sents:\n",
    "        sentence = unidecode(sentence)\n",
    "        offset = get_source_in_sentence(source_head.lower(), sentence.lower())\n",
    "        if offset != -1:\n",
    "            sent_toks = tok_lens_by_sent[int(s_idx)]\n",
    "            return {\n",
    "                'source': source_head,\n",
    "                's_idx': s_idx,\n",
    "                'start_tok_idx': sent_lens[int(s_idx)] + sent_toks[offset],\n",
    "                'end_tok_idx': sent_lens[int(s_idx)] + sent_toks[offset + len(source_head.split())],\n",
    "                'doc_idx': doc_idx\n",
    "            }\n",
    "\n",
    "    # 2. iterate through the whole document if the source is not in the source sentences\n",
    "    for sentence, _, s_idx, _ in doc_sents:\n",
    "        sentence = unidecode(sentence)\n",
    "        offset = get_source_in_sentence(source_head.lower(), sentence.lower())\n",
    "        if offset != -1:\n",
    "            sent_toks = tok_lens_by_sent[int(s_idx)]\n",
    "            return {\n",
    "                'source': source_head, \n",
    "                's_idx': s_idx,\n",
    "                'start_tok_idx': sent_lens[int(s_idx)] + sent_toks[offset],\n",
    "                'end_tok_idx': sent_lens[int(s_idx)] + sent_toks[offset + len(source_head.split())],\n",
    "                'doc_idx': doc_idx\n",
    "            }\n",
    "        \n",
    "    # 3. nothing found, returning\n",
    "    return {\n",
    "        'source': source_head, \n",
    "        's_idx': -1,\n",
    "        'e_idx': -1,\n",
    "        'start_tok_idx': -1,\n",
    "        'end_tok_idx': -1,\n",
    "        'doc_idx': doc_idx\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_chunk_from_source_offset(source_offset_chunk, all_doc_tokens, sent_lens):\n",
    "    s_idx = int(source_offset_chunk['s_idx'])\n",
    "    \n",
    "    ## \n",
    "    training_chunk = {}\n",
    "    training_chunk['start_position'] = source_offset_chunk['start_tok_idx']\n",
    "    training_chunk['end_position'] = source_offset_chunk['end_tok_idx']\n",
    "    training_chunk['context'] = all_doc_tokens\n",
    "    sent_inds = []\n",
    "    for i, l in enumerate(sent_lens):\n",
    "        if i == s_idx:\n",
    "            sent_inds += [1] * l\n",
    "        else:\n",
    "            sent_inds += [0] * l \n",
    "    \n",
    "    training_chunk['sentence_indicator_tokens'] = sent_inds\n",
    "    return training_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b4b19b53efa44cebace5bc2f26304de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/296 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import csv, itertools\n",
    "from tqdm.auto import tqdm\n",
    "from unidecode import unidecode\n",
    "\n",
    "data_path = '../models_neural/quote_attribution/data/our-annotated-data__stage-2.tsv'\n",
    "split, data_chunk = [], []\n",
    "with open(data_path) as f:\n",
    "    csv_reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    csv_data = list(csv_reader)\n",
    "\n",
    "grouped = []\n",
    "for doc_idx, doc in itertools.groupby(csv_data, key=lambda x: x[3]):  # group by doc_id\n",
    "    sorted_doc = sorted(doc, key=lambda x: int(x[2]))  # sort by sent_id\n",
    "    sorted_doc = list(map(lambda x: [x[0].strip(), x[1], x[2], x[3]] , sorted_doc))\n",
    "    grouped.append((doc_idx, sorted_doc))\n",
    "\n",
    "### \n",
    "training_data = []\n",
    "for doc_idx, doc_to_group in tqdm(grouped, total=len(grouped)):\n",
    "    doc_to_group[0][0] = 'journalist passive-voice ' + doc_to_group[0][0]\n",
    "    (\n",
    "        doc_tok_by_word, \n",
    "        doc_tok_by_sent,\n",
    "        all_doc_tokens, \n",
    "        word_len_cumsum,\n",
    "        sent_lens,\n",
    "        sent_len_cumsum\n",
    "    ) = cache_doc_tokens(doc_to_group, tokenizer, nlp)    \n",
    "    \n",
    "    doc_to_group = sorted(doc_to_group, key=lambda x: x[1]) # sort by source\n",
    "    \n",
    "    for source_heads, source_sentences in itertools.groupby(doc_to_group, key=lambda x: x[1]):\n",
    "        if source_heads == 'None':\n",
    "            continue\n",
    "        \n",
    "        for source_head in source_heads.split(';'):\n",
    "            source_head = unidecode(source_head).strip()\n",
    "            source_chunk = find_source_offset(source_head, source_sentences, doc_to_group, word_len_cumsum, sent_len_cumsum)\n",
    "            training_chunk = generate_training_chunk_from_source_offset(source_chunk, all_doc_tokens, sent_lens)\n",
    "            training_data.append(training_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source': 'Food and Drug Administration',\n",
       "  's_idx': '11',\n",
       "  'start_tok_idx': 419,\n",
       "  'end_tok_idx': 423,\n",
       "  'doc_idx': '/test/716'},\n",
       " {'source': 'Elizabeth Holmes',\n",
       "  's_idx': '1',\n",
       "  'start_tok_idx': 60,\n",
       "  'end_tok_idx': 62,\n",
       "  'doc_idx': '/test/716'},\n",
       " {'source': 'Theranos',\n",
       "  's_idx': '0',\n",
       "  'start_tok_idx': 4,\n",
       "  'end_tok_idx': 5,\n",
       "  'doc_idx': '/test/716'},\n",
       " {'source': 'Wall Street Journal',\n",
       "  's_idx': '11',\n",
       "  'start_tok_idx': 394,\n",
       "  'end_tok_idx': 397,\n",
       "  'doc_idx': '/test/716'},\n",
       " {'source': 'company spokesman',\n",
       "  's_idx': '8',\n",
       "  'start_tok_idx': 294,\n",
       "  'end_tok_idx': 296,\n",
       "  'doc_idx': '/test/716'}]"
      ]
     },
     "execution_count": 658,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_to_word_offset[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Wall Street Journal'"
      ]
     },
     "execution_count": 650,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(doc_tok_by_sent[11][11:14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' company spokesman'"
      ]
     },
     "execution_count": 661,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(all_doc_tokens[294:296])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"deepset/roberta-base-squad2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "config= AutoConfig.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 632,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do checks!!! clean up these sources!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_word_offset_df = pd.DataFrame([i for s in source_to_word_offset for i in s])\n",
    "\n",
    "# this is likely because of rows with multiple sources (sep by ';')\n",
    "source_word_offset_df.assign(c=1).groupby(['doc_idx', 'source'])['c'].sum().loc[lambda s: s>1].head()\n",
    "\n",
    "## todo: go through and correct all these sources\n",
    "_ = source_word_offset_df.loc[lambda df: df['s_idx'] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_to_group = sorted(doc_to_group, key=lambda x: int(x[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_tok_by_word, doc_tok_by_sent, blank_toks_by_sent, all_doc_tokens, word_len_cumsum = cache_doc_tokens(\n",
    "    doc_to_group, tokenizer, nlp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 1,  3,  4,  5,  6,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
       "        20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
       "        37, 38, 39, 40, 41, 42, 43, 44, 45, 46]),\n",
       " array([ 1,  2,  3,  4,  5,  6,  7,  8, 11, 12, 13, 14, 15, 16, 17, 18, 21,\n",
       "        22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,\n",
       "        39, 40, 41, 42]),\n",
       " array([ 1,  2,  3,  4,  5,  6,  8,  9, 10, 11, 12, 14, 15, 16, 17, 20, 21,\n",
       "        22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]),\n",
       " array([ 1,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 16, 17, 18, 19,\n",
       "        20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
       "        37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]),\n",
       " array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 16, 17, 18, 19,\n",
       "        20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 32, 33, 34, 35, 36, 37, 38,\n",
       "        39, 40, 41, 42]),\n",
       " array([ 1,  2,  3,  4,  6,  7,  8, 10, 11, 13, 14, 15, 18, 19, 20, 21, 23,\n",
       "        24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 36, 39, 40, 41, 42, 43, 44,\n",
       "        45, 46, 47, 48, 49, 50]),\n",
       " array([1, 3]),\n",
       " array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14]),\n",
       " array([ 1,  2,  3,  4,  5,  7,  8,  9, 10, 11, 12, 13, 15, 16, 19, 20, 21]),\n",
       " array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 15, 16, 17, 18,\n",
       "        19, 20, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31]),\n",
       " array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 22, 23, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]),\n",
       " array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "        35, 36, 37, 38, 39, 40, 41]),\n",
       " array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 14, 15, 16, 17, 18,\n",
       "        19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 35, 36,\n",
       "        37, 38, 39, 40, 41, 42, 43, 44, 45, 46]),\n",
       " array([ 1,  3,  4,  5,  6,  7,  8,  9, 10]),\n",
       " array([ 1,  2,  3,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
       "        20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37,\n",
       "        38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55,\n",
       "        56, 57, 58, 59, 60, 61, 62, 63, 64]),\n",
       " array([1, 3]),\n",
       " array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 18, 20, 21, 22, 23,\n",
       "        24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36])]"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_len_cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Food and Drug Administration'"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'company spokesman',\n",
       " 's_idx': '8',\n",
       " 'start_word_idx': 1,\n",
       " 'end_word_idx': 3,\n",
       " 'doc_idx': '/test/716'}"
      ]
     },
     "execution_count": 579,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_to_word_offset[-1][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_len_cumsum[8][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_len_cumsum[8][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' company spokesman'"
      ]
     },
     "execution_count": 582,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(doc_tok_by_sent[8][2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make sure we have a good tokenizing pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('/Users/alex/.cache/torch/transformers/named-models/roberta-base-expanded-embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sents = list(map(lambda x: x[0].strip(), sorted_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_str = ' '.join(doc_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = doc_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1])"
      ]
     },
     "execution_count": 674,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones_like(torch.tensor([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ones_like(): argument 'input' (position 1) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-675-9d3ff1e3cb6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: ones_like(): argument 'input' (position 1) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "torch.ones_like([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
