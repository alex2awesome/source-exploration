{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import json\n",
    "import glob\n",
    "import re \n",
    "CLEANR = re.compile('<.*?>') \n",
    "\n",
    "\n",
    "def check_source_row(row):\n",
    "    head = row['head']\n",
    "    sent = row['sent']\n",
    "    if re.search('-\\d', head):\n",
    "        head = re.sub('-\\d', '', head)\n",
    "    \n",
    "    heads = head.split(';')\n",
    "    checks = []\n",
    "    for x in heads:\n",
    "        not_present = x.strip().lower() in ' '.join(sent).lower()\n",
    "        checks.append(not_present)\n",
    "        \n",
    "    return all(checks)\n",
    "\n",
    "\n",
    "def get_combined_df(annotated_fn, input_fn):\n",
    "    json_dat = json.load(open(annotated_fn))['data']\n",
    "    if isinstance(json_dat, dict) and 'row_data' in json_dat:\n",
    "        json_dat = json_dat['row_data']\n",
    "    annot_df = pd.DataFrame(json_dat)\n",
    "    annot_df = annot_df.applymap(lambda x: x['field_value'] if isinstance(x, dict) else x)\n",
    "    \n",
    "    input_dat = json.load(open(input_fn))['html_data']\n",
    "    input_df = pd.DataFrame(input_dat)\n",
    "\n",
    "    annot_df_with_input = (\n",
    "        input_df[['sent', 'sent_idx']]\n",
    "             .merge(annot_df[['row_idx', 'head', 'quote_type', 'source_type']], left_on='sent_idx', right_on='row_idx')\n",
    "             .drop(['row_idx', ], axis=1)\n",
    "#      .loc[lambda df: df['sent'].str.strip().str.len() > 1]\n",
    "    )\n",
    "    \n",
    "    return annot_df_with_input\n",
    "\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "    cleantext = re.sub(CLEANR, '', raw_html)\n",
    "    return cleantext\n",
    "\n",
    "####### \n",
    "input_data_files = glob.glob('../app/data/input_data/*/*')\n",
    "annotated_files = glob.glob('../app/data/output_data_affil-role/*/*')\n",
    "checked_files = glob.glob('../app/data/checked_data_affil-role/*/*')\n",
    "all_multiply_annotated_sentences = []\n",
    "all_sources = []\n",
    "\n",
    "for annot_fn in annotated_files:\n",
    "    doc_id = re.search('\\d+', annot_fn.split('/')[-1])[0]\n",
    "    input_fn = annot_fn.replace('output_', 'input_').replace('_affil-role', '').replace('annotated-', 'to-annotate-')\n",
    "    checked_cand = annot_fn.replace('output_', 'checked_').replace('annotated-', 'checked-')\n",
    "    if checked_cand in checked_files:\n",
    "        annot_fn = checked_cand\n",
    "        \n",
    "    annot_df_w_input = get_combined_df(annot_fn, input_fn)\n",
    "    annot_df_w_input['doc_id'] = doc_id\n",
    "    multiply_annotated = annot_df_w_input.loc[lambda df: df['head'].str.contains('-\\d') == True]\n",
    "    all_multiply_annotated_sentences.append(multiply_annotated)\n",
    "    all_sources.append(annot_df_w_input)\n",
    "\n",
    "all_sources_df = pd.concat(all_sources)\n",
    "all_sources_df['sent'] = all_sources_df['sent'].apply(cleanhtml)\n",
    "\n",
    "\n",
    "def cache_doc_tokens(input_doc, tokenizer, nlp):\n",
    "    doc_tokens_by_word = []\n",
    "    doc_tokens_by_sentence = []\n",
    "    for sent, _, _, _ in input_doc:\n",
    "        words = list(map(str, nlp(sent.strip())))\n",
    "        enc = []\n",
    "        for w_idx, w in enumerate(words):\n",
    "            if w_idx == 0:\n",
    "                add_prefix_space = False\n",
    "            else:\n",
    "                add_prefix_space = True\n",
    "            enc.append(\n",
    "                tokenizer.encode(w, add_special_tokens=False, add_prefix_space=add_prefix_space)\n",
    "            )\n",
    "        doc_tokens_by_word.append(enc)\n",
    "        tokenized_sentence = [tokenizer.bos_token_id] + [i for l in enc for i in l] + [tokenizer.eos_token_id]\n",
    "        doc_tokens_by_sentence.append(tokenized_sentence)\n",
    "        \n",
    "    doc_tokens = [i for l in doc_tokens_by_sentence for i in l]    \n",
    "    word_lens_by_sent = [list(map(len, x)) for x in doc_tokens_by_word]\n",
    "    word_lens_by_sent_cumsum = list(map(lambda x: np.cumsum([1] + x), word_lens_by_sent)) # we need a [1] offset\n",
    "                                                                                          # in the cumsum because there \n",
    "                                                                                          # is an extra bos token added.\n",
    "    sent_lens = list(map(len, doc_tokens_by_sentence))\n",
    "    sent_lens_cumsum = np.cumsum([0] + sent_lens)\n",
    "                         \n",
    "    return (\n",
    "        doc_tokens_by_word,\n",
    "        doc_tokens_by_sentence,\n",
    "        doc_tokens,\n",
    "        word_lens_by_sent_cumsum,\n",
    "        sent_lens,\n",
    "        sent_lens_cumsum\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20760]"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('hello', add_special_tokens=False, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[89]"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('there', add_special_tokens=False, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42891]"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('hello', add_special_tokens=False, add_prefix_space=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8585]"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('there', add_special_tokens=False, add_prefix_space=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' there'"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([89])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'there'"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([8585])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check which sources are not found in their directly-tagged sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfound = (\n",
    "    all_sources_df\n",
    "        .loc[lambda df: df['head'] != '']\n",
    "        .groupby(['doc_id', 'head'])\n",
    "        .aggregate(list)\n",
    "        .reset_index()\n",
    "        .loc[lambda df: ~df.apply(check_source_row, axis=1)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['journalist',\n",
       " 'passive-voice',\n",
       " 'Joseph R. Biden Jr.',\n",
       " 'Donald J. Trump',\n",
       " 'Elizabeth Warren',\n",
       " 'Trump',\n",
       " 'David Cameron',\n",
       " 'John J. Bates',\n",
       " 'Mahmoud Badr; Mohammed Abdel-Aziz; Hassan Shahin; Mai Wahba; Mohammed Heikal',\n",
       " 'Mai Wahba',\n",
       " 'Ron DeSantis',\n",
       " 'Trevor Haynes',\n",
       " 'Fu Cheng Qiu',\n",
       " 'Amy McGrath',\n",
       " 'Sputnik news agency',\n",
       " 'Mika Brzezinski',\n",
       " 'I. Launa',\n",
       " 'Donald Trump',\n",
       " ' Food and Drug Administration',\n",
       " 'Vladimir V. Putin',\n",
       " 'Austin Gilbert',\n",
       " 'Jonathan Segal',\n",
       " 'Mike Pence',\n",
       " 'Ryan Ellis',\n",
       " 'some men',\n",
       " 'David W. Eaton; Xuewei Bao',\n",
       " \"Kenya's top politicians\",\n",
       " \"mine's owners\",\n",
       " 'Mahmoud Badr',\n",
       " \"Paul Ryan's spokesman\",\n",
       " 'Hassan Shahin',\n",
       " 'bill-7',\n",
       " 'Giulio Regeni',\n",
       " 'epytians',\n",
       " 'Gov. Kate Brown',\n",
       " 'Oregon State Senate',\n",
       " \"Oregon's house\",\n",
       " 'Vermont Senate Judiciary Committee',\n",
       " \"bill's sponsors\",\n",
       " 'bill-2',\n",
       " 'bill-3',\n",
       " 'bill-4',\n",
       " 'bill-8',\n",
       " 'Cody Wilson ',\n",
       " 'Jerome H. Powell',\n",
       " 'Fire departement',\n",
       " 'Asia Development Bank',\n",
       " 'Mary Walsh',\n",
       " 'Ohio State University',\n",
       " 'campaign; Kamala Harris',\n",
       " 'David Briley',\n",
       " 'Barack Obama',\n",
       " 'Kelly Ayottee',\n",
       " 'Sebastian Gorka',\n",
       " 'Uber',\n",
       " 'Target spokeswoman',\n",
       " 'CVS representatives; Wegmans representatives',\n",
       " 'Starbucks spokesman',\n",
       " 'Nagashiki Shipping representative',\n",
       " 'The Mauritian environment ministry',\n",
       " 'Cary Roberts',\n",
       " 'Bill Gates',\n",
       " 'Corporate and Leisure Aviation',\n",
       " 'Glenn Garland',\n",
       " \"Benjamin Netanyahu's administration\",\n",
       " 'George W. Bush',\n",
       " 'CBS staff member',\n",
       " 'Democrats; Justice Department',\n",
       " 'Roy D. Blunt',\n",
       " 'Kayleigh McEnany',\n",
       " 'Lin Boqiang; Lauri Myllyvirta',\n",
       " 'Julia Louis - Dreyfus',\n",
       " 'Mark Reckless',\n",
       " 'voters',\n",
       " 'Jim Wallis',\n",
       " 'Daron Acemoglu',\n",
       " 'Nation',\n",
       " 'Salon',\n",
       " 'Chinese court',\n",
       " 'Calli Donohue',\n",
       " 'journalist ',\n",
       " 'Peter Fleming',\n",
       " 'chinese government',\n",
       " 'Ms. Taylor ’s lawyers',\n",
       " 'republican legislator',\n",
       " 'Ronaldo Pérez Garcia',\n",
       " 'Saúl Nicolas Pérez',\n",
       " 'State officials; local officials',\n",
       " 'parents',\n",
       " 'Bill Clinton',\n",
       " 'Australian government',\n",
       " ' Global Carbon Project',\n",
       " 'Paul Ryan',\n",
       " 'nytimes staff members',\n",
       " 'campaign finance data',\n",
       " 'tennessee regulations',\n",
       " 'texas regulations',\n",
       " 'images and videos',\n",
       " 'CNBC\\'s \"Squawk Box\"',\n",
       " 'Scott Pruitt',\n",
       " 'Dazia Lee',\n",
       " 'Almar Latour',\n",
       " 'Andy Ngo',\n",
       " \"coroner's report\",\n",
       " 'Jerry Armstrong',\n",
       " 'Kenneth Walker',\n",
       " 'Dan Rengering',\n",
       " 'Suthep Thaugsuban',\n",
       " 'Bernie Sanders',\n",
       " 'Bernie Sanders; Elizabeth Warren; Pete Buttigieg',\n",
       " 'Julian Castro',\n",
       " 'Kamala Harris; Amy Klobuchar',\n",
       " \"Mr. Biden's team\",\n",
       " 'Brazilian government',\n",
       " 'Jair Bolsonaro',\n",
       " 'Biden campaign ',\n",
       " 'Richard McDaniel',\n",
       " 'Georgina Mary Mace',\n",
       " 'Benjamin Netanyahu',\n",
       " 'Kathleen Hartnett White ’s',\n",
       " 'Francisco José Garzón Amo',\n",
       " 'Jill Stein',\n",
       " 'Associated Press',\n",
       " 'Council of Europe',\n",
       " 'Geert Wilders',\n",
       " 'Alberto Nisman',\n",
       " 'Cristina Fernández de Kirchner',\n",
       " 'Wilbur Ross',\n",
       " 'de Blasio; commissioner',\n",
       " 'iowa couple',\n",
       " 'article-2',\n",
       " 'Jerry Brown',\n",
       " 'Mark Harris',\n",
       " 'John Kasich',\n",
       " 'government officials; Asiana officials',\n",
       " 'Kris W. Kobach',\n",
       " 'Laura Kelly',\n",
       " 'Sam Brownback',\n",
       " ' Penpa Tsering',\n",
       " \"Dalai Lama's Secretary\",\n",
       " 'Lobsang Sangay',\n",
       " 'Hanan al-Khatib',\n",
       " \"Marianne Williamson's state director\",\n",
       " 'José Dominguez',\n",
       " 'Luiz Alberto Herrera; José Dominguez',\n",
       " 'Ed Miliband',\n",
       " 'Amarillo Globe-News',\n",
       " 'Billl Bunting',\n",
       " 'Hubei Shuanghuan',\n",
       " 'Bernstein analysts',\n",
       " 'International Aviation Safety Assessment Program',\n",
       " 'Andrew R. Wheeler',\n",
       " 'Doug Jones',\n",
       " 'Election experts',\n",
       " 'Jonathon Morgan',\n",
       " 'Cheryl Costantino; Edward McCall',\n",
       " 'proponents-2 ',\n",
       " 'customers',\n",
       " 'Jean - Marie Le Pen',\n",
       " 'Stephen K. Bannon',\n",
       " 'James B. Comey',\n",
       " 'Justice Department',\n",
       " 'Rod J. Rosenstein',\n",
       " 'Damon Winter',\n",
       " 'Donna Brazile',\n",
       " 'Paulo Guedes',\n",
       " 'opponents-2',\n",
       " ' Federal Aviation Administration',\n",
       " 'Boeing',\n",
       " 'Mark Herr',\n",
       " 'Steven A. Cohen',\n",
       " 'Joe Biden',\n",
       " 'Ed Gonzalez',\n",
       " 'stock market',\n",
       " 'Lamar Smith',\n",
       " 'Muhammad Naeem',\n",
       " 'proponents',\n",
       " 'opponents',\n",
       " 'colleague of Reid Hoffman',\n",
       " 'Lamar McKay ’s',\n",
       " 'Elizabeth M. Alderman',\n",
       " 'Julián Castro',\n",
       " 'Calvin Hunt',\n",
       " 'Timothy M. Dolan',\n",
       " 'François Hollande',\n",
       " \"members of the party's left wing\",\n",
       " 'officers',\n",
       " 'protestors',\n",
       " 'Facebook spokewoman',\n",
       " 'defendents lawyer',\n",
       " 'David Duckenfield',\n",
       " \"Julián Castro's campaign\",\n",
       " 'The Guardian',\n",
       " \"India 's environment minister; environment secretary\",\n",
       " 'Nicola Sturgeon ’s',\n",
       " 'David Wahl; Lysanna Anderson',\n",
       " 'Exxon Mobile',\n",
       " 'Federal Emergency Management Agency',\n",
       " 'Court documents',\n",
       " 'Michael T. Flynn',\n",
       " 'senior official']"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unfound['head'].value_counts().index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tag sources in document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_rk(seq, subseq):\n",
    "    n = len(seq)\n",
    "    m = len(subseq)\n",
    "    if seq[:m] == subseq:\n",
    "        return 0\n",
    "    hash_subseq = sum(hash(x) for x in subseq)  # compute hash\n",
    "    curr_hash = sum(hash(x) for x in seq[:m])  # compute hash\n",
    "    for i in range(1, n - m + 1):\n",
    "        curr_hash += hash(seq[i + m - 1]) - hash(seq[i - 1])   # update hash\n",
    "        if hash_subseq == curr_hash and seq[i:i + m] == subseq:\n",
    "            return i\n",
    "    return False\n",
    "\n",
    "def get_source_in_sentence(source_head, sentence):\n",
    "    if re.search('-\\d', source_head):\n",
    "        source_head = re.sub('-\\d', '', source_head)\n",
    "    if source_head in sentence:\n",
    "        return find_rk(sentence.split(), source_head.split())\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_source_offset(source_head, source_sents, doc_sents, tok_lens_by_sent, sent_lens):\n",
    "    # 1. iterate through source-related sentences first\n",
    "    found = False\n",
    "    for sentence, _, s_idx, _ in source_sents:\n",
    "        sentence = unidecode(sentence)\n",
    "        offset = get_source_in_sentence(source_head.lower(), sentence.lower())\n",
    "        if offset != -1:\n",
    "            sent_toks = tok_lens_by_sent[int(s_idx)]\n",
    "            return {\n",
    "                'source': source_head,\n",
    "                's_idx': s_idx,\n",
    "                'start_tok_idx': sent_lens[int(s_idx)] + sent_toks[offset],\n",
    "                'end_tok_idx': sent_lens[int(s_idx)] + sent_toks[offset + len(source_head.split())],\n",
    "                'doc_idx': doc_idx\n",
    "            }\n",
    "\n",
    "    # 2. iterate through the whole document if the source is not in the source sentences\n",
    "    for sentence, _, s_idx, _ in doc_sents:\n",
    "        sentence = unidecode(sentence)\n",
    "        offset = get_source_in_sentence(source_head.lower(), sentence.lower())\n",
    "        if offset != -1:\n",
    "            sent_toks = tok_lens_by_sent[int(s_idx)]\n",
    "            return {\n",
    "                'source': source_head, \n",
    "                's_idx': s_idx,\n",
    "                'start_tok_idx': sent_lens[int(s_idx)] + sent_toks[offset],\n",
    "                'end_tok_idx': sent_lens[int(s_idx)] + sent_toks[offset + len(source_head.split())],\n",
    "                'doc_idx': doc_idx\n",
    "            }\n",
    "        \n",
    "    # 3. nothing found, returning\n",
    "    return {\n",
    "        'source': source_head, \n",
    "        's_idx': -1,\n",
    "        'e_idx': -1,\n",
    "        'start_tok_idx': -1,\n",
    "        'end_tok_idx': -1,\n",
    "        'doc_idx': doc_idx\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_chunk_from_source_offset(source_offset_chunk, all_doc_tokens, sent_lens):\n",
    "    s_idx = int(source_offset_chunk['s_idx'])\n",
    "    \n",
    "    ## \n",
    "    training_chunk = {}\n",
    "    training_chunk['start_position'] = source_offset_chunk['start_tok_idx']\n",
    "    training_chunk['end_position'] = source_offset_chunk['end_tok_idx']\n",
    "    training_chunk['context'] = all_doc_tokens\n",
    "    sent_inds = []\n",
    "    for i, l in enumerate(sent_lens):\n",
    "        if i == s_idx:\n",
    "            sent_inds += [1] * l\n",
    "        else:\n",
    "            sent_inds += [0] * l \n",
    "    \n",
    "    training_chunk['sentence_indicator_tokens'] = sent_inds\n",
    "    return training_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b4b19b53efa44cebace5bc2f26304de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/296 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import csv, itertools\n",
    "from tqdm.auto import tqdm\n",
    "from unidecode import unidecode\n",
    "\n",
    "data_path = '../models_neural/quote_attribution/data/our-annotated-data__stage-2.tsv'\n",
    "split, data_chunk = [], []\n",
    "with open(data_path) as f:\n",
    "    csv_reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    csv_data = list(csv_reader)\n",
    "\n",
    "grouped = []\n",
    "for doc_idx, doc in itertools.groupby(csv_data, key=lambda x: x[3]):  # group by doc_id\n",
    "    sorted_doc = sorted(doc, key=lambda x: int(x[2]))  # sort by sent_id\n",
    "    sorted_doc = list(map(lambda x: [x[0].strip(), x[1], x[2], x[3]] , sorted_doc))\n",
    "    grouped.append((doc_idx, sorted_doc))\n",
    "\n",
    "### \n",
    "training_data = []\n",
    "for doc_idx, doc_to_group in tqdm(grouped, total=len(grouped)):\n",
    "    doc_to_group[0][0] = 'journalist passive-voice ' + doc_to_group[0][0]\n",
    "    (\n",
    "        doc_tok_by_word, \n",
    "        doc_tok_by_sent,\n",
    "        all_doc_tokens, \n",
    "        word_len_cumsum,\n",
    "        sent_lens,\n",
    "        sent_len_cumsum\n",
    "    ) = cache_doc_tokens(doc_to_group, tokenizer, nlp)    \n",
    "    \n",
    "    doc_to_group = sorted(doc_to_group, key=lambda x: x[1]) # sort by source\n",
    "    \n",
    "    for source_heads, source_sentences in itertools.groupby(doc_to_group, key=lambda x: x[1]):\n",
    "        if source_heads == 'None':\n",
    "            continue\n",
    "        \n",
    "        for source_head in source_heads.split(';'):\n",
    "            source_head = unidecode(source_head).strip()\n",
    "            source_chunk = find_source_offset(source_head, source_sentences, doc_to_group, word_len_cumsum, sent_len_cumsum)\n",
    "            training_chunk = generate_training_chunk_from_source_offset(source_chunk, all_doc_tokens, sent_lens)\n",
    "            training_data.append(training_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source': 'Food and Drug Administration',\n",
       "  's_idx': '11',\n",
       "  'start_tok_idx': 419,\n",
       "  'end_tok_idx': 423,\n",
       "  'doc_idx': '/test/716'},\n",
       " {'source': 'Elizabeth Holmes',\n",
       "  's_idx': '1',\n",
       "  'start_tok_idx': 60,\n",
       "  'end_tok_idx': 62,\n",
       "  'doc_idx': '/test/716'},\n",
       " {'source': 'Theranos',\n",
       "  's_idx': '0',\n",
       "  'start_tok_idx': 4,\n",
       "  'end_tok_idx': 5,\n",
       "  'doc_idx': '/test/716'},\n",
       " {'source': 'Wall Street Journal',\n",
       "  's_idx': '11',\n",
       "  'start_tok_idx': 394,\n",
       "  'end_tok_idx': 397,\n",
       "  'doc_idx': '/test/716'},\n",
       " {'source': 'company spokesman',\n",
       "  's_idx': '8',\n",
       "  'start_tok_idx': 294,\n",
       "  'end_tok_idx': 296,\n",
       "  'doc_idx': '/test/716'}]"
      ]
     },
     "execution_count": 658,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_to_word_offset[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Wall Street Journal'"
      ]
     },
     "execution_count": 650,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(doc_tok_by_sent[11][11:14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' company spokesman'"
      ]
     },
     "execution_count": 661,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(all_doc_tokens[294:296])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "train_df = pd.read_csv('../models_neural/quote_attribution/data/our-annotated-data__stage-2.tsv', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13695</th>\n",
       "      <td>Asked why it had taken so long for the structures to be demolished , an employee at Social , a nearby restaurant , who declined to give his name , scrunched up his face and laughed , saying it was bureaucracy .</td>\n",
       "      <td>employee</td>\n",
       "      <td>45</td>\n",
       "      <td>/train/658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15437</th>\n",
       "      <td>One involved a manager groping employees ’ breasts .</td>\n",
       "      <td>employee</td>\n",
       "      <td>36</td>\n",
       "      <td>/train/741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                          0  \\\n",
       "13695  Asked why it had taken so long for the structures to be demolished , an employee at Social , a nearby restaurant , who declined to give his name , scrunched up his face and laughed , saying it was bureaucracy .     \n",
       "15437                                                                                                                                                                  One involved a manager groping employees ’ breasts .   \n",
       "\n",
       "              1   2           3  \n",
       "13695  employee  45  /train/658  \n",
       "15437  employee  36  /train/741  "
      ]
     },
     "execution_count": 687,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.loc[lambda df: df[1] == 'employee']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"deepset/roberta-base-squad2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "config= AutoConfig.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 632,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do checks!!! clean up these sources!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_word_offset_df = pd.DataFrame([i for s in source_to_word_offset for i in s])\n",
    "\n",
    "# this is likely because of rows with multiple sources (sep by ';')\n",
    "source_word_offset_df.assign(c=1).groupby(['doc_idx', 'source'])['c'].sum().loc[lambda s: s>1].head()\n",
    "\n",
    "## todo: go through and correct all these sources\n",
    "_ = source_word_offset_df.loc[lambda df: df['s_idx'] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_to_group = sorted(doc_to_group, key=lambda x: int(x[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_tok_by_word, doc_tok_by_sent, blank_toks_by_sent, all_doc_tokens, word_len_cumsum = cache_doc_tokens(\n",
    "    doc_to_group, tokenizer, nlp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 1,  3,  4,  5,  6,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
       "        20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
       "        37, 38, 39, 40, 41, 42, 43, 44, 45, 46]),\n",
       " array([ 1,  2,  3,  4,  5,  6,  7,  8, 11, 12, 13, 14, 15, 16, 17, 18, 21,\n",
       "        22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,\n",
       "        39, 40, 41, 42]),\n",
       " array([ 1,  2,  3,  4,  5,  6,  8,  9, 10, 11, 12, 14, 15, 16, 17, 20, 21,\n",
       "        22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]),\n",
       " array([ 1,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 16, 17, 18, 19,\n",
       "        20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
       "        37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]),\n",
       " array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 16, 17, 18, 19,\n",
       "        20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 32, 33, 34, 35, 36, 37, 38,\n",
       "        39, 40, 41, 42]),\n",
       " array([ 1,  2,  3,  4,  6,  7,  8, 10, 11, 13, 14, 15, 18, 19, 20, 21, 23,\n",
       "        24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 36, 39, 40, 41, 42, 43, 44,\n",
       "        45, 46, 47, 48, 49, 50]),\n",
       " array([1, 3]),\n",
       " array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14]),\n",
       " array([ 1,  2,  3,  4,  5,  7,  8,  9, 10, 11, 12, 13, 15, 16, 19, 20, 21]),\n",
       " array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 15, 16, 17, 18,\n",
       "        19, 20, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31]),\n",
       " array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 22, 23, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]),\n",
       " array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "        35, 36, 37, 38, 39, 40, 41]),\n",
       " array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 14, 15, 16, 17, 18,\n",
       "        19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 35, 36,\n",
       "        37, 38, 39, 40, 41, 42, 43, 44, 45, 46]),\n",
       " array([ 1,  3,  4,  5,  6,  7,  8,  9, 10]),\n",
       " array([ 1,  2,  3,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
       "        20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37,\n",
       "        38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55,\n",
       "        56, 57, 58, 59, 60, 61, 62, 63, 64]),\n",
       " array([1, 3]),\n",
       " array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 18, 20, 21, 22, 23,\n",
       "        24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36])]"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_len_cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Food and Drug Administration'"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'company spokesman',\n",
       " 's_idx': '8',\n",
       " 'start_word_idx': 1,\n",
       " 'end_word_idx': 3,\n",
       " 'doc_idx': '/test/716'}"
      ]
     },
     "execution_count": 579,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_to_word_offset[-1][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_len_cumsum[8][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_len_cumsum[8][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' company spokesman'"
      ]
     },
     "execution_count": 582,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(doc_tok_by_sent[8][2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make sure we have a good tokenizing pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('/Users/alex/.cache/torch/transformers/named-models/roberta-base-expanded-embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sents = list(map(lambda x: x[0].strip(), sorted_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_str = ' '.join(doc_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = doc_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1])"
      ]
     },
     "execution_count": 674,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones_like(torch.tensor([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ones_like(): argument 'input' (position 1) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-675-9d3ff1e3cb6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: ones_like(): argument 'input' (position 1) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "torch.ones_like([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/alex/opt/anaconda3/lib/python3.7/site-packages/en_core_web_lg/en_core_web_lg-3.3.0')"
      ]
     },
     "execution_count": 677,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "model = spacy.load(\"en_core_web_lg\")\n",
    "model._path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_server = \"\"\"{\n",
    "  \"_name_or_path\": \"/Users/alex/.cache/torch/transformers/named-models/roberta-base/\",\n",
    "  \"accumulate_grad_batches\": 1,\n",
    "  \"adam_beta1\": 0.9,\n",
    "  \"adam_beta2\": 0.999,\n",
    "  \"adam_epsilon\": 1e-08,\n",
    "  \"add_cross_attention\": false,\n",
    "  \"architectures\": [\n",
    "    \"RobertaModel\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.1,\n",
    "  \"bad_words_ids\": null,\n",
    "  \"batch_size\": 1,\n",
    "  \"bos_token_id\": 0,\n",
    "  \"chunk_size_feed_forward\": 0,\n",
    "  \"classifier_dropout\": null,\n",
    "  \"concat_headline\": false,\n",
    "  \"cross_attention_hidden_size\": null,\n",
    "  \"decoder_start_token_id\": null,\n",
    "  \"diversity_penalty\": 0.0,\n",
    "  \"do_sample\": false,\n",
    "  \"doc_embed_arithmetic\": false,\n",
    "  \"downsample_negative_data\": 1.0,\n",
    "  \"dropout\": 0.1,\n",
    "  \"early_stopping\": false,\n",
    "  \"embedding_dim\": 768,\n",
    "  \"encoder_no_repeat_ngram_size\": 0,\n",
    "  \"env\": \"local\",\n",
    "  \"eos_token_id\": 2,\n",
    "  \"eval_data_file\": null,\n",
    "  \"experiment\": \"roberta_classification\",\n",
    "  \"finetuning_task\": null,\n",
    "  \"forced_bos_token_id\": null,\n",
    "  \"forced_eos_token_id\": null,\n",
    "  \"freeze_embedding_layer\": false,\n",
    "  \"freeze_encoder_layers\": [\n",
    "    0,\n",
    "    1,\n",
    "    2,\n",
    "    3,\n",
    "    4,\n",
    "    5,\n",
    "    6,\n",
    "    7,\n",
    "    8,\n",
    "    9\n",
    "  ],\n",
    "  \"freeze_pooling_layer\": false,\n",
    "  \"freeze_transformer\": false,\n",
    "  \"gradient_checkpointing\": false,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dim\": 512,\n",
    "  \"hidden_dropout_prob\": 0.1,\n",
    "  \"hidden_size\": 768,\n",
    "  \"id2label\": {\n",
    "    \"0\": \"LABEL_0\",\n",
    "    \"1\": \"LABEL_1\"\n",
    "  },\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 3072,\n",
    "  \"is_decoder\": false,\n",
    "  \"is_encoder_decoder\": false,\n",
    "  \"label2id\": {\n",
    "    \"LABEL_0\": 0,\n",
    "    \"LABEL_1\": 1\n",
    "  },\n",
    "  \"layer_norm_eps\": 1e-05,\n",
    "  \"learning_rate\": 0.0001,\n",
    "  \"length_penalty\": 1.0,\n",
    "  \"local\": false,\n",
    "  \"local_rank\": -1,\n",
    "  \"log_all_metrics\": false,\n",
    "  \"loss_weighting\": null,\n",
    "  \"max_grad_norm\": 0,\n",
    "  \"max_length\": 20,\n",
    "  \"max_length_seq\": 200,\n",
    "  \"max_num_sent_positions\": 40,\n",
    "  \"max_num_sentences\": 100,\n",
    "  \"max_num_word_positions\": 2048,\n",
    "  \"max_position_embeddings\": 2048,\n",
    "  \"min_length\": 0,\n",
    "  \"model_type\": \"\",\n",
    "  \"no_repeat_ngram_size\": 0,\n",
    "  \"notes\": \"Stage 2: Quote Attribution. Classification. Our dataset only.\",\n",
    "  \"num_attention_heads\": 12,\n",
    "  \"num_beam_groups\": 1,\n",
    "  \"num_beams\": 1,\n",
    "  \"num_dataloader_cpus\": 10,\n",
    "  \"num_documents\": null,\n",
    "  \"num_gpus\": 1,\n",
    "  \"num_hidden_layers\": 12,\n",
    "  \"num_nodes\": 1,\n",
    "  \"num_output_tags\": 1,\n",
    "  \"num_return_sequences\": 1,\n",
    "  \"num_token_types\": 3,\n",
    "  \"num_train_epochs\": 3,\n",
    "  \"output_attentions\": false,\n",
    "  \"output_hidden_states\": false,\n",
    "  \"output_scores\": false,\n",
    "  \"pad_token_id\": 1,\n",
    "  \"position_embedding_type\": \"absolute\",\n",
    "  \"prefix\": null,\n",
    "  \"pretrained_lm_model_path\": null,\n",
    "  \"pretrained_model_path\": \"./roberta-base-expanded-embeddings\",\n",
    "  \"problem_type\": null,\n",
    "  \"pruned_heads\": {},\n",
    "  \"remove_invalid_values\": false,\n",
    "  \"repetition_penalty\": 1.0,\n",
    "  \"return_dict\": true,\n",
    "  \"return_dict_in_generate\": false,\n",
    "  \"sentence_contextualizer_model_type\": \"roberta\",\n",
    "  \"sentence_embedding_method\": \"attention\",\n",
    "  \"sep_token_id\": null,\n",
    "  \"shuffle_data\": false,\n",
    "  \"sinusoidal_embeddings\": false,\n",
    "  \"spacy_model_file\": \"spacy/en_core_web_lg\",\n",
    "  \"split_type\": \"random\",\n",
    "  \"task_specific_params\": null,\n",
    "  \"temperature\": 1.0,\n",
    "  \"tie_encoder_decoder\": false,\n",
    "  \"tie_word_embeddings\": true,\n",
    "  \"tokenizer_class\": null,\n",
    "  \"top_k\": 50,\n",
    "  \"top_p\": 1.0,\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"torchscript\": false,\n",
    "  \"train_data_file\": \"data/our-annotated-data__stage-2.tsv\",\n",
    "  \"train_on_none\": false,\n",
    "  \"transformer_attention_probs_dropout_prob\": 0.1,\n",
    "  \"transformer_hidden_dropout_prob\": 0.1,\n",
    "  \"transformers_version\": \"4.12.2\",\n",
    "  \"type_vocab_size\": 1,\n",
    "  \"use_bfloat16\": false,\n",
    "  \"use_cache\": false,\n",
    "  \"use_cpu\": false,\n",
    "  \"use_deepspeed\": false,\n",
    "  \"use_doc_emb\": false,\n",
    "  \"use_headline\": false,\n",
    "  \"use_headline_embs\": false,\n",
    "  \"use_positional\": false,\n",
    "  \"vocab_size\": 50265,\n",
    "  \"warmup_steps\": 0,\n",
    "  \"weight_decay\": 0\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_local = \"\"\"\n",
    "{\n",
    "  \"_name_or_path\": \"/Users/alex/.cache/torch/transformers/named-models/roberta-base/\",\n",
    "  \"accumulate_grad_batches\": 1,\n",
    "  \"adam_beta1\": 0.9,\n",
    "  \"adam_beta2\": 0.999,\n",
    "  \"adam_epsilon\": 1e-08,\n",
    "  \"add_cross_attention\": false,\n",
    "  \"architectures\": [\n",
    "    \"RobertaModel\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.1,\n",
    "  \"bad_words_ids\": null,\n",
    "  \"batch_size\": 1,\n",
    "  \"bos_token_id\": 0,\n",
    "  \"chunk_size_feed_forward\": 0,\n",
    "  \"classifier_dropout\": null,\n",
    "  \"concat_headline\": false,\n",
    "  \"cross_attention_hidden_size\": null,\n",
    "  \"decoder_start_token_id\": null,\n",
    "  \"diversity_penalty\": 0.0,\n",
    "  \"do_sample\": false,\n",
    "  \"doc_embed_arithmetic\": false,\n",
    "  \"downsample_negative_data\": 0.1,\n",
    "  \"dropout\": 0.1,\n",
    "  \"early_stopping\": false,\n",
    "  \"embedding_dim\": 768,\n",
    "  \"encoder_no_repeat_ngram_size\": 0,\n",
    "  \"env\": \"local\",\n",
    "  \"eos_token_id\": 2,\n",
    "  \"eval_data_file\": null,\n",
    "  \"experiment\": \"roberta_classification\",\n",
    "  \"finetuning_task\": null,\n",
    "  \"forced_bos_token_id\": null,\n",
    "  \"forced_eos_token_id\": null,\n",
    "  \"freeze_embedding_layer\": false,\n",
    "  \"freeze_encoder_layers\": [],\n",
    "  \"freeze_pooling_layer\": false,\n",
    "  \"freeze_transformer\": true,\n",
    "  \"gradient_checkpointing\": false,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dim\": 512,\n",
    "  \"hidden_dropout_prob\": 0.1,\n",
    "  \"hidden_size\": 768,\n",
    "  \"id2label\": {\n",
    "    \"0\": \"LABEL_0\",\n",
    "    \"1\": \"LABEL_1\"\n",
    "  },\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 3072,\n",
    "  \"is_decoder\": false,\n",
    "  \"is_encoder_decoder\": false,\n",
    "  \"label2id\": {\n",
    "    \"LABEL_0\": 0,\n",
    "    \"LABEL_1\": 1\n",
    "  },\n",
    "  \"layer_norm_eps\": 1e-05,\n",
    "  \"learning_rate\": 0.0001,\n",
    "  \"length_penalty\": 1.0,\n",
    "  \"local\": true,\n",
    "  \"local_rank\": -1,\n",
    "  \"log_all_metrics\": false,\n",
    "  \"loss_weighting\": null,\n",
    "  \"max_grad_norm\": 0,\n",
    "  \"max_length\": 20,\n",
    "  \"max_length_seq\": 200,\n",
    "  \"max_num_sent_positions\": 40,\n",
    "  \"max_num_sentences\": 100,\n",
    "  \"max_num_word_positions\": 2048,\n",
    "  \"max_position_embeddings\": 2048,\n",
    "  \"min_length\": 0,\n",
    "  \"model_type\": \"\",\n",
    "  \"no_repeat_ngram_size\": 0,\n",
    "  \"notes\": \"sentence-classification\",\n",
    "  \"num_attention_heads\": 12,\n",
    "  \"num_beam_groups\": 1,\n",
    "  \"num_beams\": 1,\n",
    "  \"num_dataloader_cpus\": 10,\n",
    "  \"num_documents\": 30,\n",
    "  \"num_gpus\": 0,\n",
    "  \"num_hidden_layers\": 12,\n",
    "  \"num_nodes\": 1,\n",
    "  \"num_output_tags\": 1,\n",
    "  \"num_return_sequences\": 1,\n",
    "  \"num_token_types\": 3,\n",
    "  \"num_train_epochs\": 3,\n",
    "  \"output_attentions\": false,\n",
    "  \"output_hidden_states\": false,\n",
    "  \"output_scores\": false,\n",
    "  \"pad_token_id\": 1,\n",
    "  \"position_embedding_type\": \"absolute\",\n",
    "  \"prefix\": null,\n",
    "  \"pretrained_lm_model_path\": null,\n",
    "  \"pretrained_model_path\": \"/Users/alex/.cache/torch/transformers/named-models/roberta-base-expanded-embeddings\",\n",
    "  \"problem_type\": null,\n",
    "  \"pruned_heads\": {},\n",
    "  \"remove_invalid_values\": false,\n",
    "  \"repetition_penalty\": 1.0,\n",
    "  \"return_dict\": true,\n",
    "  \"return_dict_in_generate\": false,\n",
    "  \"sentence_contextualizer_model_type\": \"roberta\",\n",
    "  \"sentence_embedding_method\": \"multiheaded-attention\",\n",
    "  \"sep_token_id\": null,\n",
    "  \"shuffle_data\": true,\n",
    "  \"sinusoidal_embeddings\": false,\n",
    "  \"spacy_model_file\": \"en_core_web_lg\",\n",
    "  \"split_type\": \"random\",\n",
    "  \"task_specific_params\": null,\n",
    "  \"temperature\": 1.0,\n",
    "  \"tie_encoder_decoder\": false,\n",
    "  \"tie_word_embeddings\": true,\n",
    "  \"tokenizer_class\": null,\n",
    "  \"top_k\": 50,\n",
    "  \"top_p\": 1.0,\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"torchscript\": false,\n",
    "  \"train_data_file\": \"/Users/alex/Projects/usc-research/source-exploration/models_neural/quote_attribution/data/our-annotated-data__stage-2.tsv\",\n",
    "  \"train_on_none\": false,\n",
    "  \"transformer_attention_probs_dropout_prob\": 0.1,\n",
    "  \"transformer_hidden_dropout_prob\": 0.1,\n",
    "  \"transformers_version\": \"4.12.2\",\n",
    "  \"type_vocab_size\": 1,\n",
    "  \"use_bfloat16\": false,\n",
    "  \"use_cache\": false,\n",
    "  \"use_cpu\": false,\n",
    "  \"use_deepspeed\": false,\n",
    "  \"use_doc_emb\": false,\n",
    "  \"use_headline\": false,\n",
    "  \"use_headline_embs\": false,\n",
    "  \"use_positional\": false,\n",
    "  \"vocab_size\": 50265,\n",
    "  \"warmup_steps\": 0,\n",
    "  \"weight_decay\": 0\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_dict = json.loads(t_server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dict = json.loads(t_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>server</th>\n",
       "      <th>local</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bad_words_ids</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>classifier_dropout</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cross_attention_hidden_size</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decoder_start_token_id</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>downsample_negative_data</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_data_file</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finetuning_task</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forced_bos_token_id</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forced_eos_token_id</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freeze_encoder_layers</th>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freeze_transformer</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>local</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loss_weighting</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>notes</th>\n",
       "      <td>Stage 2: Quote Attribution. Classification. Our dataset only.</td>\n",
       "      <td>sentence-classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_documents</th>\n",
       "      <td>None</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_gpus</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prefix</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pretrained_lm_model_path</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pretrained_model_path</th>\n",
       "      <td>./roberta-base-expanded-embeddings</td>\n",
       "      <td>/Users/alex/.cache/torch/transformers/named-models/roberta-base-expanded-embeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>problem_type</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_embedding_method</th>\n",
       "      <td>attention</td>\n",
       "      <td>multiheaded-attention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sep_token_id</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shuffle_data</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spacy_model_file</th>\n",
       "      <td>spacy/en_core_web_lg</td>\n",
       "      <td>en_core_web_lg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task_specific_params</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer_class</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_data_file</th>\n",
       "      <td>data/our-annotated-data__stage-2.tsv</td>\n",
       "      <td>/Users/alex/Projects/usc-research/source-exploration/models_neural/quote_attribution/data/our-annotated-data__stage-2.tsv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                    server  \\\n",
       "bad_words_ids                                                                         None   \n",
       "classifier_dropout                                                                    None   \n",
       "cross_attention_hidden_size                                                           None   \n",
       "decoder_start_token_id                                                                None   \n",
       "downsample_negative_data                                                               1.0   \n",
       "eval_data_file                                                                        None   \n",
       "finetuning_task                                                                       None   \n",
       "forced_bos_token_id                                                                   None   \n",
       "forced_eos_token_id                                                                   None   \n",
       "freeze_encoder_layers                                       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]   \n",
       "freeze_transformer                                                                   False   \n",
       "local                                                                                False   \n",
       "loss_weighting                                                                        None   \n",
       "notes                        Stage 2: Quote Attribution. Classification. Our dataset only.   \n",
       "num_documents                                                                         None   \n",
       "num_gpus                                                                                 1   \n",
       "prefix                                                                                None   \n",
       "pretrained_lm_model_path                                                              None   \n",
       "pretrained_model_path                                   ./roberta-base-expanded-embeddings   \n",
       "problem_type                                                                          None   \n",
       "sentence_embedding_method                                                        attention   \n",
       "sep_token_id                                                                          None   \n",
       "shuffle_data                                                                         False   \n",
       "spacy_model_file                                                      spacy/en_core_web_lg   \n",
       "task_specific_params                                                                  None   \n",
       "tokenizer_class                                                                       None   \n",
       "train_data_file                                       data/our-annotated-data__stage-2.tsv   \n",
       "\n",
       "                                                                                                                                                 local  \n",
       "bad_words_ids                                                                                                                                     None  \n",
       "classifier_dropout                                                                                                                                None  \n",
       "cross_attention_hidden_size                                                                                                                       None  \n",
       "decoder_start_token_id                                                                                                                            None  \n",
       "downsample_negative_data                                                                                                                           0.1  \n",
       "eval_data_file                                                                                                                                    None  \n",
       "finetuning_task                                                                                                                                   None  \n",
       "forced_bos_token_id                                                                                                                               None  \n",
       "forced_eos_token_id                                                                                                                               None  \n",
       "freeze_encoder_layers                                                                                                                               []  \n",
       "freeze_transformer                                                                                                                                True  \n",
       "local                                                                                                                                             True  \n",
       "loss_weighting                                                                                                                                    None  \n",
       "notes                                                                                                                          sentence-classification  \n",
       "num_documents                                                                                                                                       30  \n",
       "num_gpus                                                                                                                                             0  \n",
       "prefix                                                                                                                                            None  \n",
       "pretrained_lm_model_path                                                                                                                          None  \n",
       "pretrained_model_path                                              /Users/alex/.cache/torch/transformers/named-models/roberta-base-expanded-embeddings  \n",
       "problem_type                                                                                                                                      None  \n",
       "sentence_embedding_method                                                                                                        multiheaded-attention  \n",
       "sep_token_id                                                                                                                                      None  \n",
       "shuffle_data                                                                                                                                      True  \n",
       "spacy_model_file                                                                                                                        en_core_web_lg  \n",
       "task_specific_params                                                                                                                              None  \n",
       "tokenizer_class                                                                                                                                   None  \n",
       "train_data_file              /Users/alex/Projects/usc-research/source-exploration/models_neural/quote_attribution/data/our-annotated-data__stage-2.tsv  "
      ]
     },
     "execution_count": 698,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([\n",
    "    pd.Series(server_dict).to_frame('server'),\n",
    "    pd.Series(local_dict).to_frame('local')\n",
    "], axis=1).loc[lambda df: df['server'] != df['local']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../models_neural/src')\n",
    "import coref_resolution_util as c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'coref_resolution_util' has no attribute 'load_models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-704-0bb7e0f7f69e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'coref_resolution_util' has no attribute 'load_models'"
     ]
    }
   ],
   "source": [
    "predictor, nlp = c.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FuzzyIntersectionStrategy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-705-437b462668bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfuzzy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFuzzyIntersectionStrategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'FuzzyIntersectionStrategy' is not defined"
     ]
    }
   ],
   "source": [
    "fuzzy = FuzzyIntersectionStrategy(predictor, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'print_clusters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-706-398c4ce3dcf5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint_clusters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintersection_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclusters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'print_clusters' is not defined"
     ]
    }
   ],
   "source": [
    " print_clusters(doc, intersection_strategy.clusters(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "assignment_df = pd.read_csv('../app/data/2022-07-08__annotator-assignments.csv.gz', index_col=0)\n",
    "completed_tasks = glob.glob('../app/data/output_data_affil-role/*/*')\n",
    "rewritten_completed_tasks = list(map(lambda x: \n",
    "                                     x.replace('output_data_affil-role', 'input_data')\n",
    "                                     .replace('annotated', 'to-annotate')\n",
    "                                     , completed_tasks))\n",
    "\n",
    "completed_df = pd.Series(rewritten_completed_tasks).to_frame('fn').assign(done=True)\n",
    "\n",
    "assignment_df = (\n",
    "    assignment_df\n",
    "    .merge(completed_df, how='left', left_on='fn', right_on='fn')\n",
    "    .assign(done=lambda df: df['done'].fillna(False))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    213\n",
       "True     169\n",
       "Name: done, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assignment_df.loc[lambda df: df['annotator']=='alex']['done'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fn</th>\n",
       "      <th>annotator</th>\n",
       "      <th>file_id</th>\n",
       "      <th>done</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../app/data/input_data/14/to-annotate-746.json</td>\n",
       "      <td>alex</td>\n",
       "      <td>746.json</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../app/data/input_data/7/to-annotate-364.json</td>\n",
       "      <td>alex</td>\n",
       "      <td>364.json</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../app/data/input_data/16/to-annotate-806.json</td>\n",
       "      <td>alex</td>\n",
       "      <td>806.json</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../app/data/input_data/18/to-annotate-917.json</td>\n",
       "      <td>alex</td>\n",
       "      <td>917.json</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../app/data/input_data/16/to-annotate-814.json</td>\n",
       "      <td>alex</td>\n",
       "      <td>814.json</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>../app/data/input_data/15/to-annotate-762.json</td>\n",
       "      <td>james</td>\n",
       "      <td>762.json</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>../app/data/input_data/12/to-annotate-609.json</td>\n",
       "      <td>james</td>\n",
       "      <td>609.json</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>../app/data/input_data/2/to-annotate-139.json</td>\n",
       "      <td>james</td>\n",
       "      <td>139.json</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>../app/data/input_data/15/to-annotate-775.json</td>\n",
       "      <td>james</td>\n",
       "      <td>775.json</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>../app/data/input_data/9/to-annotate-463.json</td>\n",
       "      <td>james</td>\n",
       "      <td>463.json</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>765 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 fn annotator   file_id   done\n",
       "0    ../app/data/input_data/14/to-annotate-746.json      alex  746.json  False\n",
       "1     ../app/data/input_data/7/to-annotate-364.json      alex  364.json  False\n",
       "2    ../app/data/input_data/16/to-annotate-806.json      alex  806.json   True\n",
       "3    ../app/data/input_data/18/to-annotate-917.json      alex  917.json   True\n",
       "4    ../app/data/input_data/16/to-annotate-814.json      alex  814.json  False\n",
       "..                                              ...       ...       ...    ...\n",
       "760  ../app/data/input_data/15/to-annotate-762.json     james  762.json  False\n",
       "761  ../app/data/input_data/12/to-annotate-609.json     james  609.json  False\n",
       "762   ../app/data/input_data/2/to-annotate-139.json     james  139.json  False\n",
       "763  ../app/data/input_data/15/to-annotate-775.json     james  775.json  False\n",
       "764   ../app/data/input_data/9/to-annotate-463.json     james  463.json  False\n",
       "\n",
       "[765 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assignment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "alex_portion_assignment_df = (assignment_df\n",
    " .loc[lambda df: df['annotator']=='alex']\n",
    " .loc[lambda df: df['done'] == False]\n",
    " .assign(annotator=np.random.choice(new_annotators, size=213))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_annotator_df = pd.concat([\n",
    "    alex_portion_assignment_df,\n",
    "    assignment_df.loc[lambda df: df['annotator'] == 'james']\n",
    "]).drop('done', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_annotator_df.to_csv('../app/data/2022-08-18__annotator-assignments.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fn</th>\n",
       "      <th>annotator</th>\n",
       "      <th>file_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>../app/data/input_data/0/to-annotate-8.json</td>\n",
       "      <td>alex</td>\n",
       "      <td>8.json</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              fn annotator file_id\n",
       "274  ../app/data/input_data/0/to-annotate-8.json      alex  8.json"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_annotator_df.loc[lambda df: df['file_id'] == '8.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_annotators = ['alex', 'hari', 'khushbu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
