{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ee0b959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from fastcoref import spacy_component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0110f186",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/08/2023 02:31:26 - INFO - \t missing_keys: []\n",
      "01/08/2023 02:31:26 - INFO - \t unexpected_keys: []\n",
      "01/08/2023 02:31:26 - INFO - \t mismatched_keys: []\n",
      "01/08/2023 02:31:26 - INFO - \t error_msgs: []\n",
      "01/08/2023 02:31:26 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<fastcoref.spacy_component.spacy_component.FastCorefResolver at 0x15fa710c0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Alice goes down the rabbit hole. Where she would discover a new reality beyond her expectations.'\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"fastcoref\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89f913a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/08/2023 02:31:34 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a78ee2a7a5c540e3af63e8662b9813a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/08/2023 02:31:35 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b175a953eba04d9ca01e89019a5e46e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[[(0, 5), (39, 42), (79, 82)]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "doc._.coref_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d83d8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcoref import FCoref, LingMessCoref\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3f68d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LingMessCoref()\n",
    "model = FCoref()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791e0f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(\n",
    "   texts=['We are so happy to see you using our coref package. This package is very fast!']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "455cd3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('../resources/data/nytimes-articles-to-extract-sources.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4fe9adac",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data_df.iloc[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10590748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import pyperclip \n",
    "pyperclip.copy(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b08c306",
   "metadata": {},
   "source": [
    "# Try with Spacy and use tricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4fcd5f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../tasks/quote_detection/other_platforms/data/training_data.jsonl'\n",
    "training_data = []\n",
    "with open(filename) as f:\n",
    "    for line in f:\n",
    "        training_data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5322910d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_datum = training_data[0]\n",
    "sents = list(map(lambda x: x['sent'], training_datum['data']))\n",
    "\n",
    "# get sent charspans\n",
    "sent_lens = list(map(len, sents))\n",
    "sent_idxs = np.cumsum([0] + sent_lens)\n",
    "sent_char_spans = list(zip(sent_idxs[:-1], sent_idxs[1:]))\n",
    "\n",
    "# get clusters\n",
    "text = ''.join(sents)\n",
    "doc = nlp(text)\n",
    "clusters = doc._.coref_clusters\n",
    "\n",
    "# from clusters, get cluster mappers of heads -> spans\n",
    "s2h_mapper, _ = get_cluster_mappers(doc, clusters, is_char_clusters=True)\n",
    "s2h_to_replace = {k:v for k, v in s2h_mapper.items() if k != v}\n",
    "s2h_to_replace = {k:s2h_to_replace[k] for k in sorted(s2h_to_replace, key=lambda x: x[0])}\n",
    "\n",
    "# make sure we don't have nest corefs\n",
    "old_s2h_len = len(s2h_to_replace)\n",
    "while True:\n",
    "    k = list(s2h_to_replace.keys())\n",
    "    nested_coref = list(filter(lambda x: x[1][0] < x[0][1], zip(k[:-1], k[1:]))) # compare adjacent sorted spans to make sure the start-index of one isn't less than the end-index of another \n",
    "    to_remove = list(map(lambda x: min(x, key=lambda y: y[1] -y[0]), nested_coref)) # if it is, take the larger span\n",
    "    s2h_to_replace = {k:v for k,v in s2h_to_replace.items() if k not in to_remove} # repeat until there are no more overlapping spans\n",
    "    new_s2h_len = len(s2h_to_replace)\n",
    "    if new_s2h_len == old_s2h_len:\n",
    "        break\n",
    "    else:\n",
    "        old_s2h_len = new_s2h_len\n",
    "\n",
    "# resolve the corefs\n",
    "test_sents = copy(sents)\n",
    "for k in list(reversed(s2h_to_replace)):\n",
    "    v = s2h_to_replace[k]\n",
    "    k_sent_idx, (k_sent_s, k_sent_e) = convert_to_sent_idx_and_sent_char_span(k, sent_bins=sent_char_spans)\n",
    "    v_sent_idx, (v_sent_s, v_sent_e) = convert_to_sent_idx_and_sent_char_span(v, sent_bins=sent_char_spans)\n",
    "    \n",
    "    # determine what to replace\n",
    "    to_replace = test_sents[v_sent_idx][v_sent_s: v_sent_e]\n",
    "    final_token = doc.char_span(*k)[-1]\n",
    "    if final_token.tag_ in [\"PRP$\", \"POS\"]:\n",
    "        to_replace = to_replace + \"'s\"\n",
    "\n",
    "    \n",
    "    test_sents[k_sent_idx] = (\n",
    "        test_sents[k_sent_idx][:k_sent_s] + to_replace + test_sents[k_sent_idx][k_sent_e:]\n",
    "    )\n",
    "    \n",
    "assert len(test_sents) == len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a88d324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03f69e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58f4a5a3",
   "metadata": {},
   "source": [
    "# Maybe try redoing the sentence boundaries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "469fb4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jellyfish import jaro_distance, levenshtein_distance\n",
    "from polyleven import levenshtein\n",
    "import jsonlines\n",
    "import spacy\n",
    "from copy import copy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "f35185c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_data_df = pd.read_csv('../resources/data/nytimes-articles-to-extract-sources.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "952d08b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open('../tasks/all_annotated_sources.jsonl') as f:\n",
    "    training_data = list(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "8c76ed01",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_sents = list(map(lambda x: x['sent'].strip(), training_data[0]))\n",
    "old_text = ' '.join(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "0cf87224",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_data_df['found'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "166f1568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jaro_test = orig_data_df['text'].apply(lambda x: jaro_distance(x, text))\n",
    "# difflib_test = orig_data_df['text'].apply(lambda x: SequenceMatcher(x, text).ratio())\n",
    "lev_test = orig_data_df['text'].apply(lambda x: levenshtein(x, old_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "e2fc6987",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_id, orig_text = orig_data_df.loc[lev_test.argmin()][['entry_id', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "3b9d118d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BANGKOK', 'â€”']"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_sents[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "508050da",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(orig_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "a029178f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sents = list(map(str, doc.sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bc2544",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_new_sent_idx = 0\n",
    "new_sents_copy = copy(new_sents)\n",
    "\n",
    "\n",
    "for old_sent_idx, old_sent in enumerate(old_sents):\n",
    "    # to match sentences, first \n",
    "    if len(old_sent) > 2:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "3f48c066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BANGKOK'"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267f9e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60220abb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a38475fc",
   "metadata": {},
   "source": [
    "# Test Quote Detection Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91d18dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe74fbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../tasks/quote_detection/other_platforms/src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dc884a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_model import SentenceClassificationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a20d68b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "865b085be9eb4cfbbd0218d09b6824fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/681 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a022fc481b434abc87fe3b7aceb26701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained('alex2awesome/source-exploration__quote-detection__sentence-roberta-base')\n",
    "model = SentenceClassificationModel.from_pretrained('alex2awesome/source-exploration__quote-detection__sentence-roberta-base', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "326643ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"_name_or_path\": \"alex2awesome/source-exploration__quote-detection__sentence-roberta-base\",\n",
       "  \"architectures\": [\n",
       "    \"SentenceClassificationModel\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.25.1\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4c8872",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acd96da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b506d1b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f972f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ad7834",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e65718f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
