{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "db10a054",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '../tasks/quote_attribution/other_platforms/span-detection-approaches/')\n",
    "from qa_dataset import fix_quote_type\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import glob\n",
    "import os \n",
    "import json\n",
    "import pandas as pd \n",
    "import re\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "id": "2b919d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(x):\n",
    "    if pd.isnull(x):\n",
    "        return x\n",
    "    x = x.lower()\n",
    "    words_to_remove = ['the']\n",
    "    for w in words_to_remove:\n",
    "        x = (' %s ' % x).replace(' %s ' % w, ' ')\n",
    "    x = re.sub('\\s+', ' ', x)\n",
    "    x = re.sub('\\d+', '', x)\n",
    "    for p in string.punctuation:\n",
    "        x = x.replace(p, '')\n",
    "    return x.strip()\n",
    "\n",
    "def test_in(true_label, gpt3_guess):\n",
    "    if pd.isnull(true_label) or pd.isnull(gpt3_guess):\n",
    "        return np.nan\n",
    "    \n",
    "    true_label, gpt3_guess = clean(true_label), clean(gpt3_guess)\n",
    "    if true_label == gpt3_guess:\n",
    "        return True\n",
    "    if true_label in gpt3_guess:\n",
    "        return True\n",
    "    if gpt3_guess in true_label:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def read_results(f):\n",
    "    callback_files = glob.glob(os.path.join(f, '*'))\n",
    "\n",
    "    all_results = []\n",
    "    for r in callback_files:\n",
    "        callback_results = json.load(open(r))\n",
    "        all_results.append(callback_results)\n",
    "\n",
    "    all_results_df = pd.DataFrame(all_results)\n",
    "\n",
    "    _e_cols = list(filter(lambda x: x.endswith('_e'), all_results_df.columns))\n",
    "    # span-prediction and token classification\n",
    "    if len(_e_cols) > 0:\n",
    "        all_results_df = all_results_df[_e_cols]\n",
    "\n",
    "    return all_results_df.loc[lambda df: df.full_e.idxmax()]\n",
    "\n",
    "def get_openai_results(detection_df, attribution_df, results_type='true_label'):\n",
    "    '''\n",
    "    detection_df has columns  =   'doc_id', 'sent_idx', 'sent', 'is_quote_true_label', 'detection_prob'\n",
    "    attribution_df has columns=   'doc_id', 'sent_idx', 'head', 'quote_type', 'attribution'\n",
    "    '''\n",
    "    \n",
    "    attribution_df = attribution_df.rename(columns={'y_pred': 'attribution'})\n",
    "    for col_to_drop in ['sent', 'detection_prob']:\n",
    "        if col_to_drop in attribution_df.columns:\n",
    "            attribution_df = attribution_df.drop(col_to_drop, axis=1)\n",
    "    \n",
    "    docs_too_long = (\n",
    "        attribution_df\n",
    "        .assign(a_null=lambda df: df['attribution'].notnull())\n",
    "        .groupby('doc_id')['a_null']\n",
    "        .any()\n",
    "        .loc[lambda df: df == False].index\n",
    "    )\n",
    "    detection_df = detection_df.loc[lambda df: ~df['doc_id'].isin(docs_too_long)]\n",
    "    merged_df = detection_df.merge(attribution_df, how='left', left_on=['doc_id', 'sent_idx'], right_on=['doc_id', 'sent_idx'])\n",
    "    if results_type == 'true_label':\n",
    "#         merged_df = merged_df.loc[lambda df: df['is_quote_true_label'] == True]\n",
    "        merged_df = merged_df.loc[lambda df: ~df['quote_type'].fillna('NO QUOTE').isin(to_exclude)]\n",
    "    else:\n",
    "        merged_df = merged_df.loc[lambda df: df['detection_prob'] > .5]\n",
    "        \n",
    "    merged_df = merged_df.loc[lambda df: df['sent'].str.len() > 2]#.fillna('')\n",
    "    merged_df['is_match'] = merged_df.apply(lambda x: test_in(x['head'], x['attribution']), axis=1)#.fillna(False)\n",
    "    \n",
    "    if results_type != 'true_label':\n",
    "        merged_df['is_match'] = merged_df['is_match'].fillna(False)\n",
    "    \n",
    "    output_results = {}\n",
    "    output_results['full'] = merged_df['is_match'].mean()\n",
    "    category_results = (merged_df\n",
    "     .assign(quote_type=lambda df: df.apply(fix_quote_type, axis=1))\n",
    "     .groupby('quote_type')['is_match']\n",
    "     .mean()\n",
    "     .to_dict()\n",
    "    )\n",
    "    output_results.update(category_results)\n",
    "    return output_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4be8049",
   "metadata": {},
   "source": [
    "# Detection Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "id": "92b5d3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = glob.glob('../tasks/quote_detection/other_platforms/results/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "id": "27b9299c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trials_res = {}\n",
    "for trial in trials:\n",
    "    checkpoint_metrics = glob.glob(os.path.join(trial, 'call*'))\n",
    "    trial_name = os.path.basename(trial)\n",
    "    \n",
    "    all_results = []\n",
    "    for c in checkpoint_metrics:\n",
    "        res = json.load(open(c))\n",
    "        all_results.append(pd.Series(res))\n",
    "    res_df = pd.concat(all_results, axis=1).T\n",
    "    top_res = res_df.loc[lambda df: df['full_f1'].idxmax()]\n",
    "    \n",
    "    all_trials_res[trial_name] = top_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "id": "5b1970bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trials_res_df = pd.DataFrame(all_trials_res).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "id": "c849ff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res_final = (all_trials_res_df\n",
    " .rename(columns=lambda x: x.replace('_f1', ''))\n",
    " .pipe(lambda df: \n",
    "   pd.concat([df] + [\n",
    "       merge_cluster(df, quote_type_counts, c, m) \n",
    "       for c,m in to_merge_clusters\n",
    "   ], axis=1))\n",
    " [col_order]\n",
    " .pipe(lambda s: s * 100).round(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "id": "5dbe0aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrr}\n",
      "\\toprule\n",
      "{} &  full &  DIRECT QUOTE &  INDIRECT QUOTE &  Statement/Public Speech &  Email/Social Media &  Published Work/Press Report &  Other \\\\\n",
      "\\midrule\n",
      "roberta-base\\_\\_sentence-model\\_\\_background-excluded  &  87.0 &          92.4 &            99.1 &                     95.6 &                94.1 &                         90.0 &   66.6 \\\\\n",
      "roberta-base\\_\\_sentence-model\\_\\_background-exclud... &  87.1 &          91.0 &            98.7 &                     94.1 &                92.7 &                         85.4 &   61.4 \\\\\n",
      "big-bird\\_\\_full-sequence-model\\_\\_background-excluded &  88.2 &          92.0 &            98.7 &                     96.4 &                89.8 &                         86.4 &   65.1 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/89/6dsq1ymj63x009t6wpt25f9h0000gp/T/ipykernel_16187/2706175118.py:1: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(all_res_final.to_latex())\n"
     ]
    }
   ],
   "source": [
    "print(all_res_final.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0cc4b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db64232c",
   "metadata": {},
   "source": [
    "# Attribution Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "id": "bb89474e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = glob.glob('../tasks/quote_attribution/other_platforms/span-detection-approaches/results/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "id": "1977eb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_across_trials = {}\n",
    "for trial in trials:\n",
    "    trial_name = os.path.basename(trial)\n",
    "    results_across_trials[trial_name] = read_results(trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "id": "ecbd0771",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_across_trials_df = pd.DataFrame(results_across_trials).T\n",
    "results_across_trials_df = results_across_trials_df.rename(columns=lambda x: x.replace('_e', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "id": "2ea8a63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_order = [\n",
    "    'big-bird__token-classification-model',\n",
    "    'big-bird__token-classification-model__coref_resolved',\n",
    "    ## \n",
    "    'big-bird__qa-model',\n",
    "    'big-bird__qa-model__coref-resolved',\n",
    "    'big-bird__salience-model__augmented-data', \n",
    "    'big-bird__loss-window-2',\n",
    "    'big-bird__qa-model__roberta-large',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "id": "e7f07148",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name_and_path = [\n",
    "    ('babbage_with_nones', 'cache/2023-01-12__detection-and-attributions__attribution-model.csv'),\n",
    "    ('babbage_with_coref', 'cache/2023-01-11__babbage__with-coref-evaluation.csv'),\n",
    "    ('babbage_final', 'cache/2023-01-19__babbage-without-nones-all-training.csv'), # to rerun\n",
    "    ('curie_final', 'cache/2023-01-19__curie-without-nones-all-training.csv'), # to rerun\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "id": "beba01af",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_df = pd.read_csv('cache/2023-01-12__detection-and-attributions__attribution-model.csv')\n",
    "detection_df= (\n",
    "    detection_df[['doc_id', 'sent_idx', 'sent', 'detection_label', 'detection_prob']]\n",
    "    .rename(columns={'detection_label': 'is_quote_true_label'})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "id": "ab3bccf3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "open_ai_result_dicts = {}\n",
    "for model_name, path in model_name_and_path:\n",
    "    attributions_df = pd.read_csv(path)\n",
    "    results_true = get_openai_results(detection_df, attributions_df, results_type='true_label')\n",
    "    results_detect = get_openai_results(detection_df, attributions_df, results_type='detection')    \n",
    "    open_ai_result_dicts[model_name + '_true_label'] = results_true\n",
    "    open_ai_result_dicts[model_name + '_detect'] = results_detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "id": "1e242a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_ai_results_df = pd.DataFrame(open_ai_result_dicts).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "id": "eb2d0597",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_df = pd.concat([\n",
    "    results_across_trials_df,\n",
    "    open_ai_results_df\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "id": "f4e737bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_order_negative_results = [\n",
    "# \n",
    "    'big-bird__token-classification-model',\n",
    "        'big-bird__token-classification-model__coref_resolved',\n",
    "# \n",
    "    'big-bird__qa-model',\n",
    "        'big-bird__qa-model__coref-resolved',\n",
    "        'big-bird__loss-window-2',\n",
    "        'big-bird__salience-model__augmented-data',\n",
    "        'big-bird__qa-model__roberta-large',\n",
    "# \n",
    "    'babbage_final_true_label',\n",
    "        'babbage_with_nones_true_label',\n",
    "        'babbage_with_coref_true_label',    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "id": "934a2821",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_results = [\n",
    "    # \n",
    "    'big-bird__token-classification-model',\n",
    "    # \n",
    "    'big-bird__qa-model',\n",
    "        'big-bird__qa-model__coref-resolved',    \n",
    "    'babbage_final_true_label',\n",
    "    'babbage_with_nones_true_label',\n",
    "        'babbage_with_coref_true_label',\n",
    "    'curie_final_true_label',\n",
    "    \n",
    "    # \n",
    "    'babbage_with_nones_detect',\n",
    "    'babbage_final_detect',\n",
    "    'curie_final_detect'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "id": "e90c5787",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_results_rename = {\n",
    "        # \n",
    "    'big-bird__token-classification-model': 'Seq. Labeling',\n",
    "    # \n",
    "    'big-bird__qa-model': 'Span Detection',\n",
    "        'big-bird__qa-model__coref-resolved': 'SD+Coref Resolved',\n",
    "    'babbage_final_true_label': 'GPT3 1.3B',\n",
    "    'babbage_with_nones_true_label': 'GPT3 1.3B +Nones',\n",
    "        'babbage_with_coref_true_label': 'GPT3 1.3B + Coref',\n",
    "    'curie_final_true_label': 'GPT3 6.7B',\n",
    "    \n",
    "    # \n",
    "    'babbage_with_nones_detect': 'GPT3 1.3B +Nones, Detection',\n",
    "    'babbage_final_detect': 'GPT3 1.3B, Detection',\n",
    "    'curie_final_detect': 'GPT3 6.7B, Detection'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "id": "38d6f0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_merge_clusters = [\n",
    "    ('Statement/Public Speech', ['STATEMENT', 'PUBLIC SPEECH'],),\n",
    "    ('Email/Social Media', ['COMMUNICATION', 'SOCIAL MEDIA POST',],),\n",
    "    ('Published Work/Press Report', [ 'PUBLISHED WORK', 'PRESS REPORT',]),\n",
    "    ('Other', [ 'VOTE/POLL', 'DECLINED COMMENT', 'DIRECT OBSERVATION', 'PRICE SIGNAL'])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "id": "0f5cda49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_cluster(res_df, quote_type_counts, output_cluster_name, to_merge):\n",
    "    res = res_df[to_merge]\n",
    "    counts = quote_type_counts[to_merge]\n",
    "    summation = (res * counts).sum(axis=1)\n",
    "    avg = summation / counts.sum()\n",
    "    return avg.to_frame(output_cluster_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "id": "f798a2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_order = [\n",
    "    'full',\n",
    "    'DIRECT QUOTE',\n",
    "    'INDIRECT QUOTE',\n",
    "    # \n",
    "    'Statement/Public Speech',\n",
    "    #\n",
    "    'Email/Social Media',\n",
    "    # \n",
    "    'Published Work/Press Report',\n",
    "    # \n",
    "    'Other'    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "id": "28eef1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "quote_type_counts = pd.concat(list(map(pd.DataFrame, data_to_attribute))).apply(fix_quote_type, axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "id": "32ee26c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_all_results_df = (all_results_df\n",
    " .loc[positive_results]\n",
    " .rename(index=positive_results_rename)\n",
    " .pipe(lambda df: \n",
    "   pd.concat([df] + [\n",
    "       merge_cluster(df, quote_type_counts, c, m) \n",
    "       for c,m in to_merge_clusters\n",
    "   ], axis=1))\n",
    " [col_order]\n",
    " .pipe(lambda df: df*100).round(1)\n",
    " .fillna('-')\n",
    " .rename(columns=lambda x: x.title())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "id": "0a064030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Full</th>\n",
       "      <th>Direct Quote</th>\n",
       "      <th>Indirect Quote</th>\n",
       "      <th>Statement/Public Speech</th>\n",
       "      <th>Email/Social Media</th>\n",
       "      <th>Published Work/Press Report</th>\n",
       "      <th>Other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Seq. Labeling</th>\n",
       "      <td>38.5</td>\n",
       "      <td>37.2</td>\n",
       "      <td>43.4</td>\n",
       "      <td>39.7</td>\n",
       "      <td>33.8</td>\n",
       "      <td>31.9</td>\n",
       "      <td>13.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Span Detection</th>\n",
       "      <td>59.5</td>\n",
       "      <td>61.1</td>\n",
       "      <td>59.5</td>\n",
       "      <td>67.5</td>\n",
       "      <td>48.9</td>\n",
       "      <td>51.1</td>\n",
       "      <td>34.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SD+Coref Resolved</th>\n",
       "      <td>53.6</td>\n",
       "      <td>51.2</td>\n",
       "      <td>56.8</td>\n",
       "      <td>61.4</td>\n",
       "      <td>73.5</td>\n",
       "      <td>54.1</td>\n",
       "      <td>37.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPT3 1.3B</th>\n",
       "      <td>78.9</td>\n",
       "      <td>80.9</td>\n",
       "      <td>86.9</td>\n",
       "      <td>85.0</td>\n",
       "      <td>71.9</td>\n",
       "      <td>57.9</td>\n",
       "      <td>38.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPT3 1.3B +Nones</th>\n",
       "      <td>79.6</td>\n",
       "      <td>81.9</td>\n",
       "      <td>87.1</td>\n",
       "      <td>86.2</td>\n",
       "      <td>69.7</td>\n",
       "      <td>60.5</td>\n",
       "      <td>33.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPT3 1.3B + Coref</th>\n",
       "      <td>73.2</td>\n",
       "      <td>78.7</td>\n",
       "      <td>82.5</td>\n",
       "      <td>76.3</td>\n",
       "      <td>56.1</td>\n",
       "      <td>54.4</td>\n",
       "      <td>31.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPT3 6.7B</th>\n",
       "      <td>91.4</td>\n",
       "      <td>94.0</td>\n",
       "      <td>95.5</td>\n",
       "      <td>91.1</td>\n",
       "      <td>91.0</td>\n",
       "      <td>81.6</td>\n",
       "      <td>57.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPT3 1.3B +Nones, Detection</th>\n",
       "      <td>73.1</td>\n",
       "      <td>82.4</td>\n",
       "      <td>84.8</td>\n",
       "      <td>85.9</td>\n",
       "      <td>73.4</td>\n",
       "      <td>61.0</td>\n",
       "      <td>64.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPT3 1.3B, Detection</th>\n",
       "      <td>70.9</td>\n",
       "      <td>79.5</td>\n",
       "      <td>82.9</td>\n",
       "      <td>82.9</td>\n",
       "      <td>73.4</td>\n",
       "      <td>60.5</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPT3 6.7B, Detection</th>\n",
       "      <td>80.0</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.7</td>\n",
       "      <td>89.9</td>\n",
       "      <td>91.1</td>\n",
       "      <td>78.0</td>\n",
       "      <td>68.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Full  Direct Quote  Indirect Quote  \\\n",
       "Seq. Labeling                38.5          37.2            43.4   \n",
       "Span Detection               59.5          61.1            59.5   \n",
       "SD+Coref Resolved            53.6          51.2            56.8   \n",
       "GPT3 1.3B                    78.9          80.9            86.9   \n",
       "GPT3 1.3B +Nones             79.6          81.9            87.1   \n",
       "GPT3 1.3B + Coref            73.2          78.7            82.5   \n",
       "GPT3 6.7B                    91.4          94.0            95.5   \n",
       "GPT3 1.3B +Nones, Detection  73.1          82.4            84.8   \n",
       "GPT3 1.3B, Detection         70.9          79.5            82.9   \n",
       "GPT3 6.7B, Detection         80.0          90.4            90.7   \n",
       "\n",
       "                             Statement/Public Speech  Email/Social Media  \\\n",
       "Seq. Labeling                                   39.7                33.8   \n",
       "Span Detection                                  67.5                48.9   \n",
       "SD+Coref Resolved                               61.4                73.5   \n",
       "GPT3 1.3B                                       85.0                71.9   \n",
       "GPT3 1.3B +Nones                                86.2                69.7   \n",
       "GPT3 1.3B + Coref                               76.3                56.1   \n",
       "GPT3 6.7B                                       91.1                91.0   \n",
       "GPT3 1.3B +Nones, Detection                     85.9                73.4   \n",
       "GPT3 1.3B, Detection                            82.9                73.4   \n",
       "GPT3 6.7B, Detection                            89.9                91.1   \n",
       "\n",
       "                             Published Work/Press Report  Other  \n",
       "Seq. Labeling                                       31.9   13.2  \n",
       "Span Detection                                      51.1   34.1  \n",
       "SD+Coref Resolved                                   54.1   37.2  \n",
       "GPT3 1.3B                                           57.9   38.3  \n",
       "GPT3 1.3B +Nones                                    60.5   33.2  \n",
       "GPT3 1.3B + Coref                                   54.4   31.2  \n",
       "GPT3 6.7B                                           81.6   57.3  \n",
       "GPT3 1.3B +Nones, Detection                         61.0   64.5  \n",
       "GPT3 1.3B, Detection                                60.5   53.0  \n",
       "GPT3 6.7B, Detection                                78.0   68.9  "
      ]
     },
     "execution_count": 677,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_all_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e756e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d88701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "id": "c3161164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/89/6dsq1ymj63x009t6wpt25f9h0000gp/T/ipykernel_16187/3098487526.py:2: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  pyperclip.copy(final_all_results_df.to_latex())\n"
     ]
    }
   ],
   "source": [
    "import pyperclip\n",
    "pyperclip.copy(final_all_results_df.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86aa1c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6fe622",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6f9d78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4f3261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39a40d42",
   "metadata": {},
   "source": [
    "# run openai attribution model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b9a3f31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md             requirements-old.txt  setup.py\r\n",
      "\u001b[34mapp\u001b[m\u001b[m/                  requirements.in       \u001b[34mtasks\u001b[m\u001b[m/\r\n",
      "\u001b[34mmodels_other\u001b[m\u001b[m/         requirements.txt      \u001b[34mtmp\u001b[m\u001b[m/\r\n",
      "\u001b[34mnotebooks\u001b[m\u001b[m/            \u001b[34mresources\u001b[m\u001b[m/\r\n",
      "pluslab_setup.sh      \u001b[34mscripts\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "ls ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "af0223fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import sys\n",
    "sys.path.insert(0, '../tasks/quote_detection_and_attribution/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "2994a0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from score_new_articles import OpenAIAttributionDataset, OpenAIModel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "33471fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7ab488aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_tok = AutoTokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "5a662aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_dataset = OpenAIAttributionDataset(tokenizer=gpt_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "9523a32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_attribute = list(jsonlines.open('../tasks/data_split_annotated_sources.jsonl'))\n",
    "data_to_attribute = list(filter(lambda x: x['split'] == 'test', data_to_attribute))\n",
    "data_to_attribute = list(map(lambda x: x['data'], data_to_attribute))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "90f69e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7d2ce516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "3a0ab84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "cf8ede87",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEANR = re.compile('<.*?>')\n",
    "def cleanhtml(raw_html):\n",
    "    cleantext = re.sub(CLEANR, '', raw_html)\n",
    "    return cleantext\n",
    "\n",
    "class OpenAIAttributionDataset():\n",
    "    def __init__(self, tokenizer, max_len=2040):\n",
    "        self.prompt_template = '\"\"\"%s\"\"\".\\n\\nTo which source can we attribute this sentence:\\n\\n\"\"\"%s\"\"\"\\n\\n##\\n\\n'\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def make_prompts(self, one_doc):\n",
    "        \"\"\"\n",
    "        Makes a prompt for OpenAI fine-tuned model. Here, we're not training. We expect `one_doc_df`\n",
    "        to have the following columns:\n",
    "\n",
    "        * `sent`     if detection has not been run, and\n",
    "        * `sent`, `is_quote` if detection has been run.\n",
    "        \"\"\"\n",
    "\n",
    "        doc_sents = list(map(lambda x: cleanhtml(x['sent']), one_doc))\n",
    "        article = ' '.join(doc_sents)\n",
    "\n",
    "        all_prompts = []\n",
    "        for sent in one_doc:\n",
    "            sent_text = cleanhtml(sent['sent'])\n",
    "            num_toks = len(self.tokenizer.encode(sent_text))\n",
    "            if (len(sent_text) > 2) and (num_toks < self.max_len):\n",
    "                prompt = self.prompt_template % (article, sent_text)\n",
    "            else:\n",
    "                prompt = None\n",
    "            all_prompts.append(prompt)\n",
    "\n",
    "        return all_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "b0609836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = OpenAIModel(model_name='babbage:ft-isi-nlp-2023-01-12-06-58-08')\n",
    "model = OpenAIModel(model_name='curie:ft-isi-nlp:sep-training-set-base-2022-12-02-01-29-12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "c93fe1e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256c503d00a54d0684e075b350da87c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/86 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2079 tokens (2069 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2100 tokens (2090 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2072 tokens (2062 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2086 tokens (2076 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2067 tokens (2057 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2083 tokens (2073 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2059 tokens (2049 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2080 tokens (2070 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2070 tokens (2060 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2075 tokens (2065 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2069 tokens (2059 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2073 tokens (2063 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2064 tokens (2054 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2087 tokens (2077 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2064 tokens (2054 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2070 tokens (2060 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2070 tokens (2060 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2082 tokens (2072 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2060 tokens (2050 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2082 tokens (2072 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2097 tokens (2087 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2060 tokens (2050 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2071 tokens (2061 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2062 tokens (2052 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2079 tokens (2069 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2078 tokens (2068 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2058 tokens (2048 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2054 tokens (2044 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2067 tokens (2057 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2070 tokens (2060 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2094 tokens (2084 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2081 tokens (2071 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2070 tokens (2060 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2085 tokens (2075 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2084 tokens (2074 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2070 tokens (2060 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2093 tokens (2083 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2076 tokens (2066 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2073 tokens (2063 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2088 tokens (2078 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2073 tokens (2063 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2062 tokens (2052 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2060 tokens (2050 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2078 tokens (2068 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2075 tokens (2065 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2080 tokens (2070 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2094 tokens (2084 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2072 tokens (2062 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2054 tokens (2044 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2068 tokens (2058 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2084 tokens (2074 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2075 tokens (2065 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2073 tokens (2063 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2070 tokens (2060 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2082 tokens (2072 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2067 tokens (2057 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2117 tokens (2107 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2065 tokens (2055 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2083 tokens (2073 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2074 tokens (2064 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2075 tokens (2065 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2087 tokens (2077 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2093 tokens (2083 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2084 tokens (2074 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2076 tokens (2066 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2081 tokens (2071 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2073 tokens (2063 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2064 tokens (2054 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2071 tokens (2061 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2085 tokens (2075 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2077 tokens (2067 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2073 tokens (2063 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2060 tokens (2050 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2061 tokens (2051 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2074 tokens (2064 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2065 tokens (2055 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2054 tokens (2044 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2086 tokens (2076 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2079 tokens (2069 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2080 tokens (2070 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2056 tokens (2046 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2060 tokens (2050 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2071 tokens (2061 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2070 tokens (2060 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2060 tokens (2050 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2052 tokens (2042 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2059 tokens (2049 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2058 tokens (2048 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2050 tokens (2040 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2051 tokens (2041 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2055 tokens (2045 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2075 tokens (2065 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2067 tokens (2057 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2063 tokens (2053 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2054 tokens (2044 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2051 tokens (2041 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2055 tokens (2045 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2058 tokens (2048 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2068 tokens (2058 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2065 tokens (2055 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2054 tokens (2044 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2064 tokens (2054 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2063 tokens (2053 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2052 tokens (2042 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2051 tokens (2041 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2062 tokens (2052 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2051 tokens (2041 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2055 tokens (2045 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2058 tokens (2048 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2060 tokens (2050 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2062 tokens (2052 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2055 tokens (2045 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2066 tokens (2056 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2051 tokens (2041 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2050 tokens (2040 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2062 tokens (2052 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2050 tokens (2040 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2050 tokens (2040 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2068 tokens (2058 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2082 tokens (2072 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2068 tokens (2058 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2068 tokens (2058 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2060 tokens (2050 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2054 tokens (2044 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2060 tokens (2050 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2054 tokens (2044 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2067 tokens (2057 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2071 tokens (2061 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2053 tokens (2043 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2054 tokens (2044 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2055 tokens (2045 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2056 tokens (2046 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2050 tokens (2040 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2054 tokens (2044 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2065 tokens (2055 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2052 tokens (2042 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2058 tokens (2048 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2078 tokens (2068 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2068 tokens (2058 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2101 tokens (2091 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2056 tokens (2046 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2086 tokens (2076 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2098 tokens (2088 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2071 tokens (2061 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2083 tokens (2073 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2050 tokens (2040 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2063 tokens (2053 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2056 tokens (2046 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2065 tokens (2055 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2059 tokens (2049 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2057 tokens (2047 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2069 tokens (2059 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2070 tokens (2060 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2061 tokens (2051 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2067 tokens (2057 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2079 tokens (2069 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2054 tokens (2044 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2097 tokens (2087 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2065 tokens (2055 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2055 tokens (2045 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2074 tokens (2064 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2053 tokens (2043 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2053 tokens (2043 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2056 tokens (2046 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2076 tokens (2066 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2064 tokens (2054 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2073 tokens (2063 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2058 tokens (2048 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2065 tokens (2055 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2099 tokens (2089 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2083 tokens (2073 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2082 tokens (2072 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2056 tokens (2046 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2053 tokens (2043 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2081 tokens (2071 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2066 tokens (2056 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2050 tokens (2040 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2066 tokens (2056 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2061 tokens (2051 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2050 tokens (2040 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2085 tokens (2075 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2070 tokens (2060 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2075 tokens (2065 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2075 tokens (2065 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2090 tokens (2080 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2052 tokens (2042 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2066 tokens (2056 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2089 tokens (2079 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2076 tokens (2066 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2071 tokens (2061 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2080 tokens (2070 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2078 tokens (2068 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2075 tokens (2065 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2054 tokens (2044 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2090 tokens (2080 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2066 tokens (2056 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2068 tokens (2058 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2067 tokens (2057 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2072 tokens (2062 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2057 tokens (2047 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2077 tokens (2067 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2058 tokens (2048 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2066 tokens (2056 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2075 tokens (2065 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2061 tokens (2051 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2052 tokens (2042 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2084 tokens (2074 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2068 tokens (2058 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2085 tokens (2075 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2085 tokens (2075 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2072 tokens (2062 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2118 tokens (2108 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2099 tokens (2089 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2080 tokens (2070 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2129 tokens (2119 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2081 tokens (2071 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2114 tokens (2104 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2107 tokens (2097 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2134 tokens (2124 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2083 tokens (2073 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2093 tokens (2083 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2104 tokens (2094 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2103 tokens (2093 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2098 tokens (2088 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2090 tokens (2080 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2089 tokens (2079 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2080 tokens (2070 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2088 tokens (2078 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2083 tokens (2073 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2075 tokens (2065 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2072 tokens (2062 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2069 tokens (2059 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2078 tokens (2068 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2085 tokens (2075 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2102 tokens (2092 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2106 tokens (2096 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2111 tokens (2101 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2095 tokens (2085 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2110 tokens (2100 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2102 tokens (2092 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2073 tokens (2063 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2083 tokens (2073 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2110 tokens (2100 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2092 tokens (2082 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2077 tokens (2067 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2107 tokens (2097 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2069 tokens (2059 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2093 tokens (2083 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2095 tokens (2085 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2097 tokens (2087 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2083 tokens (2073 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2075 tokens (2065 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2117 tokens (2107 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2082 tokens (2072 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2104 tokens (2094 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2124 tokens (2114 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2089 tokens (2079 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2100 tokens (2090 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2123 tokens (2113 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2104 tokens (2094 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2100 tokens (2090 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2083 tokens (2073 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2091 tokens (2081 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2117 tokens (2107 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2083 tokens (2073 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2092 tokens (2082 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2091 tokens (2081 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2107 tokens (2097 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2093 tokens (2083 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2102 tokens (2092 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2105 tokens (2095 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2091 tokens (2081 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2100 tokens (2090 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2096 tokens (2086 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2077 tokens (2067 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2072 tokens (2062 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2069 tokens (2059 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2078 tokens (2068 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2086 tokens (2076 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2099 tokens (2089 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n",
      "attribution error: This model's maximum context length is 2049 tokens, however you requested 2087 tokens (2077 in your prompt; 10 for the completion). Please reduce your prompt; or completion length.\n"
     ]
    }
   ],
   "source": [
    "all_attributions = []\n",
    "for d in tqdm(data_to_attribute):\n",
    "    doc_output = []\n",
    "    prompts = openai_dataset.make_prompts(d)\n",
    "    for packet, prompt in zip(d, prompts):\n",
    "        if prompt is not None:\n",
    "            attribution = model.query_openai_model(prompt)\n",
    "            packet['attribution'] = attribution\n",
    "        else:\n",
    "            packet['attribution'] = 'None'\n",
    "        doc_output.append(packet)\n",
    "    all_attributions.append(doc_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "62821431",
   "metadata": {},
   "outputs": [],
   "source": [
    "curie_results = pd.concat(list(map(pd.DataFrame, all_attributions)))\n",
    "curie_results.to_csv('cache/2023-01-19__curie-without-nones-all-training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "f5d23758",
   "metadata": {},
   "outputs": [],
   "source": [
    "babbage_results = pd.concat(list(map(pd.DataFrame, all_attributions)))\n",
    "babbage_results.to_csv('cache/2023-01-19__babbage-without-nones-all-training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "514bb8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent</th>\n",
       "      <th>sent_idx</th>\n",
       "      <th>head</th>\n",
       "      <th>quote_type</th>\n",
       "      <th>source_type</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>attribution</th>\n",
       "      <th>match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In March , as small businesses across the coun...</td>\n",
       "      <td>0</td>\n",
       "      <td>Shanel Fields</td>\n",
       "      <td>QUOTE</td>\n",
       "      <td>Named Individual</td>\n",
       "      <td>905</td>\n",
       "      <td>MD Ally</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For Ms. Fields , the timing could n’t have bee...</td>\n",
       "      <td>1</td>\n",
       "      <td>Shanel Fields</td>\n",
       "      <td>BACKGROUND</td>\n",
       "      <td></td>\n",
       "      <td>905</td>\n",
       "      <td>Shanel Fields</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Her company , MD Ally , allows 911 dispatchers...</td>\n",
       "      <td>2</td>\n",
       "      <td>Shanel Fields</td>\n",
       "      <td>BACKGROUND</td>\n",
       "      <td></td>\n",
       "      <td>905</td>\n",
       "      <td>MD Ally</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“</td>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>905</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Something that a lot of people do n’t know is ...</td>\n",
       "      <td>4</td>\n",
       "      <td>Shanel Fields</td>\n",
       "      <td>QUOTE</td>\n",
       "      <td></td>\n",
       "      <td>905</td>\n",
       "      <td>Shanel Fields</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>The wind sounded like an airplane engine , he ...</td>\n",
       "      <td>75</td>\n",
       "      <td>Jorge Gutierrez</td>\n",
       "      <td>QUOTE</td>\n",
       "      <td></td>\n",
       "      <td>215</td>\n",
       "      <td>Paul Block</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>And the fire was running across hillsides agai...</td>\n",
       "      <td>76</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>215</td>\n",
       "      <td>Paul Block</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>“</td>\n",
       "      <td>77</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>215</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>It looked like someone was throwing sand , ” h...</td>\n",
       "      <td>78</td>\n",
       "      <td>Jorge Gutierrez</td>\n",
       "      <td>QUOTE</td>\n",
       "      <td></td>\n",
       "      <td>215</td>\n",
       "      <td>Paul Block</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>”</td>\n",
       "      <td>79</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>215</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6721 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sent  sent_idx  \\\n",
       "0   In March , as small businesses across the coun...         0   \n",
       "1   For Ms. Fields , the timing could n’t have bee...         1   \n",
       "2   Her company , MD Ally , allows 911 dispatchers...         2   \n",
       "3                                                   “         3   \n",
       "4   Something that a lot of people do n’t know is ...         4   \n",
       "..                                                ...       ...   \n",
       "75  The wind sounded like an airplane engine , he ...        75   \n",
       "76  And the fire was running across hillsides agai...        76   \n",
       "77                                                  “        77   \n",
       "78  It looked like someone was throwing sand , ” h...        78   \n",
       "79                                                  ”        79   \n",
       "\n",
       "               head  quote_type       source_type doc_id    attribution  match  \n",
       "0     Shanel Fields       QUOTE  Named Individual    905        MD Ally  False  \n",
       "1     Shanel Fields  BACKGROUND                      905  Shanel Fields   True  \n",
       "2     Shanel Fields  BACKGROUND                      905        MD Ally  False  \n",
       "3                                                    905           None   True  \n",
       "4     Shanel Fields       QUOTE                      905  Shanel Fields   True  \n",
       "..              ...         ...               ...    ...            ...    ...  \n",
       "75  Jorge Gutierrez       QUOTE                      215     Paul Block  False  \n",
       "76                                                   215     Paul Block   True  \n",
       "77                                                   215           None   True  \n",
       "78  Jorge Gutierrez       QUOTE                      215     Paul Block  False  \n",
       "79                                                   215           None   True  \n",
       "\n",
       "[6721 rows x 8 columns]"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "babbage_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "b2cc16b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "babbage_results['match'] = babbage_results.apply(lambda x: test_in(x['head'], x['attribution']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "30e30168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>sent_idx</th>\n",
       "      <th>sent</th>\n",
       "      <th>is_quote_true_label</th>\n",
       "      <th>detection_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>905</td>\n",
       "      <td>0</td>\n",
       "      <td>In March , as small businesses across the coun...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.001217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>905</td>\n",
       "      <td>1</td>\n",
       "      <td>For Ms. Fields , the timing could n’t have bee...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.006014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>905</td>\n",
       "      <td>2</td>\n",
       "      <td>Her company , MD Ally , allows 911 dispatchers...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>905</td>\n",
       "      <td>3</td>\n",
       "      <td>“</td>\n",
       "      <td>False</td>\n",
       "      <td>0.220838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>905</td>\n",
       "      <td>4</td>\n",
       "      <td>Something that a lot of people do n’t know is ...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.999502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6716</th>\n",
       "      <td>215</td>\n",
       "      <td>75</td>\n",
       "      <td>The wind sounded like an airplane engine , he ...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.917762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6717</th>\n",
       "      <td>215</td>\n",
       "      <td>76</td>\n",
       "      <td>And the fire was running across hillsides agai...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.568652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6718</th>\n",
       "      <td>215</td>\n",
       "      <td>77</td>\n",
       "      <td>“</td>\n",
       "      <td>False</td>\n",
       "      <td>0.282584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6719</th>\n",
       "      <td>215</td>\n",
       "      <td>78</td>\n",
       "      <td>It looked like someone was throwing sand , ” h...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.983908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6720</th>\n",
       "      <td>215</td>\n",
       "      <td>79</td>\n",
       "      <td>”</td>\n",
       "      <td>False</td>\n",
       "      <td>0.282604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6721 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      doc_id  sent_idx                                               sent  \\\n",
       "0        905         0  In March , as small businesses across the coun...   \n",
       "1        905         1  For Ms. Fields , the timing could n’t have bee...   \n",
       "2        905         2  Her company , MD Ally , allows 911 dispatchers...   \n",
       "3        905         3                                                  “   \n",
       "4        905         4  Something that a lot of people do n’t know is ...   \n",
       "...      ...       ...                                                ...   \n",
       "6716     215        75  The wind sounded like an airplane engine , he ...   \n",
       "6717     215        76  And the fire was running across hillsides agai...   \n",
       "6718     215        77                                                  “   \n",
       "6719     215        78  It looked like someone was throwing sand , ” h...   \n",
       "6720     215        79                                                  ”   \n",
       "\n",
       "      is_quote_true_label  detection_prob  \n",
       "0                    True        0.001217  \n",
       "1                   False        0.006014  \n",
       "2                   False        0.000058  \n",
       "3                   False        0.220838  \n",
       "4                    True        0.999502  \n",
       "...                   ...             ...  \n",
       "6716                 True        0.917762  \n",
       "6717                False        0.568652  \n",
       "6718                False        0.282584  \n",
       "6719                 True        0.983908  \n",
       "6720                False        0.282604  \n",
       "\n",
       "[6721 rows x 5 columns]"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detection_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "d68d1129",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_exclude = [\n",
    "    'NO QUOTE', \n",
    "    'NARRATIVE',\n",
    "    'BACKGROUND',\n",
    "    'DIRECT OBSERVATION'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "e1bba8c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8437928669410151"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "babbage_results.loc[lambda df: ~df['quote_type'].isin(to_exclude)]['match'].mean()#.fillna(False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1050878c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "6d5640fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "babbage_results = babbage_results.rename(columns={'y_pred': 'attribution'})\n",
    "for col_to_drop in ['sent', 'detection_prob']:\n",
    "    if col_to_drop in babbage_results.columns:\n",
    "        babbage_results = babbage_results.drop(col_to_drop, axis=1)\n",
    "babbage_results = babbage_results.assign(doc_id=lambda df: df['doc_id'].astype(int))\n",
    "\n",
    "merged_df = detection_df.merge(babbage_results, how='left', left_on=['doc_id', 'sent_idx'], right_on=['doc_id', 'sent_idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "ccd6b95c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8437928669410151"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.loc[lambda df: ~df['quote_type'].isin(to_exclude)]['match'].mean()#.fillna(False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "5637fc24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_idx</th>\n",
       "      <th>head</th>\n",
       "      <th>quote_type</th>\n",
       "      <th>source_type</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>attribution</th>\n",
       "      <th>match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Shanel Fields</td>\n",
       "      <td>QUOTE</td>\n",
       "      <td>Named Individual</td>\n",
       "      <td>905</td>\n",
       "      <td>MD Ally</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Shanel Fields</td>\n",
       "      <td>BACKGROUND</td>\n",
       "      <td></td>\n",
       "      <td>905</td>\n",
       "      <td>Shanel Fields</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Shanel Fields</td>\n",
       "      <td>BACKGROUND</td>\n",
       "      <td></td>\n",
       "      <td>905</td>\n",
       "      <td>MD Ally</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>905</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Shanel Fields</td>\n",
       "      <td>QUOTE</td>\n",
       "      <td></td>\n",
       "      <td>905</td>\n",
       "      <td>Shanel Fields</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>75</td>\n",
       "      <td>Jorge Gutierrez</td>\n",
       "      <td>QUOTE</td>\n",
       "      <td></td>\n",
       "      <td>215</td>\n",
       "      <td>Paul Block</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>76</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>215</td>\n",
       "      <td>Paul Block</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>77</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>215</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>78</td>\n",
       "      <td>Jorge Gutierrez</td>\n",
       "      <td>QUOTE</td>\n",
       "      <td></td>\n",
       "      <td>215</td>\n",
       "      <td>Paul Block</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>79</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>215</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6721 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    sent_idx             head  quote_type       source_type  doc_id  \\\n",
       "0          0    Shanel Fields       QUOTE  Named Individual     905   \n",
       "1          1    Shanel Fields  BACKGROUND                       905   \n",
       "2          2    Shanel Fields  BACKGROUND                       905   \n",
       "3          3                                                    905   \n",
       "4          4    Shanel Fields       QUOTE                       905   \n",
       "..       ...              ...         ...               ...     ...   \n",
       "75        75  Jorge Gutierrez       QUOTE                       215   \n",
       "76        76                                                    215   \n",
       "77        77                                                    215   \n",
       "78        78  Jorge Gutierrez       QUOTE                       215   \n",
       "79        79                                                    215   \n",
       "\n",
       "      attribution  match  \n",
       "0         MD Ally  False  \n",
       "1   Shanel Fields   True  \n",
       "2         MD Ally  False  \n",
       "3            None   True  \n",
       "4   Shanel Fields   True  \n",
       "..            ...    ...  \n",
       "75     Paul Block  False  \n",
       "76     Paul Block   True  \n",
       "77           None   True  \n",
       "78     Paul Block  False  \n",
       "79           None   True  \n",
       "\n",
       "[6721 rows x 7 columns]"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "babbage_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "1b037103",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_babbage_results = pd.read_csv('cache/2023-01-19__babbage-without-nones-all-training.csv').assign(head=lambda df: df['head'].fillna(''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "b2cbf133",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = test_babbage_results.loc[lambda df: ~df['quote_type'].fillna('NO QUOTE').isin(to_exclude)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44092fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "f92316bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                2958\n",
       "QUOTE                           2373\n",
       "BACKGROUND                       447\n",
       "STATEMENT                        161\n",
       "NARRATIVE                        134\n",
       "PUBLISHED WORK                   103\n",
       "PROPOSAL/ORDER/LAW                71\n",
       "PRESS REPORT                      68\n",
       "COMMUNICATION, NOT TO JOURNO      63\n",
       "PUBLIC SPEECH, NOT TO JOURNO      60\n",
       "DIRECT OBSERVATION                50\n",
       "LAWSUIT                           45\n",
       "PROPOSAL                          29\n",
       "VOTE/POLL                         23\n",
       "PRICE SIGNAL                      21\n",
       "DECLINED COMMENT                  19\n",
       "DOCUMENT                          19\n",
       "Other: LAWSUIT                    17\n",
       "SOCIAL MEDIA POST                 16\n",
       "TWEET                             10\n",
       "Other: Evaluation                  9\n",
       "Other: DIRECT OBSERVATION          8\n",
       "Other: Campaign filing             7\n",
       "Other: PROPOSAL                    5\n",
       "Other: Campaign Filing             4\n",
       "Other: Data Analysis               1\n",
       "Name: quote_type, dtype: int64"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "babbage_results['quote_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "5de573b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QUOTE                           2373\n",
       "BACKGROUND                       447\n",
       "STATEMENT                        161\n",
       "NARRATIVE                        134\n",
       "PUBLISHED WORK                   103\n",
       "PROPOSAL/ORDER/LAW                71\n",
       "PRESS REPORT                      68\n",
       "COMMUNICATION, NOT TO JOURNO      63\n",
       "PUBLIC SPEECH, NOT TO JOURNO      60\n",
       "DIRECT OBSERVATION                50\n",
       "LAWSUIT                           45\n",
       "PROPOSAL                          29\n",
       "VOTE/POLL                         23\n",
       "PRICE SIGNAL                      21\n",
       "DECLINED COMMENT                  19\n",
       "DOCUMENT                          19\n",
       "Other: LAWSUIT                    17\n",
       "SOCIAL MEDIA POST                 16\n",
       "TWEET                             10\n",
       "Other: Evaluation                  9\n",
       "Other: DIRECT OBSERVATION          8\n",
       "Other: Campaign filing             7\n",
       "Other: PROPOSAL                    5\n",
       "Other: Campaign Filing             4\n",
       "Other: Data Analysis               1\n",
       "Name: quote_type, dtype: int64"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_babbage_results\n",
    "#  .assign(quote_type=lambda df: df['quote_type'].fillna('NO QUOTE'))\n",
    "#  .loc[lambda df: ~df['quote_type'].isin(to_exclude)]\n",
    "#  .loc[lambda df: df['sent'].str.len() > 2]\n",
    " ['quote_type'].value_counts()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f72b6bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "3efe4b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'full': 0.888160508541915,\n",
       " '': 1.0,\n",
       " 'COMMUNICATION': 0.746031746031746,\n",
       " 'COURT PROCEEDING': 0.5967741935483871,\n",
       " 'DECLINED COMMENT': 0.631578947368421,\n",
       " 'DIRECT OBSERVATION': 0.2222222222222222,\n",
       " 'DIRECT QUOTE': 0.8173701298701299,\n",
       " 'INDIRECT QUOTE': 0.8759305210918115,\n",
       " 'PRESS REPORT': 0.5373134328358209,\n",
       " 'PRICE SIGNAL': 0.5238095238095238,\n",
       " 'PROPOSAL/ORDER/LAW': 0.6190476190476191,\n",
       " 'PUBLIC SPEECH': 0.8983050847457628,\n",
       " 'PUBLISHED WORK': 0.6287878787878788,\n",
       " 'SOCIAL MEDIA POST': 0.6538461538461539,\n",
       " 'STATEMENT': 0.8375,\n",
       " 'VOTE/POLL': 0.4782608695652174}"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_openai_results(detection_df, test_babbage_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f83d5b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
