Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
***** Running Prediction *****
  Num examples = 4
  Batch size = 1
    4 training samples
    4 validation samples
./sentence_model.py:241: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3281.)
  return (hidden.T * attention_mask.T).T.mean(axis=1)

100%|██████████| 4/4 [00:05<00:00,  1.58s/it]***** Running Evaluation *****
  Num examples = 4
  Batch size = 1
*** Pre-run evaluation ***

8it [00:13,  1.89s/it]Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
8it [00:13,  1.75s/it]
***** Running training *****
  Num examples = 4
  Num Epochs = 1
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 1
  Gradient Accumulation steps = 1
  Total optimization steps = 4
  0%|          | 0/4 [00:00<?, ?it/s]
***** pre-training eval metrics *****
  eval_f1                 =     0.6667
  eval_loss               =     0.7029
  eval_runtime            = 0:00:07.87
  eval_samples_per_second =      0.508



100%|██████████| 4/4 [00:26<00:00,  7.03s/it]
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|██████████| 4/4 [00:26<00:00,  6.54s/it]
Saving model checkpoint to /Users/alex/Projects/usc-research/source-exploration/tasks/quote_prediction/
Configuration saved in /Users/alex/Projects/usc-research/source-exploration/tasks/quote_prediction/config.json
Model weights saved in /Users/alex/Projects/usc-research/source-exploration/tasks/quote_prediction/pytorch_model.bin
***** Running Prediction *****
  Num examples = 4
  Batch size = 1
{'train_runtime': 26.1803, 'train_samples_per_second': 0.153, 'train_steps_per_second': 0.153, 'train_loss': 1.7515984773635864, 'epoch': 1.0}
***** train metrics *****
  epoch                    =        1.0
  train_loss               =     1.7516
  train_runtime            = 0:00:26.18
  train_samples            =          4
  train_samples_per_second =      0.153
  train_steps_per_second   =      0.153
*** Evaluate ***



100%|██████████| 4/4 [00:05<00:00,  1.56s/it]
***** post-training eval metrics *****
  eval_samples            =          4
  test_f1                 =     0.6667
  test_loss               =     0.7697
  test_runtime            = 0:00:07.73
  test_samples_per_second =      0.517
  test_steps_per_second   =      0.517