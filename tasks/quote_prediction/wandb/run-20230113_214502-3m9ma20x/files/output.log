Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdModel: ['cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running Prediction *****
  Num examples = 4
  Batch size = 1
    4 training samples
    4 validation samples
Python 3.7.6 (default, Jan  8 2020, 13:42:34)
Type 'copyright', 'credits' or 'license' for more information
IPython 7.22.0 -- An enhanced Interactive Python. Type '?' for help.
PyDev console: using IPython 7.22.0
Out[1]:
tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])
Out[2]: torch.Size([1, 576])
Out[3]: torch.Size([1, 576])
Out[4]:
tensor([[   65, 30147,  4138,  4985,   358, 44624,  1090,  3532,   420,   363,
         12593,  5459,   112, 19887,   578,  7037, 50279,   713,   387, 48335,
           388,  1276,  6842,   112,  1491, 36560,  4660,  3535,   420, 10481,
           114,   484,  1090,   112,   321,   100,  3050,  3104, 20079,   430,
          7612,  1648,   112,   100,  1545,   420,   363,  1679,  8074,   385,
          4575,   358,  6204,   387, 12170,   385,   321,   100,   360, 45942,
          1574,   391, 23654,  2028,  9262,  4361,  1276,  6842,  5465,  1242,
           363,  5459,   114,   321,   100,  2242, 32454, 12728,   472,   810,
          8708,   112,   358,  4765, 13554,   430, 30147,   609,   569,  3212,
           388,   968,  5459, 14224,   112,   632,   388,   363,  1090,   427,
          6260,   363,  7521,   113,  1093, 10007,   427,   569,   688, 13572,
         12593,  4350,   363,   487,   113,   391,  3199,   113,   361,  6768,
          8587,   420,   363,  2424,   651,  3503,  2562,   430,  1830, 40070,
          1288,   114,   321,   100,  3338,   363,  4772,   385,   529,  5459,
           524,  9167,   358, 29642,   391,   367, 23127, 25171,   430,   363,
          3848,   387, 10481,   112,   100,  7098,   114,   472,   810,  8708,
           632,   114, 30147,   632,   388,   363,  1090,   427,   441,   651,
         12666,  3725, 26766,   517,  7521,  9772,   391,   764,  7782,  1242,
           358,  2368,  4466,   388, 12593,   388,  2896,   391,  3002,   114,
           733,   736, 12666,  1643,  3535,   388,   363,  4837,   387, 48601,
           391, 12041,   629,   517,   363,   468,   549,  5404, 14135,  8587,
           391,   612,  7782,   112,   609,  2392,  3487,  9213,   385, 12593,
           100,   183,  2067,  1994,   112, 12205, 26905,  4950, 17332,   114,
           484,  7521,   113,  1093, 17890, 26228,   112,  7098,   114,   472,
           810,  8708,   632,   388,   363,  1090,   112,   321,   100,   166,
         15561,  3976,  4155,   385,  1112,  3407, 31421,   385, 17876, 11208,
         18600,   112,   382, 13091,   840,  3331, 15807,  1200,   114,   321,
           100,  1540,  2424,  8587,   112,   363,  1090,   632,   112,   524,
           321,   100,   182, 28500,   407,  1074,   646,  3967, 37662,  3878,
           112,  1491, 17802,   113,  5007, 20394,   112,  5697,  1046,   391,
         20482,  2147,   112,   388, 42356, 22432, 12521,  3107,   114,   321,
           100,   484, 30147,  6474,   648,  3300,   456,   685,  1806,  5605,
           391,  6234,  2729,  2199,  4435, 26766,   517,   363,  7521,   113,
          1093, 10007,   420,   363,  2398,  6997,  2594,   387,   468,  1199,
          3856,   112,   363,  1489, 24409,   430,  3393,   391,  6435,  9517,
           385,  5194,   391,  4419, 12593,   114, 10544, 34903,   112,   363,
         12593,  3538,   430, 12894,   363,  9091,   112,   632,   388,   358,
          2744,   427,   363,  1437,  6388,   387,   363,  2566,   385,   468,
          1199,  3856,   474, 10162,   112,   576,   427,   321,   100,  1270,
          3029,   387,   878,  3553, 26766,   582,   408,  3037,   850,  7735,
           517, 10319,  1852,   391,  4273,   114,   321,   100,  1871,   114,
         34903,  1545,   363,   468,  1199,  3856, 13572,   321,   100,  1270,
          2558, 14888,   114,   321,   100,  3226,   722,   705,   112,   931,
           762,   524,   688,  3024,   388, 12593,   112,   363,  6147,  3788,
           100,   183,   850, 35115,  1600,   112,  1302,  2906,   112,   719,
           363, 33459,  2641, 13572,   468,   549,  5404, 12770,   609,   651,
          8087,   363,  7521,   113, 17179,  1331,   757, 24430,   114,  7521,
          9772,  5110,   363, 33891,  5762,   456, 41876,   387,  4169,   112,
           764,  8016,  9077,   114, 12593,   419,   884,   631,   387,   363,
          1096,   100,   183,   850, 14453, 15807,  3898,   393, 48486,  1057,
           112,   452,  4120,  1512,   387,   363,  3366,   388, 20059,   862,
           387,  2158,   391,   685,  6435,  8360,   114,   484, 33891,  5762,
           524,  7090,   358,  2269,   387, 29335,   388,   363,  1714,  1328,
           391,   524,   688,  8087,   604,   387,   363,  8473,  2594,   387,
         48601,   517,  8587, 19975,   452,   363, 38803,  1331,   114,  5001,
           363, 33891,  5762,  1092,  1731, 44601,   112,   363,  3240,   112,
         38803, 38635,  2929,   524,  2168,   385,  4432,   427,   585,   582,
         28333,   441,  1727,  2846,   114,    66]])
Attention type 'block_sparse' is not possible if sequence_length: 576 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...
  0%|          | 0/4 [00:00<?, ?it/s]
Out[5]:
tensor([[   65, 30951,  6340,   112,  2621,  5070,   321,   100,  2621,  5070,
           391,  2359,  5070, 22213, 10802,   391, 20482,  2147,  2074,   612,
         20271,  4966,   420,  3736,   388,   612,   818,  7037, 19223,   388,
          2037,   913,   112,   363,  2621,  7084,  6048,  9576,   632,   114,
          1501, 18600,   648,  3494,  2199,   420,  2136,  1836,   114,  2621,
          7084,  2523, 13529, 12427,   745,  4221,   385,   408,   358,  2359,
          7084, 10802,  9682,   388,   676, 23378, 40657,   112,   358,  2621,
          7084,  4966,  8069,  5194,   387, 22473,   112,   480,   614,   126,
          4410,   380,   114,   177,   114,  3736,   112,   363,  6048,  9576,
           632,   388,   358,  4607,  2744,   114,   484,  2744,   851,   508,
          3169,   698,  2566,   523,   363, 10802,   114,  2621,  5070,  7183,
           517,  9746,   321,   100,  4699,  8348,   100,   387, 19780,   523,
           358, 20809,   113, 17906, 17013, 20482,  4427,   112, 10923,   363,
         10802,   100,   183, 14026,  1067,   388,   363,  2359,   112,   363,
         12058,   632,   114,   321,   100,  5223,  2523,   569,  3321,   764,
         49303,   391,   419,  7274,  5065,   363,  3457,   387,   363,  2359,
          7084,  2523,   112,   100,   441,  2188,   114,  8095,  1040,   762,
           648, 23825,   523, 15526,   388,   676, 23378, 40657,   112,   632,
          9865,  1102,   607,   113,  1451,   397,   112,   358,  8069,  1844,
           114,  1871,   114,  9865,   632,   713,   648,   746,  7204,  3237,
           387,  2566,   114,   484,  5264,   387,  2147,   474,   363,   818,
          7037, 19223,  1123,   363,  2779,  1302,  2359,  5070,  5712,   382,
         20482,  1469,   420,   358,  2621,  7084,  4966,  7123,   388,  3151,
           112,  5271,   835, 45434,   391,   835, 10481,   114,  1730,   363,
           741,   112,  2621,  5070,   403, 39341, 13032,   517, 34020,  2359,
          7084,  2586,  6217,   452,   764,   999, 20482, 33734,   114,   410,
          5837,   524,   688,   420,   363,  4586,  1964,   363,  2779,   100,
          7373,  7037, 20809,   113, 18186,  4966,  1302,  2548,   114,   705,
           112,   719,   835,  2621,  7084,  4966, 11043,   648,  6512, 10758,
           517,  2057, 18547,   427,   363,  2621,   632,   648, 19070,   517,
           363,  2359,   114,  2359,  5070,  6800, 26731,   363, 18547,   114,
           655, 24772,   112,  2621,  5070,  1039,  1386,  2641,  1363, 48893,
          4080,  1964,   363,  4966,   385,  7126, 11714,  6319,   757,  2359,
          5070,   112,   358, 18644, 10792,   523,   363, 10351,  1911,   427,
           651,   508,   688,  1074,   388,  1468,   913,   114,   484,  2359,
          3001,   420, 11714, 48893,  4080,   387,   764,   999,   112,   391,
           441,  8657,   385,  1469,  2621,  5070,   100,   183,   114,   676,
         23378, 40657,   112,   363,  8069,   911,   363,  2359,  7084, 10802,
          4221,   385,   524, 11507,   112,   419,   631,   387,   363,  3107,
           911,   363,  2621,   651,  9359, 23079, 11714,   114,   484,  2621,
          7084,  2523,   632,  4945,   387,   764, 48893,  3211, 13692,   648,
          7526,   517,   363, 10802,   114,    66]])
  File "<ipython-input-6-9c7e503dfa4b>", line 1
    batch_by_columns['labels']]
                              ^
SyntaxError: invalid syntax
