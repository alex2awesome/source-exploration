Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
***** Running Prediction *****
  Num examples = 4
  Batch size = 1
    4 training samples
    4 validation samples
Python 3.7.6 (default, Jan  8 2020, 13:42:34)
Type 'copyright', 'credits' or 'license' for more information
IPython 7.22.0 -- An enhanced Interactive Python. Type '?' for help.
PyDev console: using IPython 7.22.0
Out[1]:
{'input_ids': [tensor([[   65, 30147,  4138,  4985,   358, 44624,  1090,  3532,   420,   363,
           12593,  5459,   112, 19887,   578,  7037, 50279,   713,   387, 48335,
             388,  1276,  6842,   112,  1491, 36560,  4660,  3535,   420, 10481,
             114,    66,     0,     0,     0,     0,     0,     0,     0,     0,
               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
               0],
          [   65,   484,  1090,   112,   321,   100,  3050,  3104, 20079,   430,
            7612,  1648,   112,   100,  1545,   420,   363,  1679,  8074,   385,
            4575,   358,  6204,   387, 12170,   385,   321,   100,   360, 45942,
            1574,   391, 23654,  2028,  9262,  4361,  1276,  6842,  5465,  1242,
             363,  5459,   114,    66,     0,     0,     0,     0,     0,     0,
               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
               0],
          [   65,   321,   100,  2242, 32454, 12728,   472,   810,  8708,   112,
             358,  4765, 13554,   430, 30147,   609,   569,  3212,   388,   968,
            5459, 14224,   112,   632,   388,   363,  1090,   427,  6260,   363,
            7521,   113,  1093, 10007,   427,   569,   688, 13572, 12593,  4350,
             363,   487,   113,   391,  3199,   113,   361,  6768,  8587,   420,
             363,  2424,   651,  3503,  2562,   430,  1830, 40070,  1288,   114,
              66],
          [   65,   321,   100,  3338,   363,  4772,   385,   529,  5459,   524,
            9167,   358, 29642,   391,   367, 23127, 25171,   430,   363,  3848,
             387, 10481,   112,   100,  7098,   114,   472,   810,  8708,   632,
             114,    66,     0,     0,     0,     0,     0,     0,     0,     0,
               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
               0],
          [   65, 30147,   632,   388,   363,  1090,   427,   441,   651, 12666,
            3725, 26766,   517,  7521,  9772,   391,   764,  7782,  1242,   358,
            2368,  4466,   388, 12593,   388,  2896,   391,  3002,   114,    66,
               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
               0],
          [   65,   733,   736, 12666,  1643,  3535,   388,   363,  4837,   387,
           48601,   391, 12041,   629,   517,   363,   468,   549,  5404, 14135,
            8587,   391,   612,  7782,   112,   609,  2392,  3487,  9213,   385,
           12593,   100,   183,  2067,  1994,   112, 12205, 26905,  4950, 17332,
             114,    66,     0,     0,     0,     0,     0,     0,     0,     0,
               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
               0],
          [   65,   484,  7521,   113,  1093, 17890, 26228,   112,  7098,   114,
             472,   810,  8708,   632,   388,   363,  1090,   112,   321,   100,
             166, 15561,  3976,  4155,   385,  1112,  3407, 31421,   385, 17876,
           11208, 18600,   112,   382, 13091,   840,  3331, 15807,  1200,   114,
              66,     0,     0,     0,     0,     0,     0,     0,     0,     0,
               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
               0],
          [   65,   321,   100,  1540,  2424,  8587,   112,   363,  1090,   632,
             112,   524,   321,   100,   182, 28500,   407,  1074,   646,  3967,
           37662,  3878,   112,  1491, 17802,   113,  5007, 20394,   112,  5697,
            1046,   391, 20482,  2147,   112,   388, 42356, 22432, 12521,  3107,
             114,    66,     0,     0,     0,     0,     0,     0,     0,     0,
               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
               0],
          [   65,   321,   100,   484, 30147,  6474,   648,  3300,   456,   685,
            1806,  5605,   391,  6234,  2729,  2199,  4435, 26766,   517,   363,
            7521,   113,  1093, 10007,   420,   363,  2398,  6997,  2594,   387,
             468,  1199,  3856,   112,   363,  1489, 24409,   430,  3393,   391,
            6435,  9517,   385,  5194,   391,  4419, 12593,   114,    66,     0,
               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
               0],
          [   65, 10544, 34903,   112,   363, 12593,  3538,   430, 12894,   363,
            9091,   112,   632,   388,   358,  2744,   427,   363,  1437,  6388,
             387,   363,  2566,   385,   468,  1199,  3856,   474, 10162,   112,
             576,   427,   321,   100,  1270,  3029,   387,   878,  3553, 26766,
             582,   408,  3037,   850,  7735,   517, 10319,  1852,   391,  4273,
             114,    66,     0,     0,     0,     0,     0,     0,     0,     0,
               0],
          [   65,   321,   100,  1871,   114, 34903,  1545,   363,   468,  1199,
            3856, 13572,   321,   100,  1270,  2558, 14888,   114,    66,     0,
               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
               0],
          [   65,   321,   100,  3226,   722,   705,   112,   931,   762,   524,
             688,  3024,   388, 12593,   112,   363,  6147,  3788,   100,   183,
             850, 35115,  1600,   112,  1302,  2906,   112,   719,   363, 33459,
            2641, 13572,   468,   549,  5404, 12770,   609,   651,  8087,   363,
            7521,   113, 17179,  1331,   757, 24430,   114,    66,     0,     0,
               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
               0],
          [   65,  7521,  9772,  5110,   363, 33891,  5762,   456, 41876,   387,
            4169,   112,   764,  8016,  9077,   114,    66,     0,     0,     0,
               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
               0],
          [   65, 12593,   419,   884,   631,   387,   363,  1096,   100,   183,
             850, 14453, 15807,  3898,   393, 48486,  1057,   112,   452,  4120,
            1512,   387,   363,  3366,   388, 20059,   862,   387,  2158,   391,
             685,  6435,  8360,   114,    66,     0,     0,     0,     0,     0,
               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
               0],
          [   65,   484, 33891,  5762,   524,  7090,   358,  2269,   387, 29335,
             388,   363,  1714,  1328,   391,   524,   688,  8087,   604,   387,
             363,  8473,  2594,   387, 48601,   517,  8587, 19975,   452,   363,
           38803,  1331,   114,    66,     0,     0,     0,     0,     0,     0,
               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
               0],
          [   65,  5001,   363, 33891,  5762,  1092,  1731, 44601,   112,   363,
            3240,   112, 38803, 38635,  2929,   524,  2168,   385,  4432,   427,
             585,   582, 28333,   441,  1727,  2846,   114,    66,     0,     0,
               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
               0]])],
 'input_id_att_mask': [tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0.],
          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
           1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0.],
          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
           1., 1., 1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0.],
          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0.],
          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
           1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0.],
          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
           1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0.],
          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
           1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0.],
          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0.],
          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
           0., 0., 0., 0., 0., 0., 0.],
          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
           1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0.],
          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0.],
          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0.],
          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0.],
          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0.],
          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
           0., 0., 0., 0., 0., 0., 0.]])],
 'source_ids': [tensor([[3],
          [3],
          [4],
          [4],
          [3],
          [0],
          [4],
          [3],
          [3],
          [2],
          [2],
          [0],
          [0],
          [0],
          [0],
          [0]])],
 'source_id_att_mask': [tensor([[1.],
          [1.],
          [1.],
          [1.],
          [1.],
          [1.],
          [1.],
          [1.],
          [1.],
          [1.],
          [1.],
          [1.],
          [1.],
          [1.],
          [1.],
          [1.]])],
 'labels': [tensor([1])]}
Attention type 'block_sparse' is not possible if sequence_length: 1 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...
./sentence_model.py:241: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3281.)
  return (hidden.T * attention_mask.T).T.mean(axis=1)
Out[2]: tensor([[-0.0354,  0.2743]])
Out[3]: tensor([1])


100%|██████████| 4/4 [01:08<00:00, 13.28s/it]
Out[4]: [array([1, 1, 0, 0])]
Out[5]:
array([[-0.03543545,  0.27426195],
       [-0.04034809,  0.18248089],
       [-0.0221758 ,  0.29404622],
